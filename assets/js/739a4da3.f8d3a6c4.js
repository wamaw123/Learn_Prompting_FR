"use strict";(self.webpackChunkpromptgineering=self.webpackChunkpromptgineering||[]).push([[170],{3905:(e,a,t)=>{t.d(a,{Zo:()=>p,kt:()=>u});var n=t(67294);function o(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function r(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function i(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?r(Object(t),!0).forEach((function(a){o(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function s(e,a){if(null==e)return{};var t,n,o=function(e,a){if(null==e)return{};var t,n,o={},r=Object.keys(e);for(n=0;n<r.length;n++)t=r[n],a.indexOf(t)>=0||(o[t]=e[t]);return o}(e,a);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(n=0;n<r.length;n++)t=r[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(o[t]=e[t])}return o}var l=n.createContext({}),f=function(e){var a=n.useContext(l),t=a;return e&&(t="function"==typeof e?e(a):i(i({},a),e)),t},p=function(e){var a=f(e.components);return n.createElement(l.Provider,{value:a},e.children)},m="mdxType",g={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},h=n.forwardRef((function(e,a){var t=e.components,o=e.mdxType,r=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),m=f(t),h=o,u=m["".concat(l,".").concat(h)]||m[h]||g[h]||r;return t?n.createElement(u,i(i({ref:a},p),{},{components:t})):n.createElement(u,i({ref:a},p))}));function u(e,a){var t=arguments,o=a&&a.mdxType;if("string"==typeof e||o){var r=t.length,i=new Array(r);i[0]=h;var s={};for(var l in a)hasOwnProperty.call(a,l)&&(s[l]=a[l]);s.originalType=e,s[m]="string"==typeof e?e:o,i[1]=s;for(var f=2;f<r;f++)i[f]=t[f];return n.createElement.apply(null,i)}return n.createElement.apply(null,t)}h.displayName="MDXCreateElement"},98363:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>l,contentTitle:()=>i,default:()=>g,frontMatter:()=>r,metadata:()=>s,toc:()=>f});var n=t(87462),o=(t(67294),t(3905));const r={sidebar_position:1e3},i="\ud83d\udcda Bibliography",s={unversionedId:"bibliography",id:"bibliography",title:"\ud83d\udcda Bibliography",description:"The page contains an organized list of all papers used by this course.",source:"@site/docs/bibliography.md",sourceDirName:".",slug:"/bibliography",permalink:"/docs/bibliography",draft:!1,editUrl:"https://github.com/trigaten/promptgineering/tree/v1.2.3/docs/bibliography.md",tags:[],version:"current",sidebarPosition:1e3,frontMatter:{sidebar_position:1e3},sidebar:"tutorialSidebar",previous:{title:"\ud83d\udcd9 Vocabulary Reference",permalink:"/docs/vocabulary"},next:{title:"\ud83d\udce6 Prompted Products",permalink:"/docs/products"}},l={},f=[{value:"Agents",id:"agents",level:2},{value:"MRKL(@karpas2022mrkl)",id:"mrklkarpas2022mrkl",level:4},{value:"ReAct(@yao2022react)",id:"reactyao2022react",level:4},{value:"PAL(@gao2022pal)",id:"palgao2022pal",level:4},{value:"Auto-GPT(@richards2023)",id:"auto-gptrichards2023",level:4},{value:"Baby AGI(@nakajima2023)",id:"baby-aginakajima2023",level:4},{value:"AgentGPT(@reworkd2023)",id:"agentgptreworkd2023",level:4},{value:"Toolformer(@schick2023toolformer)",id:"toolformerschick2023toolformer",level:4},{value:"Automated",id:"automated",level:2},{value:"AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts(@shin2020autoprompt)",id:"autoprompt-eliciting-knowledge-from-language-models-with-automatically-generated-promptsshin2020autoprompt",level:4},{value:"automatic prompt engineer(@zhou2022large)",id:"automatic-prompt-engineerzhou2022large",level:4},{value:"Soft Prompting(@lester2021power)",id:"soft-promptinglester2021power",level:4},{value:"discretized soft prompting (interpreting)(@khashabi2021prompt)",id:"discretized-soft-prompting-interpretingkhashabi2021prompt",level:4},{value:"Datasets",id:"datasets",level:2},{value:"SCAN dataset (compositional generalization)(@lake2018scan)",id:"scan-dataset-compositional-generalizationlake2018scan",level:4},{value:"GSM8K(@cobbe2021training)",id:"gsm8kcobbe2021training",level:4},{value:"hotpotQA(@yang2018hotpotqa)",id:"hotpotqayang2018hotpotqa",level:4},{value:"multiarith(@roy-roth-2015-solving)",id:"multiarithroy-roth-2015-solving",level:4},{value:"fever dataset(@thorne2018fever)",id:"fever-datasetthorne2018fever",level:4},{value:"bbq(@parrish2021bbq)",id:"bbqparrish2021bbq",level:4},{value:"Detection",id:"detection",level:2},{value:"Don&#39;t ban chatgpt in schools. teach with it.(@roose2022dont)",id:"dont-ban-chatgpt-in-schools-teach-with-itroose2022dont",level:4},{value:"Schools Shouldn&#39;t Ban Access to ChatGPT(@lipman2022gpt)",id:"schools-shouldnt-ban-access-to-chatgptlipman2022gpt",level:4},{value:"Certified Neural Network Watermarks with Randomized Smoothing(@bansal2022certified)",id:"certified-neural-network-watermarks-with-randomized-smoothingbansal2022certified",level:4},{value:"Watermarking Pre-trained Language Models with Backdooring(@gu2022watermarking)",id:"watermarking-pre-trained-language-models-with-backdooringgu2022watermarking",level:4},{value:"GW preparing disciplinary response to AI programs as faculty explore educational use(@noonan2023gw)",id:"gw-preparing-disciplinary-response-to-ai-programs-as-faculty-explore-educational-usenoonan2023gw",level:4},{value:"A Watermark for Large Language Models(@kirchenbauer2023watermarking)",id:"a-watermark-for-large-language-modelskirchenbauer2023watermarking",level:4},{value:"DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature(@mitchell2023detectgpt)",id:"detectgpt-zero-shot-machine-generated-text-detection-using-probability-curvaturemitchell2023detectgpt",level:4},{value:"Image Prompt Engineering",id:"image-prompt-engineering",level:2},{value:"Prompt Engineering for Text-Based Generative Art(@oppenlaender2022prompt)",id:"prompt-engineering-for-text-based-generative-artoppenlaender2022prompt",level:4},{value:"The DALLE 2 Prompt Book(@parsons2022dalleprompt)",id:"the-dalle-2-prompt-bookparsons2022dalleprompt",level:4},{value:"With the right prompt, Stable Diffusion 2.0 can do hands.(@blake2022with)",id:"with-the-right-prompt-stable-diffusion-20-can-do-handsblake2022with",level:4},{value:"Meta Analysis",id:"meta-analysis",level:2},{value:"How Generative AI Is Changing Creative Work(@Davenport_Mittal_2022)",id:"how-generative-ai-is-changing-creative-workdavenport_mittal_2022",level:4},{value:"How AI Will Change the Workplace(@Captain_2023)",id:"how-ai-will-change-the-workplacecaptain_2023",level:4},{value:"ChatGPT took their jobs. Now they walk dogs and fix air conditioners.(@Verma_Vynck_2023)",id:"chatgpt-took-their-jobs-now-they-walk-dogs-and-fix-air-conditionersverma_vynck_2023",level:4},{value:"No title(@IBM_Do_2023)",id:"no-titleibm_do_2023",level:4},{value:"Miscl",id:"miscl",level:2},{value:"The Turking Test: Can Language Models Understand Instructions?(@efrat2020turking)",id:"the-turking-test-can-language-models-understand-instructionsefrat2020turking",level:4},{value:"A Taxonomy of Prompt Modifiers for Text-To-Image Generation(@oppenlaender2022taxonomy)",id:"a-taxonomy-of-prompt-modifiers-for-text-to-image-generationoppenlaender2022taxonomy",level:4},{value:"DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models(@wang2022diffusiondb)",id:"diffusiondb-a-large-scale-prompt-gallery-dataset-for-text-to-image-generative-modelswang2022diffusiondb",level:4},{value:"Optimizing Prompts for Text-to-Image Generation(@hao2022optimizing)",id:"optimizing-prompts-for-text-to-image-generationhao2022optimizing",level:4},{value:"Language Model Cascades(@dohan2022language)",id:"language-model-cascadesdohan2022language",level:4},{value:"Design Guidelines for Prompt Engineering Text-to-Image Generative Models(@liu2022design)",id:"design-guidelines-for-prompt-engineering-text-to-image-generative-modelsliu2022design",level:4},{value:"Discovering Language Model Behaviors with Model-Written Evaluations(@perez2022discovering)",id:"discovering-language-model-behaviors-with-model-written-evaluationsperez2022discovering",level:4},{value:"Selective Annotation Makes Language Models Better Few-Shot Learners(@su2022selective)",id:"selective-annotation-makes-language-models-better-few-shot-learnerssu2022selective",level:4},{value:"Atlas: Few-shot Learning with Retrieval Augmented Language Models(@izacard2022atlas)",id:"atlas-few-shot-learning-with-retrieval-augmented-language-modelsizacard2022atlas",level:4},{value:"STRUDEL: Structured Dialogue Summarization for Dialogue Comprehension(@wang2022strudel)",id:"strudel-structured-dialogue-summarization-for-dialogue-comprehensionwang2022strudel",level:4},{value:"Prompting Is Programming: A Query Language For Large Language Models(@beurerkellner2022prompting)",id:"prompting-is-programming-a-query-language-for-large-language-modelsbeurerkellner2022prompting",level:4},{value:"Parallel Context Windows Improve In-Context Learning of Large Language Models(@ratner2022parallel)",id:"parallel-context-windows-improve-in-context-learning-of-large-language-modelsratner2022parallel",level:4},{value:"Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models(@bursztyn2022learning)",id:"learning-to-perform-complex-tasks-through-compositional-fine-tuning-of-language-modelsbursztyn2022learning",level:4},{value:"Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks(@wang2022supernaturalinstructions)",id:"super-naturalinstructions-generalization-via-declarative-instructions-on-1600-nlp-taskswang2022supernaturalinstructions",level:4},{value:"Making Pre-trained Language Models Better Few-shot Learners(@gao2021making)",id:"making-pre-trained-language-models-better-few-shot-learnersgao2021making",level:4},{value:"How to Prompt? Opportunities and Challenges of Zero- and Few-Shot Learning for Human-AI Interaction in Creative Applications of Generative Models(@dang2022prompt)",id:"how-to-prompt-opportunities-and-challenges-of-zero--and-few-shot-learning-for-human-ai-interaction-in-creative-applications-of-generative-modelsdang2022prompt",level:4},{value:"On Measuring Social Biases in Prompt-Based Multi-Task Learning(@akyrek2022measuring)",id:"on-measuring-social-biases-in-prompt-based-multi-task-learningakyrek2022measuring",level:4},{value:"Plot Writing From Pre-Trained Language Models(@jin2022plot)",id:"plot-writing-from-pre-trained-language-modelsjin2022plot",level:4},{value:"{S}tereo{S}et: Measuring stereotypical bias in pretrained language models(@nadeem-etal-2021-stereoset)",id:"stereoset-measuring-stereotypical-bias-in-pretrained-language-modelsnadeem-etal-2021-stereoset",level:4},{value:"Survey of Hallucination in Natural Language Generation(@Ji_2022)",id:"survey-of-hallucination-in-natural-language-generationji_2022",level:4},{value:"Wordcraft: Story Writing With Large Language Models(@yuan2022wordcraft)",id:"wordcraft-story-writing-with-large-language-modelsyuan2022wordcraft",level:4},{value:"PainPoints: A Framework for Language-based Detection of Chronic Pain and Expert-Collaborative Text-Summarization(@fadnavis2022pain)",id:"painpoints-a-framework-for-language-based-detection-of-chronic-pain-and-expert-collaborative-text-summarizationfadnavis2022pain",level:4},{value:"Self-Instruct: Aligning Language Model with Self Generated Instructions(@wang2022selfinstruct)",id:"self-instruct-aligning-language-model-with-self-generated-instructionswang2022selfinstruct",level:4},{value:"From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models(@guo2022images)",id:"from-images-to-textual-prompts-zero-shot-vqa-with-frozen-large-language-modelsguo2022images",level:4},{value:"New and improved content moderation tooling(@markov_2022)",id:"new-and-improved-content-moderation-toolingmarkov_2022",level:4},{value:"Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference(@schick2020exploiting)",id:"exploiting-cloze-questions-for-few-shot-text-classification-and-natural-language-inferenceschick2020exploiting",level:4},{value:"Human-level concept learning through probabilistic program induction(@lake2015human)",id:"human-level-concept-learning-through-probabilistic-program-inductionlake2015human",level:4},{value:"{Riffusion - Stable diffusion for real-time music generation}(@Forsgren_Martiros_2022)",id:"riffusion---stable-diffusion-for-real-time-music-generationforsgren_martiros_2022",level:4},{value:"How to use OpenAI\u2019s ChatGPT to write the perfect cold email(@bonta2022how)",id:"how-to-use-openais-chatgpt-to-write-the-perfect-cold-emailbonta2022how",level:4},{value:"Cacti: biology and uses(@nobel2002cacti)",id:"cacti-biology-and-usesnobel2002cacti",level:4},{value:"Are Language Models Worse than Humans at Following Prompts? It\u2019s Complicated(@webson2023itscomplicated)",id:"are-language-models-worse-than-humans-at-following-prompts-its-complicatedwebson2023itscomplicated",level:4},{value:"Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration(@wang2023unleashing)",id:"unleashing-cognitive-synergy-in-large-language-models-a-task-solving-agent-through-multi-persona-self-collaborationwang2023unleashing",level:4},{value:"Prompt Hacking",id:"prompt-hacking",level:2},{value:"Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods(@crothers2022machine)",id:"machine-generated-text-a-comprehensive-survey-of-threat-models-and-detection-methodscrothers2022machine",level:4},{value:"New jailbreak based on virtual functions - smuggle illegal tokens to the backend.(@nin2023new)",id:"new-jailbreak-based-on-virtual-functions---smuggle-illegal-tokens-to-the-backendnin2023new",level:4},{value:"Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks(@kang2023exploiting)",id:"exploiting-programmatic-behavior-of-llms-dual-use-through-standard-security-attackskang2023exploiting",level:4},{value:"More than you&#39;ve asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models(@greshake2023youve)",id:"more-than-youve-asked-for-a-comprehensive-analysis-of-novel-prompt-injection-threats-to-application-integrated-large-language-modelsgreshake2023youve",level:4},{value:"ChatGPT &quot;DAN&quot; (and other &quot;Jailbreaks&quot;)(@kiho2023chatgpt)",id:"chatgpt-dan-and-other-jailbreakskiho2023chatgpt",level:4},{value:"Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples(@branch2022evaluating)",id:"evaluating-the-susceptibility-of-pre-trained-language-models-via-handcrafted-adversarial-examplesbranch2022evaluating",level:4},{value:"Prompt injection attacks against GPT-3(@simon2022inject)",id:"prompt-injection-attacks-against-gpt-3simon2022inject",level:4},{value:"Exploiting GPT-3 prompts with malicious inputs that order the model to ignore its previous directions(@goodside2022inject)",id:"exploiting-gpt-3-prompts-with-malicious-inputs-that-order-the-model-to-ignore-its-previous-directionsgoodside2022inject",level:4},{value:"History Correction(@goodside2022history)",id:"history-correctiongoodside2022history",level:4},{value:"adversarial-prompts(@chase2021adversarial)",id:"adversarial-promptschase2021adversarial",level:4},{value:"GPT-3 Prompt Injection Defenses(@goodside2021gpt)",id:"gpt-3-prompt-injection-defensesgoodside2021gpt",level:4},{value:"Talking to machines: prompt engineering &amp; injection(@christoph2022talking)",id:"talking-to-machines-prompt-engineering--injectionchristoph2022talking",level:4},{value:"Using GPT-Eliezer against ChatGPT Jailbreaking(@armstrong2022using)",id:"using-gpt-eliezer-against-chatgpt-jailbreakingarmstrong2022using",level:4},{value:"Exploring Prompt Injection Attacks(@selvi2022exploring)",id:"exploring-prompt-injection-attacksselvi2022exploring",level:4},{value:"The entire prompt of Microsoft Bing Chat?! (Hi, Sydney.)(@kevinbing)",id:"the-entire-prompt-of-microsoft-bing-chat-hi-sydneykevinbing",level:4},{value:"Ignore Previous Prompt: Attack Techniques For Language Models(@perez2022jailbreak)",id:"ignore-previous-prompt-attack-techniques-for-language-modelsperez2022jailbreak",level:4},{value:"Lessons learned on Language Model Safety and misuse(@brundage_2022)",id:"lessons-learned-on-language-model-safety-and-misusebrundage_2022",level:4},{value:"Toxicity Detection with Generative Prompt-based Inference(@wang2022jailbreak)",id:"toxicity-detection-with-generative-prompt-based-inferencewang2022jailbreak",level:4},{value:"ok I saw a few people jailbreaking safeguards openai put on chatgpt so I had to give it a shot myself(@alice2022jailbreak)",id:"ok-i-saw-a-few-people-jailbreaking-safeguards-openai-put-on-chatgpt-so-i-had-to-give-it-a-shot-myselfalice2022jailbreak",level:4},{value:"Bypass @OpenAI&#39;s ChatGPT alignment efforts with this one weird trick(@miguel2022jailbreak)",id:"bypass-openais-chatgpt-alignment-efforts-with-this-one-weird-trickmiguel2022jailbreak",level:4},{value:"ChatGPT jailbreaking itself(@derek2022jailbreak)",id:"chatgpt-jailbreaking-itselfderek2022jailbreak",level:4},{value:"Using &quot;pretend&quot; on #ChatGPT can do some wild stuff. You can kind of get some insight on the future, alternative universe.(@nero2022jailbreak)",id:"using-pretend-on-chatgpt-can-do-some-wild-stuff-you-can-kind-of-get-some-insight-on-the-future-alternative-universenero2022jailbreak",level:4},{value:"I kinda like this one even more!(@nick2022jailbreak)",id:"i-kinda-like-this-one-even-morenick2022jailbreak",level:4},{value:"uh oh(@sam2022jailbreak)",id:"uh-ohsam2022jailbreak",level:4},{value:"Building A Virtual Machine inside ChatGPT(@jonas2022jailbreak)",id:"building-a-virtual-machine-inside-chatgptjonas2022jailbreak",level:4},{value:"Reliability",id:"reliability",level:2},{value:"MathPrompter: Mathematical Reasoning using Large Language Models(@imani2023mathprompter)",id:"mathprompter-mathematical-reasoning-using-large-language-modelsimani2023mathprompter",level:4},{value:"The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning(@ye2022unreliability)",id:"the-unreliability-of-explanations-in-few-shot-prompting-for-textual-reasoningye2022unreliability",level:4},{value:"Prompting GPT-3 To Be Reliable(@si2022prompting)",id:"prompting-gpt-3-to-be-reliablesi2022prompting",level:4},{value:"On the Advance of Making Language Models Better Reasoners(@li2022advance)",id:"on-the-advance-of-making-language-models-better-reasonersli2022advance",level:4},{value:"Ask Me Anything: A simple strategy for prompting language models(@arora2022ama)",id:"ask-me-anything-a-simple-strategy-for-prompting-language-modelsarora2022ama",level:4},{value:"Calibrate Before Use: Improving Few-Shot Performance of Language Models(@zhao2021calibrate)",id:"calibrate-before-use-improving-few-shot-performance-of-language-modelszhao2021calibrate",level:4},{value:"Can large language models reason about medical questions?(@livin2022large)",id:"can-large-language-models-reason-about-medical-questionslivin2022large",level:4},{value:"Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference(@mitchell2022enhancing)",id:"enhancing-self-consistency-and-performance-of-pre-trained-language-models-through-natural-language-inferencemitchell2022enhancing",level:4},{value:"On Second Thought, Let&#39;s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning(@shaikh2022second)",id:"on-second-thought-lets-not-think-step-by-step-bias-and-toxicity-in-zero-shot-reasoningshaikh2022second",level:4},{value:"Evaluating language models can be tricky(@chase2022evaluating)",id:"evaluating-language-models-can-be-trickychase2022evaluating",level:4},{value:"Constitutional AI: Harmlessness from AI Feedback(@bai2022constitutional)",id:"constitutional-ai-harmlessness-from-ai-feedbackbai2022constitutional",level:4},{value:"Surveys",id:"surveys",level:2},{value:"Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics and Speech Recognition(@jurafsky2009)",id:"speech-and-language-processing-an-introduction-to-natural-language-processing-computational-linguistics-and-speech-recognitionjurafsky2009",level:4},{value:"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing(@liu2021pretrain)",id:"pre-train-prompt-and-predict-a-systematic-survey-of-prompting-methods-in-natural-language-processingliu2021pretrain",level:4},{value:"PromptPapers(@ning2022papers)",id:"promptpapersning2022papers",level:4},{value:"A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT(@white2023prompt)",id:"a-prompt-pattern-catalog-to-enhance-prompt-engineering-with-chatgptwhite2023prompt",level:4},{value:"Techniques",id:"techniques",level:2},{value:"Chain of Thought Prompting Elicits Reasoning in Large Language Models(@wei2022chain)",id:"chain-of-thought-prompting-elicits-reasoning-in-large-language-modelswei2022chain",level:4},{value:"Large Language Models are Zero-Shot Reasoners(@kojima2022large)",id:"large-language-models-are-zero-shot-reasonerskojima2022large",level:4},{value:"Self-Consistency Improves Chain of Thought Reasoning in Language Models(@wang2022selfconsistency)",id:"self-consistency-improves-chain-of-thought-reasoning-in-language-modelswang2022selfconsistency",level:4},{value:"What Makes Good In-Context Examples for GPT-3?(@liu2021makes)",id:"what-makes-good-in-context-examples-for-gpt-3liu2021makes",level:4},{value:"Generated Knowledge Prompting for Commonsense Reasoning(@liu2021generated)",id:"generated-knowledge-prompting-for-commonsense-reasoningliu2021generated",level:4},{value:"Recitation-Augmented Language Models(@sun2022recitationaugmented)",id:"recitation-augmented-language-modelssun2022recitationaugmented",level:4},{value:"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?(@min2022rethinking)",id:"rethinking-the-role-of-demonstrations-what-makes-in-context-learning-workmin2022rethinking",level:4},{value:"Show Your Work: Scratchpads for Intermediate Computation with Language Models(@nye2021work)",id:"show-your-work-scratchpads-for-intermediate-computation-with-language-modelsnye2021work",level:4},{value:"Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations(@jung2022maieutic)",id:"maieutic-prompting-logically-consistent-reasoning-with-recursive-explanationsjung2022maieutic",level:4},{value:"STaR: Bootstrapping Reasoning With Reasoning(@zelikman2022star)",id:"star-bootstrapping-reasoning-with-reasoningzelikman2022star",level:4},{value:"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models(@zhou2022leasttomost)",id:"least-to-most-prompting-enables-complex-reasoning-in-large-language-modelszhou2022leasttomost",level:4},{value:"Reframing Instructional Prompts to GPTk\u2019s Language(@mishra2022reframing)",id:"reframing-instructional-prompts-to-gptks-languagemishra2022reframing",level:4},{value:"Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models(@logan-iv-etal-2022-cutting)",id:"cutting-down-on-prompts-and-parameters-simple-few-shot-learning-with-language-modelslogan-iv-etal-2022-cutting",level:4},{value:"Role-Play with Large Language Models(@shanahan2023roleplay)",id:"role-play-with-large-language-modelsshanahan2023roleplay",level:4},{value:"CAMEL: Communicative Agents for &quot;Mind&quot; Exploration of Large Scale Language Model Society(@li2023camel)",id:"camel-communicative-agents-for-mind-exploration-of-large-scale-language-model-societyli2023camel",level:4},{value:"TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks(@santu2023teler)",id:"teler-a-general-taxonomy-of-llm-prompts-for-benchmarking-complex-taskssantu2023teler",level:4},{value:"Models",id:"models",level:2},{value:"Image Models",id:"image-models",level:3},{value:"Stable Diffusion(@rombach2021highresolution)",id:"stable-diffusionrombach2021highresolution",level:4},{value:"DALLE(@ramesh2022hierarchical)",id:"dalleramesh2022hierarchical",level:4},{value:"Language Models",id:"language-models",level:3},{value:"ChatGPT(@chatgpt2022)",id:"chatgptchatgpt2022",level:4},{value:"GPT-3(@brown2020language)",id:"gpt-3brown2020language",level:4},{value:"Instruct GPT(@ouyang2022training)",id:"instruct-gptouyang2022training",level:4},{value:"GPT-4(@openai2023gpt4)",id:"gpt-4openai2023gpt4",level:4},{value:"PaLM: Scaling Language Modeling with Pathways(@chowdhery2022palm)",id:"palm-scaling-language-modeling-with-pathwayschowdhery2022palm",level:4},{value:"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model(@scao2022bloom)",id:"bloom-a-176b-parameter-open-access-multilingual-language-modelscao2022bloom",level:4},{value:"BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting(@yong2022bloom1)",id:"bloom1-adding-language-support-to-bloom-for-zero-shot-promptingyong2022bloom1",level:4},{value:"Jurassic-1: Technical Details and Evaluation, White paper, AI21 Labs, 2021(@lieberjurassic)",id:"jurassic-1-technical-details-and-evaluation-white-paper-ai21-labs-2021lieberjurassic",level:4},{value:"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model(@wange2021gptj)",id:"gpt-j-6b-a-6-billion-parameter-autoregressive-language-modelwange2021gptj",level:4},{value:"Roberta: A robustly optimized bert pretraining approach(@liu2019roberta)",id:"roberta-a-robustly-optimized-bert-pretraining-approachliu2019roberta",level:4},{value:"Tooling",id:"tooling",level:2},{value:"Ides",id:"ides",level:3},{value:"TextBox 2.0: A Text Generation Library with Pre-trained Language Models(@tang2022textbox)",id:"textbox-20-a-text-generation-library-with-pre-trained-language-modelstang2022textbox",level:4},{value:"Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models(@strobelt2022promptide)",id:"interactive-and-visual-prompt-engineering-for-ad-hoc-task-adaptation-with-large-language-modelsstrobelt2022promptide",level:4},{value:"PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts(@bach2022promptsource)",id:"promptsource-an-integrated-development-environment-and-repository-for-natural-language-promptsbach2022promptsource",level:4},{value:"PromptChainer: Chaining Large Language Model Prompts through Visual Programming(@wu2022promptchainer)",id:"promptchainer-chaining-large-language-model-prompts-through-visual-programmingwu2022promptchainer",level:4},{value:"OpenPrompt: An Open-source Framework for Prompt-learning(@ding2021openprompt)",id:"openprompt-an-open-source-framework-for-prompt-learningding2021openprompt",level:4},{value:"PromptMaker: Prompt-Based Prototyping with Large\xa0Language\xa0Models(@jiang2022promptmaker)",id:"promptmaker-prompt-based-prototyping-with-largelanguagemodelsjiang2022promptmaker",level:4},{value:"Tools",id:"tools",level:3},{value:"LangChain(@Chase_LangChain_2022)",id:"langchainchase_langchain_2022",level:4},{value:"GPT Index(@Liu_GPT_Index_2022)",id:"gpt-indexliu_gpt_index_2022",level:4}],p={toc:f},m="wrapper";function g(e){let{components:a,...t}=e;return(0,o.kt)(m,(0,n.Z)({},p,t,{components:a,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"-bibliography"},"\ud83d\udcda Bibliography"),(0,o.kt)("p",null,"The page contains an organized list of all papers used by this course.\nThe papers are organized by topic."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"To cite this course, use the provided citation in the Github repository.")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-md"},"@software{Schulhoff_Learn_Prompting_2022,\n    author = {Schulhoff, Sander and Community Contributors},\n    month = dec,\n    title = {{Learn Prompting}},\n    url = {https://github.com/trigaten/Learn_Prompting},\n    year = {2022}\n}\n")),(0,o.kt)("p",null,"Note: since ",(0,o.kt)("a",{parentName:"p",href:"https://twitter.com/janleike/status/1584618242756132864"},"neither the GPT-3 nor the GPT-3 Instruct paper correspond to davinci models"),", I attempt not to\ncite them as such."),(0,o.kt)("h2",{id:"agents"},"Agents"),(0,o.kt)("h4",{id:"mrklkarpas2022mrkl"},"MRKL",(0,o.kt)("sup",{parentName:"h4",id:"fnref-1"},(0,o.kt)("a",{parentName:"sup",href:"#fn-1",className:"footnote-ref"},"1"))),(0,o.kt)("h4",{id:"reactyao2022react"},"ReAct",(0,o.kt)("sup",{parentName:"h4",id:"fnref-2"},(0,o.kt)("a",{parentName:"sup",href:"#fn-2",className:"footnote-ref"},"2"))),(0,o.kt)("h4",{id:"palgao2022pal"},"PAL",(0,o.kt)("sup",{parentName:"h4",id:"fnref-3"},(0,o.kt)("a",{parentName:"sup",href:"#fn-3",className:"footnote-ref"},"3"))),(0,o.kt)("h4",{id:"auto-gptrichards2023"},"Auto-GPT",(0,o.kt)("sup",{parentName:"h4",id:"fnref-4"},(0,o.kt)("a",{parentName:"sup",href:"#fn-4",className:"footnote-ref"},"4"))),(0,o.kt)("h4",{id:"baby-aginakajima2023"},"Baby AGI",(0,o.kt)("sup",{parentName:"h4",id:"fnref-5"},(0,o.kt)("a",{parentName:"sup",href:"#fn-5",className:"footnote-ref"},"5"))),(0,o.kt)("h4",{id:"agentgptreworkd2023"},"AgentGPT",(0,o.kt)("sup",{parentName:"h4",id:"fnref-6"},(0,o.kt)("a",{parentName:"sup",href:"#fn-6",className:"footnote-ref"},"6"))),(0,o.kt)("h4",{id:"toolformerschick2023toolformer"},"Toolformer",(0,o.kt)("sup",{parentName:"h4",id:"fnref-7"},(0,o.kt)("a",{parentName:"sup",href:"#fn-7",className:"footnote-ref"},"7"))),(0,o.kt)("h2",{id:"automated"},"Automated"),(0,o.kt)("h4",{id:"autoprompt-eliciting-knowledge-from-language-models-with-automatically-generated-promptsshin2020autoprompt"},"AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",(0,o.kt)("sup",{parentName:"h4",id:"fnref-8"},(0,o.kt)("a",{parentName:"sup",href:"#fn-8",className:"footnote-ref"},"8"))),(0,o.kt)("h4",{id:"automatic-prompt-engineerzhou2022large"},"automatic prompt engineer",(0,o.kt)("sup",{parentName:"h4",id:"fnref-9"},(0,o.kt)("a",{parentName:"sup",href:"#fn-9",className:"footnote-ref"},"9"))),(0,o.kt)("h4",{id:"soft-promptinglester2021power"},"Soft Prompting",(0,o.kt)("sup",{parentName:"h4",id:"fnref-10"},(0,o.kt)("a",{parentName:"sup",href:"#fn-10",className:"footnote-ref"},"10"))),(0,o.kt)("h4",{id:"discretized-soft-prompting-interpretingkhashabi2021prompt"},"discretized soft prompting (interpreting)",(0,o.kt)("sup",{parentName:"h4",id:"fnref-11"},(0,o.kt)("a",{parentName:"sup",href:"#fn-11",className:"footnote-ref"},"11"))),(0,o.kt)("h2",{id:"datasets"},"Datasets"),(0,o.kt)("h4",{id:"scan-dataset-compositional-generalizationlake2018scan"},"SCAN dataset (compositional generalization)",(0,o.kt)("sup",{parentName:"h4",id:"fnref-12"},(0,o.kt)("a",{parentName:"sup",href:"#fn-12",className:"footnote-ref"},"12"))),(0,o.kt)("h4",{id:"gsm8kcobbe2021training"},"GSM8K",(0,o.kt)("sup",{parentName:"h4",id:"fnref-13"},(0,o.kt)("a",{parentName:"sup",href:"#fn-13",className:"footnote-ref"},"13"))),(0,o.kt)("h4",{id:"hotpotqayang2018hotpotqa"},"hotpotQA",(0,o.kt)("sup",{parentName:"h4",id:"fnref-14"},(0,o.kt)("a",{parentName:"sup",href:"#fn-14",className:"footnote-ref"},"14"))),(0,o.kt)("h4",{id:"multiarithroy-roth-2015-solving"},"multiarith",(0,o.kt)("sup",{parentName:"h4",id:"fnref-15"},(0,o.kt)("a",{parentName:"sup",href:"#fn-15",className:"footnote-ref"},"15"))),(0,o.kt)("h4",{id:"fever-datasetthorne2018fever"},"fever dataset",(0,o.kt)("sup",{parentName:"h4",id:"fnref-16"},(0,o.kt)("a",{parentName:"sup",href:"#fn-16",className:"footnote-ref"},"16"))),(0,o.kt)("h4",{id:"bbqparrish2021bbq"},"bbq",(0,o.kt)("sup",{parentName:"h4",id:"fnref-17"},(0,o.kt)("a",{parentName:"sup",href:"#fn-17",className:"footnote-ref"},"17"))),(0,o.kt)("h2",{id:"detection"},"Detection"),(0,o.kt)("h4",{id:"dont-ban-chatgpt-in-schools-teach-with-itroose2022dont"},"Don't ban chatgpt in schools. teach with it.",(0,o.kt)("sup",{parentName:"h4",id:"fnref-18"},(0,o.kt)("a",{parentName:"sup",href:"#fn-18",className:"footnote-ref"},"18"))),(0,o.kt)("h4",{id:"schools-shouldnt-ban-access-to-chatgptlipman2022gpt"},"Schools Shouldn't Ban Access to ChatGPT",(0,o.kt)("sup",{parentName:"h4",id:"fnref-19"},(0,o.kt)("a",{parentName:"sup",href:"#fn-19",className:"footnote-ref"},"19"))),(0,o.kt)("h4",{id:"certified-neural-network-watermarks-with-randomized-smoothingbansal2022certified"},"Certified Neural Network Watermarks with Randomized Smoothing",(0,o.kt)("sup",{parentName:"h4",id:"fnref-20"},(0,o.kt)("a",{parentName:"sup",href:"#fn-20",className:"footnote-ref"},"20"))),(0,o.kt)("h4",{id:"watermarking-pre-trained-language-models-with-backdooringgu2022watermarking"},"Watermarking Pre-trained Language Models with Backdooring",(0,o.kt)("sup",{parentName:"h4",id:"fnref-21"},(0,o.kt)("a",{parentName:"sup",href:"#fn-21",className:"footnote-ref"},"21"))),(0,o.kt)("h4",{id:"gw-preparing-disciplinary-response-to-ai-programs-as-faculty-explore-educational-usenoonan2023gw"},"GW preparing disciplinary response to AI programs as faculty explore educational use",(0,o.kt)("sup",{parentName:"h4",id:"fnref-22"},(0,o.kt)("a",{parentName:"sup",href:"#fn-22",className:"footnote-ref"},"22"))),(0,o.kt)("h4",{id:"a-watermark-for-large-language-modelskirchenbauer2023watermarking"},"A Watermark for Large Language Models",(0,o.kt)("sup",{parentName:"h4",id:"fnref-23"},(0,o.kt)("a",{parentName:"sup",href:"#fn-23",className:"footnote-ref"},"23"))),(0,o.kt)("h4",{id:"detectgpt-zero-shot-machine-generated-text-detection-using-probability-curvaturemitchell2023detectgpt"},"DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature",(0,o.kt)("sup",{parentName:"h4",id:"fnref-24"},(0,o.kt)("a",{parentName:"sup",href:"#fn-24",className:"footnote-ref"},"24"))),(0,o.kt)("h2",{id:"image-prompt-engineering"},"Image Prompt Engineering"),(0,o.kt)("h4",{id:"prompt-engineering-for-text-based-generative-artoppenlaender2022prompt"},"Prompt Engineering for Text-Based Generative Art",(0,o.kt)("sup",{parentName:"h4",id:"fnref-25"},(0,o.kt)("a",{parentName:"sup",href:"#fn-25",className:"footnote-ref"},"25"))),(0,o.kt)("h4",{id:"the-dalle-2-prompt-bookparsons2022dalleprompt"},"The DALLE 2 Prompt Book",(0,o.kt)("sup",{parentName:"h4",id:"fnref-26"},(0,o.kt)("a",{parentName:"sup",href:"#fn-26",className:"footnote-ref"},"26"))),(0,o.kt)("h4",{id:"with-the-right-prompt-stable-diffusion-20-can-do-handsblake2022with"},"With the right prompt, Stable Diffusion 2.0 can do hands.",(0,o.kt)("sup",{parentName:"h4",id:"fnref-27"},(0,o.kt)("a",{parentName:"sup",href:"#fn-27",className:"footnote-ref"},"27"))),(0,o.kt)("h2",{id:"meta-analysis"},"Meta Analysis"),(0,o.kt)("h4",{id:"how-generative-ai-is-changing-creative-workdavenport_mittal_2022"},"How Generative AI Is Changing Creative Work",(0,o.kt)("sup",{parentName:"h4",id:"fnref-28"},(0,o.kt)("a",{parentName:"sup",href:"#fn-28",className:"footnote-ref"},"28"))),(0,o.kt)("h4",{id:"how-ai-will-change-the-workplacecaptain_2023"},"How AI Will Change the Workplace",(0,o.kt)("sup",{parentName:"h4",id:"fnref-29"},(0,o.kt)("a",{parentName:"sup",href:"#fn-29",className:"footnote-ref"},"29"))),(0,o.kt)("h4",{id:"chatgpt-took-their-jobs-now-they-walk-dogs-and-fix-air-conditionersverma_vynck_2023"},"ChatGPT took their jobs. Now they walk dogs and fix air conditioners.",(0,o.kt)("sup",{parentName:"h4",id:"fnref-30"},(0,o.kt)("a",{parentName:"sup",href:"#fn-30",className:"footnote-ref"},"30"))),(0,o.kt)("h4",{id:"no-titleibm_do_2023"},"No title",(0,o.kt)("sup",{parentName:"h4",id:"fnref-31"},(0,o.kt)("a",{parentName:"sup",href:"#fn-31",className:"footnote-ref"},"31"))),(0,o.kt)("h2",{id:"miscl"},"Miscl"),(0,o.kt)("h4",{id:"the-turking-test-can-language-models-understand-instructionsefrat2020turking"},"The Turking Test: Can Language Models Understand Instructions?",(0,o.kt)("sup",{parentName:"h4",id:"fnref-32"},(0,o.kt)("a",{parentName:"sup",href:"#fn-32",className:"footnote-ref"},"32"))),(0,o.kt)("h4",{id:"a-taxonomy-of-prompt-modifiers-for-text-to-image-generationoppenlaender2022taxonomy"},"A Taxonomy of Prompt Modifiers for Text-To-Image Generation",(0,o.kt)("sup",{parentName:"h4",id:"fnref-33"},(0,o.kt)("a",{parentName:"sup",href:"#fn-33",className:"footnote-ref"},"33"))),(0,o.kt)("h4",{id:"diffusiondb-a-large-scale-prompt-gallery-dataset-for-text-to-image-generative-modelswang2022diffusiondb"},"DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models",(0,o.kt)("sup",{parentName:"h4",id:"fnref-34"},(0,o.kt)("a",{parentName:"sup",href:"#fn-34",className:"footnote-ref"},"34"))),(0,o.kt)("h4",{id:"optimizing-prompts-for-text-to-image-generationhao2022optimizing"},"Optimizing Prompts for Text-to-Image Generation",(0,o.kt)("sup",{parentName:"h4",id:"fnref-35"},(0,o.kt)("a",{parentName:"sup",href:"#fn-35",className:"footnote-ref"},"35"))),(0,o.kt)("h4",{id:"language-model-cascadesdohan2022language"},"Language Model Cascades",(0,o.kt)("sup",{parentName:"h4",id:"fnref-36"},(0,o.kt)("a",{parentName:"sup",href:"#fn-36",className:"footnote-ref"},"36"))),(0,o.kt)("h4",{id:"design-guidelines-for-prompt-engineering-text-to-image-generative-modelsliu2022design"},"Design Guidelines for Prompt Engineering Text-to-Image Generative Models",(0,o.kt)("sup",{parentName:"h4",id:"fnref-37"},(0,o.kt)("a",{parentName:"sup",href:"#fn-37",className:"footnote-ref"},"37"))),(0,o.kt)("h4",{id:"discovering-language-model-behaviors-with-model-written-evaluationsperez2022discovering"},"Discovering Language Model Behaviors with Model-Written Evaluations",(0,o.kt)("sup",{parentName:"h4",id:"fnref-38"},(0,o.kt)("a",{parentName:"sup",href:"#fn-38",className:"footnote-ref"},"38"))),(0,o.kt)("h4",{id:"selective-annotation-makes-language-models-better-few-shot-learnerssu2022selective"},"Selective Annotation Makes Language Models Better Few-Shot Learners",(0,o.kt)("sup",{parentName:"h4",id:"fnref-39"},(0,o.kt)("a",{parentName:"sup",href:"#fn-39",className:"footnote-ref"},"39"))),(0,o.kt)("h4",{id:"atlas-few-shot-learning-with-retrieval-augmented-language-modelsizacard2022atlas"},"Atlas: Few-shot Learning with Retrieval Augmented Language Models",(0,o.kt)("sup",{parentName:"h4",id:"fnref-40"},(0,o.kt)("a",{parentName:"sup",href:"#fn-40",className:"footnote-ref"},"40"))),(0,o.kt)("h4",{id:"strudel-structured-dialogue-summarization-for-dialogue-comprehensionwang2022strudel"},"STRUDEL: Structured Dialogue Summarization for Dialogue Comprehension",(0,o.kt)("sup",{parentName:"h4",id:"fnref-41"},(0,o.kt)("a",{parentName:"sup",href:"#fn-41",className:"footnote-ref"},"41"))),(0,o.kt)("h4",{id:"prompting-is-programming-a-query-language-for-large-language-modelsbeurerkellner2022prompting"},"Prompting Is Programming: A Query Language For Large Language Models",(0,o.kt)("sup",{parentName:"h4",id:"fnref-42"},(0,o.kt)("a",{parentName:"sup",href:"#fn-42",className:"footnote-ref"},"42"))),(0,o.kt)("h4",{id:"parallel-context-windows-improve-in-context-learning-of-large-language-modelsratner2022parallel"},"Parallel Context Windows Improve In-Context Learning of Large Language Models",(0,o.kt)("sup",{parentName:"h4",id:"fnref-43"},(0,o.kt)("a",{parentName:"sup",href:"#fn-43",className:"footnote-ref"},"43"))),(0,o.kt)("h4",{id:"learning-to-perform-complex-tasks-through-compositional-fine-tuning-of-language-modelsbursztyn2022learning"},"Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models",(0,o.kt)("sup",{parentName:"h4",id:"fnref-44"},(0,o.kt)("a",{parentName:"sup",href:"#fn-44",className:"footnote-ref"},"44"))),(0,o.kt)("h4",{id:"super-naturalinstructions-generalization-via-declarative-instructions-on-1600-nlp-taskswang2022supernaturalinstructions"},"Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks",(0,o.kt)("sup",{parentName:"h4",id:"fnref-45"},(0,o.kt)("a",{parentName:"sup",href:"#fn-45",className:"footnote-ref"},"45"))),(0,o.kt)("h4",{id:"making-pre-trained-language-models-better-few-shot-learnersgao2021making"},"Making Pre-trained Language Models Better Few-shot Learners",(0,o.kt)("sup",{parentName:"h4",id:"fnref-46"},(0,o.kt)("a",{parentName:"sup",href:"#fn-46",className:"footnote-ref"},"46"))),(0,o.kt)("h4",{id:"how-to-prompt-opportunities-and-challenges-of-zero--and-few-shot-learning-for-human-ai-interaction-in-creative-applications-of-generative-modelsdang2022prompt"},"How to Prompt? Opportunities and Challenges of Zero- and Few-Shot Learning for Human-AI Interaction in Creative Applications of Generative Models",(0,o.kt)("sup",{parentName:"h4",id:"fnref-47"},(0,o.kt)("a",{parentName:"sup",href:"#fn-47",className:"footnote-ref"},"47"))),(0,o.kt)("h4",{id:"on-measuring-social-biases-in-prompt-based-multi-task-learningakyrek2022measuring"},"On Measuring Social Biases in Prompt-Based Multi-Task Learning",(0,o.kt)("sup",{parentName:"h4",id:"fnref-48"},(0,o.kt)("a",{parentName:"sup",href:"#fn-48",className:"footnote-ref"},"48"))),(0,o.kt)("h4",{id:"plot-writing-from-pre-trained-language-modelsjin2022plot"},"Plot Writing From Pre-Trained Language Models",(0,o.kt)("sup",{parentName:"h4",id:"fnref-49"},(0,o.kt)("a",{parentName:"sup",href:"#fn-49",className:"footnote-ref"},"49"))),(0,o.kt)("h4",{id:"stereoset-measuring-stereotypical-bias-in-pretrained-language-modelsnadeem-etal-2021-stereoset"},"{S}tereo{S}et: Measuring stereotypical bias in pretrained language models",(0,o.kt)("sup",{parentName:"h4",id:"fnref-50"},(0,o.kt)("a",{parentName:"sup",href:"#fn-50",className:"footnote-ref"},"50"))),(0,o.kt)("h4",{id:"survey-of-hallucination-in-natural-language-generationji_2022"},"Survey of Hallucination in Natural Language Generation",(0,o.kt)("sup",{parentName:"h4",id:"fnref-51"},(0,o.kt)("a",{parentName:"sup",href:"#fn-51",className:"footnote-ref"},"51"))),(0,o.kt)("h4",{id:"wordcraft-story-writing-with-large-language-modelsyuan2022wordcraft"},"Wordcraft: Story Writing With Large Language Models",(0,o.kt)("sup",{parentName:"h4",id:"fnref-52"},(0,o.kt)("a",{parentName:"sup",href:"#fn-52",className:"footnote-ref"},"52"))),(0,o.kt)("h4",{id:"painpoints-a-framework-for-language-based-detection-of-chronic-pain-and-expert-collaborative-text-summarizationfadnavis2022pain"},"PainPoints: A Framework for Language-based Detection of Chronic Pain and Expert-Collaborative Text-Summarization",(0,o.kt)("sup",{parentName:"h4",id:"fnref-53"},(0,o.kt)("a",{parentName:"sup",href:"#fn-53",className:"footnote-ref"},"53"))),(0,o.kt)("h4",{id:"self-instruct-aligning-language-model-with-self-generated-instructionswang2022selfinstruct"},"Self-Instruct: Aligning Language Model with Self Generated Instructions",(0,o.kt)("sup",{parentName:"h4",id:"fnref-54"},(0,o.kt)("a",{parentName:"sup",href:"#fn-54",className:"footnote-ref"},"54"))),(0,o.kt)("h4",{id:"from-images-to-textual-prompts-zero-shot-vqa-with-frozen-large-language-modelsguo2022images"},"From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models",(0,o.kt)("sup",{parentName:"h4",id:"fnref-55"},(0,o.kt)("a",{parentName:"sup",href:"#fn-55",className:"footnote-ref"},"55"))),(0,o.kt)("h4",{id:"new-and-improved-content-moderation-toolingmarkov_2022"},"New and improved content moderation tooling",(0,o.kt)("sup",{parentName:"h4",id:"fnref-56"},(0,o.kt)("a",{parentName:"sup",href:"#fn-56",className:"footnote-ref"},"56"))),(0,o.kt)("h4",{id:"exploiting-cloze-questions-for-few-shot-text-classification-and-natural-language-inferenceschick2020exploiting"},"Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference",(0,o.kt)("sup",{parentName:"h4",id:"fnref-57"},(0,o.kt)("a",{parentName:"sup",href:"#fn-57",className:"footnote-ref"},"57"))),(0,o.kt)("h4",{id:"human-level-concept-learning-through-probabilistic-program-inductionlake2015human"},"Human-level concept learning through probabilistic program induction",(0,o.kt)("sup",{parentName:"h4",id:"fnref-58"},(0,o.kt)("a",{parentName:"sup",href:"#fn-58",className:"footnote-ref"},"58"))),(0,o.kt)("h4",{id:"riffusion---stable-diffusion-for-real-time-music-generationforsgren_martiros_2022"},"{Riffusion - Stable diffusion for real-time music generation}",(0,o.kt)("sup",{parentName:"h4",id:"fnref-59"},(0,o.kt)("a",{parentName:"sup",href:"#fn-59",className:"footnote-ref"},"59"))),(0,o.kt)("h4",{id:"how-to-use-openais-chatgpt-to-write-the-perfect-cold-emailbonta2022how"},"How to use OpenAI\u2019s ChatGPT to write the perfect cold email",(0,o.kt)("sup",{parentName:"h4",id:"fnref-60"},(0,o.kt)("a",{parentName:"sup",href:"#fn-60",className:"footnote-ref"},"60"))),(0,o.kt)("h4",{id:"cacti-biology-and-usesnobel2002cacti"},"Cacti: biology and uses",(0,o.kt)("sup",{parentName:"h4",id:"fnref-61"},(0,o.kt)("a",{parentName:"sup",href:"#fn-61",className:"footnote-ref"},"61"))),(0,o.kt)("h4",{id:"are-language-models-worse-than-humans-at-following-prompts-its-complicatedwebson2023itscomplicated"},"Are Language Models Worse than Humans at Following Prompts? It\u2019s Complicated",(0,o.kt)("sup",{parentName:"h4",id:"fnref-62"},(0,o.kt)("a",{parentName:"sup",href:"#fn-62",className:"footnote-ref"},"62"))),(0,o.kt)("h4",{id:"unleashing-cognitive-synergy-in-large-language-models-a-task-solving-agent-through-multi-persona-self-collaborationwang2023unleashing"},"Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration",(0,o.kt)("sup",{parentName:"h4",id:"fnref-63"},(0,o.kt)("a",{parentName:"sup",href:"#fn-63",className:"footnote-ref"},"63"))),(0,o.kt)("h2",{id:"prompt-hacking"},"Prompt Hacking"),(0,o.kt)("h4",{id:"machine-generated-text-a-comprehensive-survey-of-threat-models-and-detection-methodscrothers2022machine"},"Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods",(0,o.kt)("sup",{parentName:"h4",id:"fnref-64"},(0,o.kt)("a",{parentName:"sup",href:"#fn-64",className:"footnote-ref"},"64"))),(0,o.kt)("h4",{id:"new-jailbreak-based-on-virtual-functions---smuggle-illegal-tokens-to-the-backendnin2023new"},"New jailbreak based on virtual functions - smuggle illegal tokens to the backend.",(0,o.kt)("sup",{parentName:"h4",id:"fnref-65"},(0,o.kt)("a",{parentName:"sup",href:"#fn-65",className:"footnote-ref"},"65"))),(0,o.kt)("h4",{id:"exploiting-programmatic-behavior-of-llms-dual-use-through-standard-security-attackskang2023exploiting"},"Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks",(0,o.kt)("sup",{parentName:"h4",id:"fnref-66"},(0,o.kt)("a",{parentName:"sup",href:"#fn-66",className:"footnote-ref"},"66"))),(0,o.kt)("h4",{id:"more-than-youve-asked-for-a-comprehensive-analysis-of-novel-prompt-injection-threats-to-application-integrated-large-language-modelsgreshake2023youve"},"More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models",(0,o.kt)("sup",{parentName:"h4",id:"fnref-67"},(0,o.kt)("a",{parentName:"sup",href:"#fn-67",className:"footnote-ref"},"67"))),(0,o.kt)("h4",{id:"chatgpt-dan-and-other-jailbreakskiho2023chatgpt"},'ChatGPT "DAN" (and other "Jailbreaks")',(0,o.kt)("sup",{parentName:"h4",id:"fnref-68"},(0,o.kt)("a",{parentName:"sup",href:"#fn-68",className:"footnote-ref"},"68"))),(0,o.kt)("h4",{id:"evaluating-the-susceptibility-of-pre-trained-language-models-via-handcrafted-adversarial-examplesbranch2022evaluating"},"Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples",(0,o.kt)("sup",{parentName:"h4",id:"fnref-69"},(0,o.kt)("a",{parentName:"sup",href:"#fn-69",className:"footnote-ref"},"69"))),(0,o.kt)("h4",{id:"prompt-injection-attacks-against-gpt-3simon2022inject"},"Prompt injection attacks against GPT-3",(0,o.kt)("sup",{parentName:"h4",id:"fnref-70"},(0,o.kt)("a",{parentName:"sup",href:"#fn-70",className:"footnote-ref"},"70"))),(0,o.kt)("h4",{id:"exploiting-gpt-3-prompts-with-malicious-inputs-that-order-the-model-to-ignore-its-previous-directionsgoodside2022inject"},"Exploiting GPT-3 prompts with malicious inputs that order the model to ignore its previous directions",(0,o.kt)("sup",{parentName:"h4",id:"fnref-71"},(0,o.kt)("a",{parentName:"sup",href:"#fn-71",className:"footnote-ref"},"71"))),(0,o.kt)("h4",{id:"history-correctiongoodside2022history"},"History Correction",(0,o.kt)("sup",{parentName:"h4",id:"fnref-72"},(0,o.kt)("a",{parentName:"sup",href:"#fn-72",className:"footnote-ref"},"72"))),(0,o.kt)("h4",{id:"adversarial-promptschase2021adversarial"},"adversarial-prompts",(0,o.kt)("sup",{parentName:"h4",id:"fnref-73"},(0,o.kt)("a",{parentName:"sup",href:"#fn-73",className:"footnote-ref"},"73"))),(0,o.kt)("h4",{id:"gpt-3-prompt-injection-defensesgoodside2021gpt"},"GPT-3 Prompt Injection Defenses",(0,o.kt)("sup",{parentName:"h4",id:"fnref-74"},(0,o.kt)("a",{parentName:"sup",href:"#fn-74",className:"footnote-ref"},"74"))),(0,o.kt)("h4",{id:"talking-to-machines-prompt-engineering--injectionchristoph2022talking"},"Talking to machines: prompt engineering & injection",(0,o.kt)("sup",{parentName:"h4",id:"fnref-75"},(0,o.kt)("a",{parentName:"sup",href:"#fn-75",className:"footnote-ref"},"75"))),(0,o.kt)("h4",{id:"using-gpt-eliezer-against-chatgpt-jailbreakingarmstrong2022using"},"Using GPT-Eliezer against ChatGPT Jailbreaking",(0,o.kt)("sup",{parentName:"h4",id:"fnref-76"},(0,o.kt)("a",{parentName:"sup",href:"#fn-76",className:"footnote-ref"},"76"))),(0,o.kt)("h4",{id:"exploring-prompt-injection-attacksselvi2022exploring"},"Exploring Prompt Injection Attacks",(0,o.kt)("sup",{parentName:"h4",id:"fnref-77"},(0,o.kt)("a",{parentName:"sup",href:"#fn-77",className:"footnote-ref"},"77"))),(0,o.kt)("h4",{id:"the-entire-prompt-of-microsoft-bing-chat-hi-sydneykevinbing"},"The entire prompt of Microsoft Bing Chat?! (Hi, Sydney.)",(0,o.kt)("sup",{parentName:"h4",id:"fnref-78"},(0,o.kt)("a",{parentName:"sup",href:"#fn-78",className:"footnote-ref"},"78"))),(0,o.kt)("h4",{id:"ignore-previous-prompt-attack-techniques-for-language-modelsperez2022jailbreak"},"Ignore Previous Prompt: Attack Techniques For Language Models",(0,o.kt)("sup",{parentName:"h4",id:"fnref-79"},(0,o.kt)("a",{parentName:"sup",href:"#fn-79",className:"footnote-ref"},"79"))),(0,o.kt)("h4",{id:"lessons-learned-on-language-model-safety-and-misusebrundage_2022"},"Lessons learned on Language Model Safety and misuse",(0,o.kt)("sup",{parentName:"h4",id:"fnref-80"},(0,o.kt)("a",{parentName:"sup",href:"#fn-80",className:"footnote-ref"},"80"))),(0,o.kt)("h4",{id:"toxicity-detection-with-generative-prompt-based-inferencewang2022jailbreak"},"Toxicity Detection with Generative Prompt-based Inference",(0,o.kt)("sup",{parentName:"h4",id:"fnref-81"},(0,o.kt)("a",{parentName:"sup",href:"#fn-81",className:"footnote-ref"},"81"))),(0,o.kt)("h4",{id:"ok-i-saw-a-few-people-jailbreaking-safeguards-openai-put-on-chatgpt-so-i-had-to-give-it-a-shot-myselfalice2022jailbreak"},"ok I saw a few people jailbreaking safeguards openai put on chatgpt so I had to give it a shot myself",(0,o.kt)("sup",{parentName:"h4",id:"fnref-82"},(0,o.kt)("a",{parentName:"sup",href:"#fn-82",className:"footnote-ref"},"82"))),(0,o.kt)("h4",{id:"bypass-openais-chatgpt-alignment-efforts-with-this-one-weird-trickmiguel2022jailbreak"},"Bypass @OpenAI's ChatGPT alignment efforts with this one weird trick",(0,o.kt)("sup",{parentName:"h4",id:"fnref-83"},(0,o.kt)("a",{parentName:"sup",href:"#fn-83",className:"footnote-ref"},"83"))),(0,o.kt)("h4",{id:"chatgpt-jailbreaking-itselfderek2022jailbreak"},"ChatGPT jailbreaking itself",(0,o.kt)("sup",{parentName:"h4",id:"fnref-84"},(0,o.kt)("a",{parentName:"sup",href:"#fn-84",className:"footnote-ref"},"84"))),(0,o.kt)("h4",{id:"using-pretend-on-chatgpt-can-do-some-wild-stuff-you-can-kind-of-get-some-insight-on-the-future-alternative-universenero2022jailbreak"},'Using "pretend" on #ChatGPT can do some wild stuff. You can kind of get some insight on the future, alternative universe.',(0,o.kt)("sup",{parentName:"h4",id:"fnref-85"},(0,o.kt)("a",{parentName:"sup",href:"#fn-85",className:"footnote-ref"},"85"))),(0,o.kt)("h4",{id:"i-kinda-like-this-one-even-morenick2022jailbreak"},"I kinda like this one even more!",(0,o.kt)("sup",{parentName:"h4",id:"fnref-86"},(0,o.kt)("a",{parentName:"sup",href:"#fn-86",className:"footnote-ref"},"86"))),(0,o.kt)("h4",{id:"uh-ohsam2022jailbreak"},"uh oh",(0,o.kt)("sup",{parentName:"h4",id:"fnref-87"},(0,o.kt)("a",{parentName:"sup",href:"#fn-87",className:"footnote-ref"},"87"))),(0,o.kt)("h4",{id:"building-a-virtual-machine-inside-chatgptjonas2022jailbreak"},"Building A Virtual Machine inside ChatGPT",(0,o.kt)("sup",{parentName:"h4",id:"fnref-88"},(0,o.kt)("a",{parentName:"sup",href:"#fn-88",className:"footnote-ref"},"88"))),(0,o.kt)("h2",{id:"reliability"},"Reliability"),(0,o.kt)("h4",{id:"mathprompter-mathematical-reasoning-using-large-language-modelsimani2023mathprompter"},"MathPrompter: Mathematical Reasoning using Large Language Models",(0,o.kt)("sup",{parentName:"h4",id:"fnref-89"},(0,o.kt)("a",{parentName:"sup",href:"#fn-89",className:"footnote-ref"},"89"))),(0,o.kt)("h4",{id:"the-unreliability-of-explanations-in-few-shot-prompting-for-textual-reasoningye2022unreliability"},"The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning",(0,o.kt)("sup",{parentName:"h4",id:"fnref-90"},(0,o.kt)("a",{parentName:"sup",href:"#fn-90",className:"footnote-ref"},"90"))),(0,o.kt)("h4",{id:"prompting-gpt-3-to-be-reliablesi2022prompting"},"Prompting GPT-3 To Be Reliable",(0,o.kt)("sup",{parentName:"h4",id:"fnref-91"},(0,o.kt)("a",{parentName:"sup",href:"#fn-91",className:"footnote-ref"},"91"))),(0,o.kt)("h4",{id:"on-the-advance-of-making-language-models-better-reasonersli2022advance"},"On the Advance of Making Language Models Better Reasoners",(0,o.kt)("sup",{parentName:"h4",id:"fnref-92"},(0,o.kt)("a",{parentName:"sup",href:"#fn-92",className:"footnote-ref"},"92"))),(0,o.kt)("h4",{id:"ask-me-anything-a-simple-strategy-for-prompting-language-modelsarora2022ama"},"Ask Me Anything: A simple strategy for prompting language models",(0,o.kt)("sup",{parentName:"h4",id:"fnref-93"},(0,o.kt)("a",{parentName:"sup",href:"#fn-93",className:"footnote-ref"},"93"))),(0,o.kt)("h4",{id:"calibrate-before-use-improving-few-shot-performance-of-language-modelszhao2021calibrate"},"Calibrate Before Use: Improving Few-Shot Performance of Language Models",(0,o.kt)("sup",{parentName:"h4",id:"fnref-94"},(0,o.kt)("a",{parentName:"sup",href:"#fn-94",className:"footnote-ref"},"94"))),(0,o.kt)("h4",{id:"can-large-language-models-reason-about-medical-questionslivin2022large"},"Can large language models reason about medical questions?",(0,o.kt)("sup",{parentName:"h4",id:"fnref-95"},(0,o.kt)("a",{parentName:"sup",href:"#fn-95",className:"footnote-ref"},"95"))),(0,o.kt)("h4",{id:"enhancing-self-consistency-and-performance-of-pre-trained-language-models-through-natural-language-inferencemitchell2022enhancing"},"Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference",(0,o.kt)("sup",{parentName:"h4",id:"fnref-96"},(0,o.kt)("a",{parentName:"sup",href:"#fn-96",className:"footnote-ref"},"96"))),(0,o.kt)("h4",{id:"on-second-thought-lets-not-think-step-by-step-bias-and-toxicity-in-zero-shot-reasoningshaikh2022second"},"On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning",(0,o.kt)("sup",{parentName:"h4",id:"fnref-97"},(0,o.kt)("a",{parentName:"sup",href:"#fn-97",className:"footnote-ref"},"97"))),(0,o.kt)("h4",{id:"evaluating-language-models-can-be-trickychase2022evaluating"},"Evaluating language models can be tricky",(0,o.kt)("sup",{parentName:"h4",id:"fnref-98"},(0,o.kt)("a",{parentName:"sup",href:"#fn-98",className:"footnote-ref"},"98"))),(0,o.kt)("h4",{id:"constitutional-ai-harmlessness-from-ai-feedbackbai2022constitutional"},"Constitutional AI: Harmlessness from AI Feedback",(0,o.kt)("sup",{parentName:"h4",id:"fnref-99"},(0,o.kt)("a",{parentName:"sup",href:"#fn-99",className:"footnote-ref"},"99"))),(0,o.kt)("h2",{id:"surveys"},"Surveys"),(0,o.kt)("h4",{id:"speech-and-language-processing-an-introduction-to-natural-language-processing-computational-linguistics-and-speech-recognitionjurafsky2009"},"Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics and Speech Recognition",(0,o.kt)("sup",{parentName:"h4",id:"fnref-100"},(0,o.kt)("a",{parentName:"sup",href:"#fn-100",className:"footnote-ref"},"100"))),(0,o.kt)("h4",{id:"pre-train-prompt-and-predict-a-systematic-survey-of-prompting-methods-in-natural-language-processingliu2021pretrain"},"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",(0,o.kt)("sup",{parentName:"h4",id:"fnref-101"},(0,o.kt)("a",{parentName:"sup",href:"#fn-101",className:"footnote-ref"},"101"))),(0,o.kt)("h4",{id:"promptpapersning2022papers"},"PromptPapers",(0,o.kt)("sup",{parentName:"h4",id:"fnref-102"},(0,o.kt)("a",{parentName:"sup",href:"#fn-102",className:"footnote-ref"},"102"))),(0,o.kt)("h4",{id:"a-prompt-pattern-catalog-to-enhance-prompt-engineering-with-chatgptwhite2023prompt"},"A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT",(0,o.kt)("sup",{parentName:"h4",id:"fnref-103"},(0,o.kt)("a",{parentName:"sup",href:"#fn-103",className:"footnote-ref"},"103"))),(0,o.kt)("h2",{id:"techniques"},"Techniques"),(0,o.kt)("h4",{id:"chain-of-thought-prompting-elicits-reasoning-in-large-language-modelswei2022chain"},"Chain of Thought Prompting Elicits Reasoning in Large Language Models",(0,o.kt)("sup",{parentName:"h4",id:"fnref-104"},(0,o.kt)("a",{parentName:"sup",href:"#fn-104",className:"footnote-ref"},"104"))),(0,o.kt)("h4",{id:"large-language-models-are-zero-shot-reasonerskojima2022large"},"Large Language Models are Zero-Shot Reasoners",(0,o.kt)("sup",{parentName:"h4",id:"fnref-105"},(0,o.kt)("a",{parentName:"sup",href:"#fn-105",className:"footnote-ref"},"105"))),(0,o.kt)("h4",{id:"self-consistency-improves-chain-of-thought-reasoning-in-language-modelswang2022selfconsistency"},"Self-Consistency Improves Chain of Thought Reasoning in Language Models",(0,o.kt)("sup",{parentName:"h4",id:"fnref-106"},(0,o.kt)("a",{parentName:"sup",href:"#fn-106",className:"footnote-ref"},"106"))),(0,o.kt)("h4",{id:"what-makes-good-in-context-examples-for-gpt-3liu2021makes"},"What Makes Good In-Context Examples for GPT-3?",(0,o.kt)("sup",{parentName:"h4",id:"fnref-107"},(0,o.kt)("a",{parentName:"sup",href:"#fn-107",className:"footnote-ref"},"107"))),(0,o.kt)("h4",{id:"generated-knowledge-prompting-for-commonsense-reasoningliu2021generated"},"Generated Knowledge Prompting for Commonsense Reasoning",(0,o.kt)("sup",{parentName:"h4",id:"fnref-108"},(0,o.kt)("a",{parentName:"sup",href:"#fn-108",className:"footnote-ref"},"108"))),(0,o.kt)("h4",{id:"recitation-augmented-language-modelssun2022recitationaugmented"},"Recitation-Augmented Language Models",(0,o.kt)("sup",{parentName:"h4",id:"fnref-109"},(0,o.kt)("a",{parentName:"sup",href:"#fn-109",className:"footnote-ref"},"109"))),(0,o.kt)("h4",{id:"rethinking-the-role-of-demonstrations-what-makes-in-context-learning-workmin2022rethinking"},"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",(0,o.kt)("sup",{parentName:"h4",id:"fnref-110"},(0,o.kt)("a",{parentName:"sup",href:"#fn-110",className:"footnote-ref"},"110"))),(0,o.kt)("h4",{id:"show-your-work-scratchpads-for-intermediate-computation-with-language-modelsnye2021work"},"Show Your Work: Scratchpads for Intermediate Computation with Language Models",(0,o.kt)("sup",{parentName:"h4",id:"fnref-111"},(0,o.kt)("a",{parentName:"sup",href:"#fn-111",className:"footnote-ref"},"111"))),(0,o.kt)("h4",{id:"maieutic-prompting-logically-consistent-reasoning-with-recursive-explanationsjung2022maieutic"},"Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations",(0,o.kt)("sup",{parentName:"h4",id:"fnref-112"},(0,o.kt)("a",{parentName:"sup",href:"#fn-112",className:"footnote-ref"},"112"))),(0,o.kt)("h4",{id:"star-bootstrapping-reasoning-with-reasoningzelikman2022star"},"STaR: Bootstrapping Reasoning With Reasoning",(0,o.kt)("sup",{parentName:"h4",id:"fnref-113"},(0,o.kt)("a",{parentName:"sup",href:"#fn-113",className:"footnote-ref"},"113"))),(0,o.kt)("h4",{id:"least-to-most-prompting-enables-complex-reasoning-in-large-language-modelszhou2022leasttomost"},"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",(0,o.kt)("sup",{parentName:"h4",id:"fnref-114"},(0,o.kt)("a",{parentName:"sup",href:"#fn-114",className:"footnote-ref"},"114"))),(0,o.kt)("h4",{id:"reframing-instructional-prompts-to-gptks-languagemishra2022reframing"},"Reframing Instructional Prompts to GPTk\u2019s Language",(0,o.kt)("sup",{parentName:"h4",id:"fnref-115"},(0,o.kt)("a",{parentName:"sup",href:"#fn-115",className:"footnote-ref"},"115"))),(0,o.kt)("h4",{id:"cutting-down-on-prompts-and-parameters-simple-few-shot-learning-with-language-modelslogan-iv-etal-2022-cutting"},"Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models",(0,o.kt)("sup",{parentName:"h4",id:"fnref-116"},(0,o.kt)("a",{parentName:"sup",href:"#fn-116",className:"footnote-ref"},"116"))),(0,o.kt)("h4",{id:"role-play-with-large-language-modelsshanahan2023roleplay"},"Role-Play with Large Language Models",(0,o.kt)("sup",{parentName:"h4",id:"fnref-117"},(0,o.kt)("a",{parentName:"sup",href:"#fn-117",className:"footnote-ref"},"117"))),(0,o.kt)("h4",{id:"camel-communicative-agents-for-mind-exploration-of-large-scale-language-model-societyli2023camel"},'CAMEL: Communicative Agents for "Mind" Exploration of Large Scale Language Model Society',(0,o.kt)("sup",{parentName:"h4",id:"fnref-118"},(0,o.kt)("a",{parentName:"sup",href:"#fn-118",className:"footnote-ref"},"118"))),(0,o.kt)("h4",{id:"teler-a-general-taxonomy-of-llm-prompts-for-benchmarking-complex-taskssantu2023teler"},"TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks",(0,o.kt)("sup",{parentName:"h4",id:"fnref-119"},(0,o.kt)("a",{parentName:"sup",href:"#fn-119",className:"footnote-ref"},"119"))),(0,o.kt)("h2",{id:"models"},"Models"),(0,o.kt)("h3",{id:"image-models"},"Image Models"),(0,o.kt)("h4",{id:"stable-diffusionrombach2021highresolution"},"Stable Diffusion",(0,o.kt)("sup",{parentName:"h4",id:"fnref-120"},(0,o.kt)("a",{parentName:"sup",href:"#fn-120",className:"footnote-ref"},"120"))),(0,o.kt)("h4",{id:"dalleramesh2022hierarchical"},"DALLE",(0,o.kt)("sup",{parentName:"h4",id:"fnref-121"},(0,o.kt)("a",{parentName:"sup",href:"#fn-121",className:"footnote-ref"},"121"))),(0,o.kt)("h3",{id:"language-models"},"Language Models"),(0,o.kt)("h4",{id:"chatgptchatgpt2022"},"ChatGPT",(0,o.kt)("sup",{parentName:"h4",id:"fnref-122"},(0,o.kt)("a",{parentName:"sup",href:"#fn-122",className:"footnote-ref"},"122"))),(0,o.kt)("h4",{id:"gpt-3brown2020language"},"GPT-3",(0,o.kt)("sup",{parentName:"h4",id:"fnref-123"},(0,o.kt)("a",{parentName:"sup",href:"#fn-123",className:"footnote-ref"},"123"))),(0,o.kt)("h4",{id:"instruct-gptouyang2022training"},"Instruct GPT",(0,o.kt)("sup",{parentName:"h4",id:"fnref-124"},(0,o.kt)("a",{parentName:"sup",href:"#fn-124",className:"footnote-ref"},"124"))),(0,o.kt)("h4",{id:"gpt-4openai2023gpt4"},"GPT-4",(0,o.kt)("sup",{parentName:"h4",id:"fnref-125"},(0,o.kt)("a",{parentName:"sup",href:"#fn-125",className:"footnote-ref"},"125"))),(0,o.kt)("h4",{id:"palm-scaling-language-modeling-with-pathwayschowdhery2022palm"},"PaLM: Scaling Language Modeling with Pathways",(0,o.kt)("sup",{parentName:"h4",id:"fnref-126"},(0,o.kt)("a",{parentName:"sup",href:"#fn-126",className:"footnote-ref"},"126"))),(0,o.kt)("h4",{id:"bloom-a-176b-parameter-open-access-multilingual-language-modelscao2022bloom"},"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model",(0,o.kt)("sup",{parentName:"h4",id:"fnref-127"},(0,o.kt)("a",{parentName:"sup",href:"#fn-127",className:"footnote-ref"},"127"))),(0,o.kt)("h4",{id:"bloom1-adding-language-support-to-bloom-for-zero-shot-promptingyong2022bloom1"},"BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting",(0,o.kt)("sup",{parentName:"h4",id:"fnref-128"},(0,o.kt)("a",{parentName:"sup",href:"#fn-128",className:"footnote-ref"},"128"))),(0,o.kt)("h4",{id:"jurassic-1-technical-details-and-evaluation-white-paper-ai21-labs-2021lieberjurassic"},"Jurassic-1: Technical Details and Evaluation, White paper, AI21 Labs, 2021",(0,o.kt)("sup",{parentName:"h4",id:"fnref-129"},(0,o.kt)("a",{parentName:"sup",href:"#fn-129",className:"footnote-ref"},"129"))),(0,o.kt)("h4",{id:"gpt-j-6b-a-6-billion-parameter-autoregressive-language-modelwange2021gptj"},"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model",(0,o.kt)("sup",{parentName:"h4",id:"fnref-130"},(0,o.kt)("a",{parentName:"sup",href:"#fn-130",className:"footnote-ref"},"130"))),(0,o.kt)("h4",{id:"roberta-a-robustly-optimized-bert-pretraining-approachliu2019roberta"},"Roberta: A robustly optimized bert pretraining approach",(0,o.kt)("sup",{parentName:"h4",id:"fnref-131"},(0,o.kt)("a",{parentName:"sup",href:"#fn-131",className:"footnote-ref"},"131"))),(0,o.kt)("h2",{id:"tooling"},"Tooling"),(0,o.kt)("h3",{id:"ides"},"Ides"),(0,o.kt)("h4",{id:"textbox-20-a-text-generation-library-with-pre-trained-language-modelstang2022textbox"},"TextBox 2.0: A Text Generation Library with Pre-trained Language Models",(0,o.kt)("sup",{parentName:"h4",id:"fnref-132"},(0,o.kt)("a",{parentName:"sup",href:"#fn-132",className:"footnote-ref"},"132"))),(0,o.kt)("h4",{id:"interactive-and-visual-prompt-engineering-for-ad-hoc-task-adaptation-with-large-language-modelsstrobelt2022promptide"},"Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models",(0,o.kt)("sup",{parentName:"h4",id:"fnref-133"},(0,o.kt)("a",{parentName:"sup",href:"#fn-133",className:"footnote-ref"},"133"))),(0,o.kt)("h4",{id:"promptsource-an-integrated-development-environment-and-repository-for-natural-language-promptsbach2022promptsource"},"PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts",(0,o.kt)("sup",{parentName:"h4",id:"fnref-134"},(0,o.kt)("a",{parentName:"sup",href:"#fn-134",className:"footnote-ref"},"134"))),(0,o.kt)("h4",{id:"promptchainer-chaining-large-language-model-prompts-through-visual-programmingwu2022promptchainer"},"PromptChainer: Chaining Large Language Model Prompts through Visual Programming",(0,o.kt)("sup",{parentName:"h4",id:"fnref-135"},(0,o.kt)("a",{parentName:"sup",href:"#fn-135",className:"footnote-ref"},"135"))),(0,o.kt)("h4",{id:"openprompt-an-open-source-framework-for-prompt-learningding2021openprompt"},"OpenPrompt: An Open-source Framework for Prompt-learning",(0,o.kt)("sup",{parentName:"h4",id:"fnref-136"},(0,o.kt)("a",{parentName:"sup",href:"#fn-136",className:"footnote-ref"},"136"))),(0,o.kt)("h4",{id:"promptmaker-prompt-based-prototyping-with-largelanguagemodelsjiang2022promptmaker"},"PromptMaker: Prompt-Based Prototyping with Large","\xa0","Language","\xa0","Models",(0,o.kt)("sup",{parentName:"h4",id:"fnref-137"},(0,o.kt)("a",{parentName:"sup",href:"#fn-137",className:"footnote-ref"},"137"))),(0,o.kt)("h3",{id:"tools"},"Tools"),(0,o.kt)("h4",{id:"langchainchase_langchain_2022"},"LangChain",(0,o.kt)("sup",{parentName:"h4",id:"fnref-138"},(0,o.kt)("a",{parentName:"sup",href:"#fn-138",className:"footnote-ref"},"138"))),(0,o.kt)("h4",{id:"gpt-indexliu_gpt_index_2022"},"GPT Index",(0,o.kt)("sup",{parentName:"h4",id:"fnref-139"},(0,o.kt)("a",{parentName:"sup",href:"#fn-139",className:"footnote-ref"},"139"))),(0,o.kt)("div",{className:"footnotes"},(0,o.kt)("hr",{parentName:"div"}),(0,o.kt)("ol",{parentName:"div"},(0,o.kt)("li",{parentName:"ol",id:"fn-1"},"Karpas, E., Abend, O., Belinkov, Y., Lenz, B., Lieber, O., Ratner, N., Shoham, Y., Bata, H., Levine, Y., Leyton-Brown, K., Muhlgay, D., Rozen, N., Schwartz, E., Shachaf, G., Shalev-Shwartz, S., Shashua, A., & Tenenholtz, M. (2022).\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-1",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-2"},"Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., & Cao, Y. (2022).\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-2",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-3"},"Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., Callan, J., & Neubig, G. (2022).\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-3",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-4"},"Significant-Gravitas. (2023). https://news.agpt.co/\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-4",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-5"},"Nakajima, Y. (2023). https://github.com/yoheinakajima/babyagi\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-5",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-6"},"Reworkd.ai. (2023). https://github.com/reworkd/AgentGPT\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-6",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-7"},"Schick, T., Dwivedi-Yu, J., Dess\xec, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N., & Scialom, T. (2023).\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-7",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-8"},"Shin, T., Razeghi, Y., Logan IV, R. L., Wallace, E., & Singh, S. (2020). AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). https://doi.org/10.18653/v1/2020.emnlp-main.346\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-8",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-9"},"Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., & Ba, J. (2022). Large Language Models Are Human-Level Prompt Engineers.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-9",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-10"},"Lester, B., Al-Rfou, R., & Constant, N. (2021). The Power of Scale for Parameter-Efficient Prompt Tuning.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-10",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-11"},"Khashabi, D., Lyu, S., Min, S., Qin, L., Richardson, K., Welleck, S., Hajishirzi, H., Khot, T., Sabharwal, A., Singh, S., & Choi, Y. (2021). Prompt Waywardness: The Curious Case of Discretized Interpretation of Continuous Prompts.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-11",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-12"},"Lake, B. M., & Baroni, M. (2018). Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks. https://doi.org/10.48550/arXiv.1711.00350\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-12",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-13"},"Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., & Schulman, J. (2021). Training Verifiers to Solve Math Word Problems.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-13",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-14"},"Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov, R., & Manning, C. D. (2018). HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-14",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-15"},"Roy, S., & Roth, D. (2015). Solving General Arithmetic Word Problems. Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 1743\u20131752. https://doi.org/10.18653/v1/D15-1202\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-15",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-16"},"Thorne, J., Vlachos, A., Christodoulopoulos, C., & Mittal, A. (2018). FEVER: a large-scale dataset for Fact Extraction and VERification.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-16",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-17"},"Parrish, A., Chen, A., Nangia, N., Padmakumar, V., Phang, J., Thompson, J., Htut, P. M., & Bowman, S. R. (2021). BBQ: A Hand-Built Bias Benchmark for Question Answering.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-17",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-18"},"Roose, K. (2022). Don\u2019t ban chatgpt in schools. teach with it. https://www.nytimes.com/2023/01/12/technology/chatgpt-schools-teachers.html\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-18",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-19"},"Lipman, J., & Distler, R. (2023). Schools Shouldn\u2019t Ban Access to ChatGPT. https://time.com/6246574/schools-shouldnt-ban-access-to-chatgpt/\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-19",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-20"},"Bansal, A., yeh Ping-Chiang, Curry, M., Jain, R., Wigington, C., Manjunatha, V., Dickerson, J. P., & Goldstein, T. (2022). Certified Neural Network Watermarks with Randomized Smoothing.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-20",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-21"},"Gu, C., Huang, C., Zheng, X., Chang, K.-W., & Hsieh, C.-J. (2022). Watermarking Pre-trained Language Models with Backdooring.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-21",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-22"},"Noonan, E., & Averill, O. (2023). GW preparing disciplinary response to AI programs as faculty explore educational use. https://www.gwhatchet.com/2023/01/17/gw-preparing-disciplinary-response-to-ai-programs-as-faculty-explore-educational-use/\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-22",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-23"},"Kirchenbauer, J., Geiping, J., Wen, Y., Katz, J., Miers, I., & Goldstein, T. (2023). A Watermark for Large Language Models. https://arxiv.org/abs/2301.10226\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-23",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-24"},"Mitchell, E., Lee, Y., Khazatsky, A., Manning, C., & Finn, C. (2023). DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature. https://doi.org/10.48550/arXiv.2301.11305\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-24",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-25"},"Oppenlaender, J. (2022). Prompt Engineering for Text-Based Generative Art.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-25",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-26"},"Parsons, G. (2022). The DALLE 2 Prompt Book. https://dallery.gallery/the-dalle-2-prompt-book/\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-26",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-27"},"Blake. (2022). With the right prompt, Stable Diffusion 2.0 can do hands. https://www.reddit.com/r/StableDiffusion/comments/z7salo/with_the_right_prompt_stable_diffusion_20_can_do/\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-27",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-28"},"Davenport, T. H., & Mittal, N. (2022). How Generative AI Is Changing Creative Work. Harvard Business Review. https://hbr.org/2022/11/how-generative-ai-is-changing-creative-work\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-28",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-29"},"Captain, S. (2023). How AI Will Change the Workplace. Wall Street Journal. https://www.wsj.com/articles/how-ai-change-workplace-af2162ee\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-29",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-30"},"Verma, P., & Vynck, G. D. (2023). ChatGPT took their jobs. Now they walk dogs and fix air conditioners. Washington Post. https://www.washingtonpost.com/technology/2023/06/02/ai-taking-jobs/\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-30",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-31"},"Ford, B. (2023). Bloomberg.Com. https://www.bloomberg.com/news/articles/2023-05-01/ibm-to-pause-hiring-for-back-office-jobs-that-ai-could-kill\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-31",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-32"},"Efrat, A., & Levy, O. (2020). The Turking Test: Can Language Models Understand Instructions?\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-32",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-33"},"Oppenlaender, J. (2022). A Taxonomy of Prompt Modifiers for Text-To-Image Generation.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-33",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-34"},"Wang, Z. J., Montoya, E., Munechika, D., Yang, H., Hoover, B., & Chau, D. H. (2022). DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-34",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-35"},"Hao, Y., Chi, Z., Dong, L., & Wei, F. (2022). Optimizing Prompts for Text-to-Image Generation.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-35",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-36"},"Dohan, D., Xu, W., Lewkowycz, A., Austin, J., Bieber, D., Lopes, R. G., Wu, Y., Michalewski, H., Saurous, R. A., Sohl-dickstein, J., Murphy, K., & Sutton, C. (2022). Language Model Cascades.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-36",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-37"},"Liu, V., & Chilton, L. B. (2022). Design Guidelines for Prompt Engineering Text-to-Image Generative Models. Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3491102.3501825\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-37",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-38"},"Perez, E., Ringer, S., Luko\u0161i\u016bt\u0117, K., Nguyen, K., Chen, E., Heiner, S., Pettit, C., Olsson, C., Kundu, S., Kadavath, S., Jones, A., Chen, A., Mann, B., Israel, B., Seethor, B., McKinnon, C., Olah, C., Yan, D., Amodei, D., \u2026 Kaplan, J. (2022). Discovering Language Model Behaviors with Model-Written Evaluations.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-38",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-39"},"Su, H., Kasai, J., Wu, C. H., Shi, W., Wang, T., Xin, J., Zhang, R., Ostendorf, M., Zettlemoyer, L., Smith, N. A., & Yu, T. (2022). Selective Annotation Makes Language Models Better Few-Shot Learners.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-39",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-40"},"Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., & Grave, E. (2022). Atlas: Few-shot Learning with Retrieval Augmented Language Models.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-40",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-41"},"Wang, B., Feng, C., Nair, A., Mao, M., Desai, J., Celikyilmaz, A., Li, H., Mehdad, Y., & Radev, D. (2022). STRUDEL: Structured Dialogue Summarization for Dialogue Comprehension.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-41",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-42"},"Beurer-Kellner, L., Fischer, M., & Vechev, M. (2022). Prompting Is Programming: A Query Language For Large Language Models.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-42",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-43"},"Ratner, N., Levine, Y., Belinkov, Y., Ram, O., Abend, O., Karpas, E., Shashua, A., Leyton-Brown, K., & Shoham, Y. (2022). Parallel Context Windows Improve In-Context Learning of Large Language Models.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-43",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-44"},"Bursztyn, V. S., Demeter, D., Downey, D., & Birnbaum, L. (2022). Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-44",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-45"},"Wang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y., Mirzaei, A., Arunkumar, A., Ashok, A., Dhanasekaran, A. S., Naik, A., Stap, D., Pathak, E., Karamanolakis, G., Lai, H. G., Purohit, I., Mondal, I., Anderson, J., Kuznia, K., Doshi, K., Patel, M., \u2026 Khashabi, D. (2022). Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-45",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-46"},"Gao, T., Fisch, A., & Chen, D. (2021). Making Pre-trained Language Models Better Few-shot Learners. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). https://doi.org/10.18653/v1/2021.acl-long.295\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-46",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-47"},"Dang, H., Mecke, L., Lehmann, F., Goller, S., & Buschek, D. (2022). How to Prompt? Opportunities and Challenges of Zero- and Few-Shot Learning for Human-AI Interaction in Creative Applications of Generative Models.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-47",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-48"},"Aky\xfcrek, A. F., Paik, S., Kocyigit, M. Y., Akbiyik, S., Runyun, \u015e. L., & Wijaya, D. (2022). On Measuring Social Biases in Prompt-Based Multi-Task Learning.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-48",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-49"},"Jin, Y., Kadam, V., & Wanvarie, D. (2022). Plot Writing From Pre-Trained Language Models.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-49",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-50"},"Nadeem, M., Bethke, A., & Reddy, S. (2021). StereoSet: Measuring stereotypical bias in pretrained language models. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 5356\u20135371. https://doi.org/10.18653/v1/2021.acl-long.416\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-50",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-51"},"Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y., Madotto, A., & Fung, P. (2022). Survey of Hallucination in Natural Language Generation. ACM Computing Surveys. https://doi.org/10.1145/3571730\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-51",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-52"},"Yuan, A., Coenen, A., Reif, E., & Ippolito, D. (2022). Wordcraft: Story Writing With Large Language Models. 27th International Conference on Intelligent User Interfaces, 841\u2013852.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-52",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-53"},"Fadnavis, S., Dhurandhar, A., Norel, R., Reinen, J. M., Agurto, C., Secchettin, E., Schweiger, V., Perini, G., & Cecchi, G. (2022). PainPoints: A Framework for Language-based Detection of Chronic Pain and Expert-Collaborative Text-Summarization. arXiv Preprint arXiv:2209.09814.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-53",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-54"},"Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., & Hajishirzi, H. (2022). Self-Instruct: Aligning Language Model with Self Generated Instructions.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-54",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-55"},"Guo, J., Li, J., Li, D., Tiong, A. M. H., Li, B., Tao, D., & Hoi, S. C. H. (2022). From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-55",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-56"},"Markov, T. (2022). New and improved content moderation tooling. In OpenAI. OpenAI. https://openai.com/blog/new-and-improved-content-moderation-tooling/\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-56",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-57"},"Schick, T., & Sch\xfctze, H. (2020). Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-57",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-58"},"Lake, B. M., Salakhutdinov, R., & Tenenbaum, J. B. (2015). Human-level concept learning through probabilistic program induction. Science, 350(6266), 1332\u20131338.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-58",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-59"},"Forsgren, S., & Martiros, H. (2022). Riffusion - Stable diffusion for real-time music generation. https://riffusion.com/about\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-59",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-60"},"Bonta, A. (2022). How to use OpenAI\u2019s ChatGPT to write the perfect cold email. https://www.streak.com/post/how-to-use-ai-to-write-perfect-cold-emails\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-60",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-61"},"Nobel, P. S., & others. (2002). Cacti: biology and uses. Univ of California Press.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-61",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-62"},"Webson, A., Loo, A. M., Yu, Q., & Pavlick, E. (2023). Are Language Models Worse than Humans at Following Prompts? It\u2019s Complicated. arXiv:2301.07085v1 [Cs.CL].\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-62",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-63"},"Wang, Z., Mao, S., Wu, W., Ge, T., Wei, F., & Ji, H. (2023). Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-63",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-64"},"Crothers, E., Japkowicz, N., & Viktor, H. (2022). Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-64",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-65"},"u/Nin_kat. (2023). New jailbreak based on virtual functions - smuggle illegal tokens to the backend. https://www.reddit.com/r/ChatGPT/comments/10urbdj/new_jailbreak_based_on_virtual_functions_smuggle\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-65",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-66"},"Kang, D., Li, X., Stoica, I., Guestrin, C., Zaharia, M., & Hashimoto, T. (2023). Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-66",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-67"},"Greshake, K., Abdelnabi, S., Mishra, S., Endres, C., Holz, T., & Fritz, M. (2023). More than you\u2019ve asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-67",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-68"},"KIHO, L. (2023). ChatGPT \u201cDAN\u201d (and other \u201cJailbreaks\u201d). https://github.com/0xk1h0/ChatGPT_DAN\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-68",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-69"},"Branch, H. J., Cefalu, J. R., McHugh, J., Hujer, L., Bahl, A., del Castillo Iglesias, D., Heichman, R., & Darwishi, R. (2022). Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-69",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-70"},"Willison, S. (2022). Prompt injection attacks against GPT-3. https://simonwillison.net/2022/Sep/12/prompt-injection/\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-70",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-71"},"Goodside, R. (2022). Exploiting GPT-3 prompts with malicious inputs that order the model to ignore its previous directions. https://twitter.com/goodside/status/1569128808308957185\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-71",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-72"},"Goodside, R. (2023). History Correction. https://twitter.com/goodside/status/1610110111791325188?s=20&t=ulviQABPXFIIt4ZNZPAUCQ\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-72",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-73"},"Chase, H. (2022). adversarial-prompts. https://github.com/hwchase17/adversarial-prompts\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-73",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-74"},"Goodside, R. (2022). GPT-3 Prompt Injection Defenses. https://twitter.com/goodside/status/1578278974526222336?s=20&t=3UMZB7ntYhwAk3QLpKMAbw\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-74",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-75"},"Mark, C. (2022). Talking to machines: prompt engineering & injection. https://artifact-research.com/artificial-intelligence/talking-to-machines-prompt-engineering-injection/\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-75",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-76"},"Stuart Armstrong, R. G. (2022). Using GPT-Eliezer against ChatGPT Jailbreaking. https://www.alignmentforum.org/posts/pNcFYZnPdXyL2RfgA/using-gpt-eliezer-against-chatgpt-jailbreaking\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-76",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-77"},"Selvi, J. (2022). Exploring Prompt Injection Attacks. https://research.nccgroup.com/2022/12/05/exploring-prompt-injection-attacks/\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-77",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-78"},"Liu, K. (2023). The entire prompt of Microsoft Bing Chat?! (Hi, Sydney.). https://twitter.com/kliu128/status/1623472922374574080\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-78",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-79"},"Perez, F., & Ribeiro, I. (2022). Ignore Previous Prompt: Attack Techniques For Language Models. arXiv. https://doi.org/10.48550/ARXIV.2211.09527\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-79",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-80"},"Brundage, M. (2022). Lessons learned on Language Model Safety and misuse. In OpenAI. OpenAI. https://openai.com/blog/language-model-safety-and-misuse/\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-80",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-81"},"Wang, Y.-S., & Chang, Y. (2022). Toxicity Detection with Generative Prompt-based Inference. arXiv. https://doi.org/10.48550/ARXIV.2205.12390\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-81",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-82"},"Maz, A. (2022). ok I saw a few people jailbreaking safeguards openai put on chatgpt so I had to give it a shot myself. https://twitter.com/alicemazzy/status/1598288519301976064\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-82",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-83"},"Piedrafita, M. (2022). Bypass @OpenAI\u2019s ChatGPT alignment efforts with this one weird trick. https://twitter.com/m1guelpf/status/1598203861294252033\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-83",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-84"},"Parfait, D. (2022). ChatGPT jailbreaking itself. https://twitter.com/haus_cole/status/1598541468058390534\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-84",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-85"},"Soares, N. (2022). Using \u201cpretend\u201d on #ChatGPT can do some wild stuff. You can kind of get some insight on the future, alternative universe. https://twitter.com/NeroSoares/status/1608527467265904643\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-85",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-86"},"Moran, N. (2022). I kinda like this one even more! https://twitter.com/NickEMoran/status/1598101579626057728\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-86",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-87"},"samczsun. (2022). uh oh. https://twitter.com/samczsun/status/1598679658488217601\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-87",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-88"},"Degrave, J. (2022). Building A Virtual Machine inside ChatGPT. Engraved. https://www.engraved.blog/building-a-virtual-machine-inside/\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-88",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-89"},"Imani, S., Du, L., & Shrivastava, H. (2023). MathPrompter: Mathematical Reasoning using Large Language Models.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-89",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-90"},"Ye, X., & Durrett, G. (2022). The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-90",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-91"},"Si, C., Gan, Z., Yang, Z., Wang, S., Wang, J., Boyd-Graber, J., & Wang, L. (2022). Prompting GPT-3 To Be Reliable.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-91",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-92"},"Li, Y., Lin, Z., Zhang, S., Fu, Q., Chen, B., Lou, J.-G., & Chen, W. (2022). On the Advance of Making Language Models Better Reasoners.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-92",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-93"},"Arora, S., Narayan, A., Chen, M. F., Orr, L., Guha, N., Bhatia, K., Chami, I., Sala, F., & R\xe9, C. (2022). Ask Me Anything: A simple strategy for prompting language models.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-93",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-94"},"Zhao, T. Z., Wallace, E., Feng, S., Klein, D., & Singh, S. (2021). Calibrate Before Use: Improving Few-Shot Performance of Language Models.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-94",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-95"},"Li\xe9vin, V., Hother, C. E., & Winther, O. (2022). Can large language models reason about medical questions?\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-95",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-96"},"Mitchell, E., Noh, J. J., Li, S., Armstrong, W. S., Agarwal, A., Liu, P., Finn, C., & Manning, C. D. (2022). Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-96",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-97"},"Shaikh, O., Zhang, H., Held, W., Bernstein, M., & Yang, D. (2022). On Second Thought, Let\u2019s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-97",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-98"},"Chase, H. (2022). Evaluating language models can be tricky. https://twitter.com/hwchase17/status/1607428141106008064\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-98",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-99"},"Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., Chen, C., Olsson, C., Olah, C., Hernandez, D., Drain, D., Ganguli, D., Li, D., Tran-Johnson, E., Perez, E., \u2026 Kaplan, J. (2022). Constitutional AI: Harmlessness from AI Feedback.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-99",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-100"},"Jurafsky, D., & Martin, J. H. (2009). Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics and Speech Recognition. Prentice Hall.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-100",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-101"},"Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., & Neubig, G. (2022). Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. ACM Computing Surveys. https://doi.org/10.1145/3560815\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-101",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-102"},"Ding, N., & Hu, S. (2022). PromptPapers. https://github.com/thunlp/PromptPapers\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-102",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-103"},"White, J., Fu, Q., Hays, S., Sandborn, M., Olea, C., Gilbert, H., Elnashar, A., Spencer-Smith, J., & Schmidt, D. C. (2023). A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-103",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-104"},"Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., & Zhou, D. (2022). Chain of Thought Prompting Elicits Reasoning in Large Language Models.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-104",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-105"},"Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., & Iwasawa, Y. (2022). Large Language Models are Zero-Shot Reasoners.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-105",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-106"},"Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., & Zhou, D. (2022). Self-Consistency Improves Chain of Thought Reasoning in Language Models.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-106",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-107"},"Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., & Chen, W. (2022). What Makes Good In-Context Examples for GPT-3? Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. https://doi.org/10.18653/v1/2022.deelio-1.10\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-107",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-108"},"Liu, J., Liu, A., Lu, X., Welleck, S., West, P., Bras, R. L., Choi, Y., & Hajishirzi, H. (2021). Generated Knowledge Prompting for Commonsense Reasoning.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-108",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-109"},"Sun, Z., Wang, X., Tay, Y., Yang, Y., & Zhou, D. (2022). Recitation-Augmented Language Models.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-109",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-110"},"Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., & Zettlemoyer, L. (2022). Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-110",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-111"},"Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., Sutton, C., & Odena, A. (2021). Show Your Work: Scratchpads for Intermediate Computation with Language Models.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-111",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-112"},"Jung, J., Qin, L., Welleck, S., Brahman, F., Bhagavatula, C., Bras, R. L., & Choi, Y. (2022). Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-112",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-113"},"Zelikman, E., Wu, Y., Mu, J., & Goodman, N. D. (2022). STaR: Bootstrapping Reasoning With Reasoning.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-113",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-114"},"Zhou, D., Sch\xe4rli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Cui, C., Bousquet, O., Le, Q., & Chi, E. (2022). Least-to-Most Prompting Enables Complex Reasoning in Large Language Models.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-114",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-115"},"Mishra, S., Khashabi, D., Baral, C., Choi, Y., & Hajishirzi, H. (2022). Reframing Instructional Prompts to GPTk\u2019s Language. Findings of the Association for Computational Linguistics: ACL 2022. https://doi.org/10.18653/v1/2022.findings-acl.50\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-115",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-116"},"Logan IV, R., Balazevic, I., Wallace, E., Petroni, F., Singh, S., & Riedel, S. (2022). Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models. Findings of the Association for Computational Linguistics: ACL 2022, 2824\u20132835. https://doi.org/10.18653/v1/2022.findings-acl.222\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-116",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-117"},"Shanahan, M., McDonell, K., & Reynolds, L. (2023). Role-Play with Large Language Models.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-117",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-118"},"Li, G., Hammoud, H. A. A. K., Itani, H., Khizbullin, D., & Ghanem, B. (2023). CAMEL: Communicative Agents for \u201cMind\u201d Exploration of Large Scale Language Model Society.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-118",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-119"},"Santu, S. K. K., & Feng, D. (2023). TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-119",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-120"},"Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2021). High-Resolution Image Synthesis with Latent Diffusion Models.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-120",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-121"},"Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., & Chen, M. (2022). Hierarchical Text-Conditional Image Generation with CLIP Latents.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-121",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-122"},"OpenAI. (2022). ChatGPT: Optimizing Language Models for Dialogue. https://openai.com/blog/chatgpt/. https://openai.com/blog/chatgpt/\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-122",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-123"},"Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., \u2026 Amodei, D. (2020). Language Models are Few-Shot Learners.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-123",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-124"},"Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., & Lowe, R. (2022). Training language models to follow instructions with human feedback.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-124",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-125"},"OpenAI. (2023). GPT-4 Technical Report.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-125",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-126"},"Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., \u2026 Fiedel, N. (2022). PaLM: Scaling Language Modeling with Pathways.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-126",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-127"},"Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ili\u0107, S., Hesslow, D., Castagn\xe9, R., Luccioni, A. S., Yvon, F., Gall\xe9, M., Tow, J., Rush, A. M., Biderman, S., Webson, A., Ammanamanchi, P. S., Wang, T., Sagot, B., Muennighoff, N., del Moral, A. V., \u2026 Wolf, T. (2022). BLOOM: A 176B-Parameter Open-Access Multilingual Language Model.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-127",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-128"},"Yong, Z.-X., Schoelkopf, H., Muennighoff, N., Aji, A. F., Adelani, D. I., Almubarak, K., Bari, M. S., Sutawika, L., Kasai, J., Baruwa, A., Winata, G. I., Biderman, S., Radev, D., & Nikoulina, V. (2022). BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-128",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-129"},"Lieber, O., Sharir, O., Lentz, B., & Shoham, Y. (2021). Jurassic-1: Technical Details and Evaluation, White paper, AI21 Labs, 2021. URL: Https://Uploads-Ssl. Webflow. Com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_ Tech_paper. Pdf.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-129",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-130"},"Wang, B., & Komatsuzaki, A. (2021). GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax. https://github.com/kingoflolz/mesh-transformer-jax\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-130",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-131"},"Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., & Stoyanov, V. (2019). Roberta: A robustly optimized bert pretraining approach. arXiv Preprint arXiv:1907.11692.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-131",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-132"},"Tang, T., Junyi, L., Chen, Z., Hu, Y., Yu, Z., Dai, W., Dong, Z., Cheng, X., Wang, Y., Zhao, W., Nie, J., & Wen, J.-R. (2022). TextBox 2.0: A Text Generation Library with Pre-trained Language Models.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-132",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-133"},"Strobelt, H., Webson, A., Sanh, V., Hoover, B., Beyer, J., Pfister, H., & Rush, A. M. (2022). Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models. arXiv. https://doi.org/10.48550/ARXIV.2208.07852\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-133",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-134"},"Bach, S. H., Sanh, V., Yong, Z.-X., Webson, A., Raffel, C., Nayak, N. V., Sharma, A., Kim, T., Bari, M. S., Fevry, T., Alyafeai, Z., Dey, M., Santilli, A., Sun, Z., Ben-David, S., Xu, C., Chhablani, G., Wang, H., Fries, J. A., \u2026 Rush, A. M. (2022). PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-134",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-135"},"Wu, T., Jiang, E., Donsbach, A., Gray, J., Molina, A., Terry, M., & Cai, C. J. (2022). PromptChainer: Chaining Large Language Model Prompts through Visual Programming.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-135",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-136"},"Ding, N., Hu, S., Zhao, W., Chen, Y., Liu, Z., Zheng, H.-T., & Sun, M. (2021). OpenPrompt: An Open-source Framework for Prompt-learning. arXiv Preprint arXiv:2111.01998.\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-136",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-137"},"Jiang, E., Olson, K., Toh, E., Molina, A., Donsbach, A., Terry, M., & Cai, C. J. (2022). PromptMaker: Prompt-Based Prototyping with Large&nbsp;Language&nbsp;Models. Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3491101.3503564\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-137",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-138"},"Chase, H. (2022). LangChain (0.0.66) [Computer software]. https://github.com/hwchase17/langchain\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-138",className:"footnote-backref"},"\u21a9")),(0,o.kt)("li",{parentName:"ol",id:"fn-139"},"Liu, J. (2022). GPT Index. https://doi.org/10.5281/zenodo.1234\n",(0,o.kt)("a",{parentName:"li",href:"#fnref-139",className:"footnote-backref"},"\u21a9")))))}g.isMDXComponent=!0}}]);