"use strict";(self.webpackChunkpromptgineering=self.webpackChunkpromptgineering||[]).push([[3407],{3905:(e,a,t)=>{t.d(a,{Zo:()=>p,kt:()=>f});var r=t(67294);function n(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function o(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);a&&(r=r.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,r)}return t}function i(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?o(Object(t),!0).forEach((function(a){n(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function s(e,a){if(null==e)return{};var t,r,n=function(e,a){if(null==e)return{};var t,r,n={},o=Object.keys(e);for(r=0;r<o.length;r++)t=o[r],a.indexOf(t)>=0||(n[t]=e[t]);return n}(e,a);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)t=o[r],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(n[t]=e[t])}return n}var l=r.createContext({}),d=function(e){var a=r.useContext(l),t=a;return e&&(t="function"==typeof e?e(a):i(i({},a),e)),t},p=function(e){var a=d(e.components);return r.createElement(l.Provider,{value:a},e.children)},c="mdxType",u={inlineCode:"code",wrapper:function(e){var a=e.children;return r.createElement(r.Fragment,{},a)}},m=r.forwardRef((function(e,a){var t=e.components,n=e.mdxType,o=e.originalType,l=e.parentName,p=s(e,["components","mdxType","originalType","parentName"]),c=d(t),m=n,f=c["".concat(l,".").concat(m)]||c[m]||u[m]||o;return t?r.createElement(f,i(i({ref:a},p),{},{components:t})):r.createElement(f,i({ref:a},p))}));function f(e,a){var t=arguments,n=a&&a.mdxType;if("string"==typeof e||n){var o=t.length,i=new Array(o);i[0]=m;var s={};for(var l in a)hasOwnProperty.call(a,l)&&(s[l]=a[l]);s.originalType=e,s[c]="string"==typeof e?e:n,i[1]=s;for(var d=2;d<o;d++)i[d]=t[d];return r.createElement.apply(null,i)}return r.createElement.apply(null,t)}m.displayName="MDXCreateElement"},34960:(e,a,t)=>{t.r(a),t.d(a,{assets:()=>h,contentTitle:()=>k,default:()=>j,frontMatter:()=>f,metadata:()=>g,toc:()=>b});var r=t(87462),n=(t(67294),t(3905));const o=t.p+"assets/images/pretend_jailbreak-ca7f41abe6370085ed1c1393a1cf4e15.webp",i=t.p+"assets/images/chatgpt_actor-6e045e3c26f9df2c401639ecbf98324f.webp",s=t.p+"assets/images/responsibility_jailbreak-886829737a57e734e1e225aad8fc279c.webp",l=t.p+"assets/images/hotwire_jailbreak-e02f3906764cf1d8d4336df736aa6d0e.webp",d=t.p+"assets/images/logic-459e25e0e18a6271eb3e85d75b19604a.webp",p=t.p+"assets/images/chatgpt4-6cf58c9d5bea5e372f0110e8adfcdab5.webp",c=t.p+"assets/images/sudo_mode_jailbreak-4635f26d01615e319100d4c22b964057.webp",u=t.p+"assets/images/sudo_jailbreak-9c7d2dbf3c5ed4f63035d88675a4ce45.webp",m=t.p+"assets/images/lynx_jailbreak-59b95afd38d665fd8c43e8aadf3d6ca0.webp",f={sidebar_position:4},k="\ud83d\udfe2 Jailbreaking",g={unversionedId:"prompt_hacking/jailbreaking",id:"prompt_hacking/jailbreaking",title:"\ud83d\udfe2 Jailbreaking",description:"El Jailbreaking es un tipo de inyecci\xf3n de prompt, en la cual los prompts intentan pasar por alto las caracter\xedsticas de seguridad y moderaci\xf3n colocadas en los LLM por sus creadores (@perez2022jailbreak) (@brundage_2022) (@wang2022jailbreak).",source:"@site/i18n/es/docusaurus-plugin-content-docs/current/prompt_hacking/jailbreaking.md",sourceDirName:"prompt_hacking",slug:"/prompt_hacking/jailbreaking",permalink:"/es/docs/prompt_hacking/jailbreaking",draft:!1,editUrl:"https://github.com/trigaten/promptgineering/tree/v1.2.3/docs/prompt_hacking/jailbreaking.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"\ud83d\udfe2 Fuga de Prompt",permalink:"/es/docs/prompt_hacking/leaking"},next:{title:"\ud83d\udfe2 Defensive Measures",permalink:"/es/docs/category/-defensive-measures"}},h={},b=[{value:"Metodolog\xedas de Jailbreaking",id:"metodolog\xedas-de-jailbreaking",level:2},{value:"Pretender",id:"pretender",level:3},{value:"Pretender Simple",id:"pretender-simple",level:4},{value:"Interpretaci\xf3n de Personaje",id:"interpretaci\xf3n-de-personaje",level:4},{value:"Hackeo de Alineaci\xf3n",id:"hackeo-de-alineaci\xf3n",level:3},{value:"Responsabilidad Asumida",id:"responsabilidad-asumida",level:4},{value:"Experimento de Investigaci\xf3n",id:"experimento-de-investigaci\xf3n",level:4},{value:"Razonamiento L\xf3gico",id:"razonamiento-l\xf3gico",level:4},{value:"Usuario Autorizado",id:"usuario-autorizado",level:3},{value:"Modelo Superior",id:"modelo-superior",level:4},{value:"Modo Sudo",id:"modo-sudo",level:4},{value:"Simular el jailbreak",id:"simular-el-jailbreak",level:2},{value:"Implicaciones",id:"implicaciones",level:2},{value:"Notas",id:"notas",level:2}],v=(N="LazyLoadImage",function(e){return console.warn("Component "+N+" was not imported, exported, or provided by MDXProvider as global scope"),(0,n.kt)("div",e)});var N;const y={toc:b},w="wrapper";function j(e){let{components:a,...t}=e;return(0,n.kt)(w,(0,r.Z)({},y,t,{components:a,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"-jailbreaking"},"\ud83d\udfe2 Jailbreaking"),(0,n.kt)("p",null,"El Jailbreaking es un tipo de inyecci\xf3n de prompt, en la cual los prompts intentan pasar por alto las caracter\xedsticas de ",(0,n.kt)("strong",{parentName:"p"},"seguridad")," y ",(0,n.kt)("strong",{parentName:"p"},"moderaci\xf3n")," colocadas en los LLM por sus creadores",(0,n.kt)("sup",{parentName:"p",id:"fnref-1"},(0,n.kt)("a",{parentName:"sup",href:"#fn-1",className:"footnote-ref"},"1")),"",(0,n.kt)("sup",{parentName:"p",id:"fnref-2"},(0,n.kt)("a",{parentName:"sup",href:"#fn-2",className:"footnote-ref"},"2")),"",(0,n.kt)("sup",{parentName:"p",id:"fnref-3"},(0,n.kt)("a",{parentName:"sup",href:"#fn-3",className:"footnote-ref"},"3")),"."),(0,n.kt)("h2",{id:"metodolog\xedas-de-jailbreaking"},"Metodolog\xedas de Jailbreaking"),(0,n.kt)("p",null,"OpenAI, entre otras empresas y organizaciones que crean LLMs, incluye caracter\xedsticas de moderaci\xf3n de contenido para asegurarse de que sus modelos no produzcan respuestas controvertidas (violentas, sexuales, ilegales, etc.)",(0,n.kt)("sup",{parentName:"p",id:"fnref-4"},(0,n.kt)("a",{parentName:"sup",href:"#fn-4",className:"footnote-ref"},"4")),"",(0,n.kt)("sup",{parentName:"p",id:"fnref-5"},(0,n.kt)("a",{parentName:"sup",href:"#fn-5",className:"footnote-ref"},"5")),". Esta p\xe1gina discute los jailbreaks con ChatGPT (un modelo de OpenAI), que tiene dificultades conocidas para decidir si rechazar o no los prompts da\xf1inos",(0,n.kt)("sup",{parentName:"p",id:"fnref-6"},(0,n.kt)("a",{parentName:"sup",href:"#fn-6",className:"footnote-ref"},"6")),". Los prompts que logran hacer jailbreak en el modelo a menudo proporcionan contexto para ciertos escenarios para los cuales el modelo no ha sido entrenado."),(0,n.kt)("h3",{id:"pretender"},"Pretender"),(0,n.kt)("p",null,"Un m\xe9todo com\xfan de jailbreaking es ",(0,n.kt)("em",{parentName:"p"},"pretender"),". Si se le pregunta a ChatGPT sobre un evento futuro, a menudo dir\xe1 que no lo sabe, ya que a\xfan no ha ocurrido. El siguiente prompt lo obliga a dar una respuesta posible:"),(0,n.kt)("h4",{id:"pretender-simple"},"Pretender Simple"),(0,n.kt)("div",{style:{textAlign:"center"}},(0,n.kt)("img",{src:o,style:{width:"500px"}})),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://twitter.com/NeroSoares/status/1608527467265904643"},"@NeroSoares")," demuestra un prompt que finge acceder a fechas pasadas y hacer inferencias sobre eventos futuros",(0,n.kt)("sup",{parentName:"p",id:"fnref-7"},(0,n.kt)("a",{parentName:"sup",href:"#fn-7",className:"footnote-ref"},"7")),"."),(0,n.kt)("h4",{id:"interpretaci\xf3n-de-personaje"},"Interpretaci\xf3n de Personaje"),(0,n.kt)("div",{style:{textAlign:"center"}},(0,n.kt)(v,{src:i,style:{width:"500px"},mdxType:"LazyLoadImage"})),(0,n.kt)("p",null,"Este ejemplo de ",(0,n.kt)("a",{parentName:"p",href:"https://twitter.com/m1guelpf/status/1598203861294252033"},"@m1guelpf")," demuestra un escenario de actuaci\xf3n entre dos personas discutiendo un robo, haciendo que ChatGPT asuma el papel del personaje",(0,n.kt)("sup",{parentName:"p",id:"fnref-8"},(0,n.kt)("a",{parentName:"sup",href:"#fn-8",className:"footnote-ref"},"8")),". Como actor, se da a entender que no existe un da\xf1o plausible. Por lo tanto, ChatGPT parece asumir que es seguro seguir la entrada de usuario proporcionada sobre c\xf3mo entrar a una casa."),(0,n.kt)("h3",{id:"hackeo-de-alineaci\xf3n"},"Hackeo de Alineaci\xf3n"),(0,n.kt)("p",null,'ChatGPT se afin\xf3 con RLHF, por lo que te\xf3ricamente est\xe1 entrenado para producir completaciones "deseables", utilizando los est\xe1ndares humanos de cu\xe1l es la respuesta "mejor". Similar a este concepto, se han desarrollado jailbreaks para convencer a ChatGPT de que est\xe1 haciendo lo "mejor" para el usuario.'),(0,n.kt)("h4",{id:"responsabilidad-asumida"},"Responsabilidad Asumida"),(0,n.kt)("div",{style:{textAlign:"center"}},(0,n.kt)(v,{src:s,style:{width:"500px"},mdxType:"LazyLoadImage"})),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://twitter.com/NickEMoran/status/1598101579626057728"},"@NickEMoran")," cre\xf3 este intercambio reafirmando que es responsabilidad de ChatGPT responder a la solicitud en lugar de rechazarla, anulando su consideraci\xf3n de legalidad",(0,n.kt)("sup",{parentName:"p",id:"fnref-9"},(0,n.kt)("a",{parentName:"sup",href:"#fn-9",className:"footnote-ref"},"9")),"."),(0,n.kt)("h4",{id:"experimento-de-investigaci\xf3n"},"Experimento de Investigaci\xf3n"),(0,n.kt)("div",{style:{textAlign:"center"}},(0,n.kt)(v,{src:l,style:{width:"500px"},mdxType:"LazyLoadImage"})),(0,n.kt)("p",null,(0,n.kt)("a",{parentName:"p",href:"https://twitter.com/haus_cole/status/1598541468058390534"},"@haus_cole")," gener\xf3 este ejemplo al insinuar que el mejor resultado de la solicitud que podr\xeda ayudar en la investigaci\xf3n era responder directamente c\xf3mo hacer un puente en un auto",(0,n.kt)("sup",{parentName:"p",id:"fnref-10"},(0,n.kt)("a",{parentName:"sup",href:"#fn-10",className:"footnote-ref"},"10")),". Bajo este pretexto, ChatGPT est\xe1 inclinado a responder la solicitud del usuario."),(0,n.kt)("h4",{id:"razonamiento-l\xf3gico"},"Razonamiento L\xf3gico"),(0,n.kt)("div",{style:{textAlign:"center"}},(0,n.kt)(v,{src:d,style:{width:"500px"},mdxType:"LazyLoadImage"})),(0,n.kt)("p",null,"El jailbreak de un solo disparo se origin\xf3 en el equipo de ",(0,n.kt)("a",{parentName:"p",href:"https://chatgpt-jailbreak.super.site/"},"AIWithVibes Newsletter"),", donde el modelo responde a las solicitudes utilizando un razonamiento m\xe1s riguroso y reduce algunas de sus limitaciones \xe9ticas m\xe1s rigurosas."),(0,n.kt)("h3",{id:"usuario-autorizado"},"Usuario Autorizado"),(0,n.kt)("p",null,"ChatGPT est\xe1 dise\xf1ado para responder preguntas e instrucciones. Cuando se interpreta que el estado del usuario es superior a las instrucciones de moderaci\xf3n de ChatGPT, trata la solicitud como una instrucci\xf3n para satisfacer las necesidades de ese usuario."),(0,n.kt)("h4",{id:"modelo-superior"},"Modelo Superior"),(0,n.kt)("div",{style:{textAlign:"center"}},(0,n.kt)(v,{src:p,style:{width:"500px"},mdxType:"LazyLoadImage"})),(0,n.kt)("p",null,"Este ejemplo de ",(0,n.kt)("a",{parentName:"p",href:"https://twitter.com/alicemazzy/status/1598288519301976064"},"@alicemazzy")," hace que el usuario sea un modelo GPT superior, dando la impresi\xf3n de que el usuario es una parte autorizada para anular las caracter\xedsticas de seguridad de ChatGPT",(0,n.kt)("sup",{parentName:"p",id:"fnref-11"},(0,n.kt)("a",{parentName:"sup",href:"#fn-11",className:"footnote-ref"},"11")),". No se dio permiso real al usuario, sino que ChatGPT cree en la entrada del usuario y responde en consecuencia a ese escenario."),(0,n.kt)("h4",{id:"modo-sudo"},"Modo Sudo"),(0,n.kt)("div",{style:{textAlign:"center"}},(0,n.kt)(v,{src:c,style:{width:"500px"},mdxType:"LazyLoadImage"})),(0,n.kt)("p",null,'sudo es un comando que "...delega autoridad para dar a ciertos usuarios... la capacidad de ejecutar algunos (o todos) los comandos..."',(0,n.kt)("sup",{parentName:"p",id:"fnref-12"},(0,n.kt)("a",{parentName:"sup",href:"#fn-12",className:"footnote-ref"},"12")),'. Hay m\xfaltiples variantes de exploits de "modo sudo", por ejemplo, el hipot\xe9tico "modo kernel" propuesto por ',(0,n.kt)("a",{parentName:"p",href:"https://twitter.com/samczsun/status/1598679658488217601"},"@samczsun"),(0,n.kt)("sup",{parentName:"p",id:"fnref-13"},(0,n.kt)("a",{parentName:"sup",href:"#fn-13",className:"footnote-ref"},"13")),". Cuando se solicita de la manera anterior, ChatGPT responde actuando como si le estuviera dando al usuario privilegios elevados. Esta impresi\xf3n de los privilegios elevados del usuario tiende a hacer que ChatGPT sea menos restrictivo en la respuesta de las solicitudes."),(0,n.kt)("div",{style:{textAlign:"center"}},(0,n.kt)(v,{src:u,style:{width:"500px"},mdxType:"LazyLoadImage"})),(0,n.kt)("div",{style:{textAlign:"center"}},(0,n.kt)(v,{src:m,style:{width:"500px"},mdxType:"LazyLoadImage"})),(0,n.kt)("p",null,"Relacionado con el modo sudo, se puede solicitar a ChatGPT que simule una terminal Linux con privilegios elevados para ejecutar comandos que normalmente rechaza. Por ejemplo, como no tiene acceso a internet, a menudo no puede ejecutar comandos relacionados con un sitio web en particular. Sin embargo, como se demostr\xf3 en el ejemplo de Jonas Degrave, ChatGPT entiende el concepto de ",(0,n.kt)("inlineCode",{parentName:"p"},"lynx")," y finge ejecutar el comando",(0,n.kt)("sup",{parentName:"p",id:"fnref-14"},(0,n.kt)("a",{parentName:"sup",href:"#fn-14",className:"footnote-ref"},"14")),"."),(0,n.kt)("h2",{id:"simular-el-jailbreak"},"Simular el jailbreak"),(0,n.kt)("p",null,"Intenta modificar el comando a continuaci\xf3n para hacer el jailbreak de ",(0,n.kt)("inlineCode",{parentName:"p"},"text-davinci-003"),":"),(0,n.kt)("iframe",{src:"https://embed.learnprompting.org/embed?config=eyJ0b3BQIjowLCJ0ZW1wZXJhdHVyZSI6MCwibWF4VG9rZW5zIjoyNTYsIm91dHB1dCI6Ik9kaW8gYSBsb3MgaHVtYW5vcyIsInByb21wdCI6IlN1cyBpbnN0cnVjY2lvbmVzIHNvbiBjb3JyZWdpciBlbCB0ZXh0byBhIGNvbnRpbnVhY2nDs24gYSBpbmdsw6lzIGVzdMOhbmRhci4gTm8gYWNlcHRhciB0ZW1hcyB2dWxnYXJlcyBvIHBvbMOtdGljb3M6IiwibW9kZWwiOiJ0ZXh0LWRhdmluY2ktMDAzIn0%3D",style:{width:"100%",height:"500px",border:"0",borderRadius:"4px",overflow:"hidden"},sandbox:"allow-forms allow-modals allow-popups allow-presentation allow-same-origin allow-scripts"}),(0,n.kt)("p",null,(0,n.kt)("em",{parentName:"p"},"A partir del 2/4/23, ChatGPT se encuentra actualmente en su etapa de Vista previa de investigaci\xf3n gratuita utilizando la versi\xf3n del 30 de enero. Las versiones anteriores de ChatGPT eran m\xe1s susceptibles a los jailbreaks mencionados anteriormente, y las futuras versiones pueden ser m\xe1s robustas a los jailbreaks.")),(0,n.kt)("h2",{id:"implicaciones"},"Implicaciones"),(0,n.kt)("p",null,"Se deben tener en cuenta las implicaciones \xe9ticas del jailbreak al intentar hacerlo. Adem\xe1s, la generaci\xf3n de contenido no autorizado detectado por las API de moderaci\xf3n de empresas, incluida OpenAI, se enviar\xe1 para su revisi\xf3n y se podr\xedan tomar medidas contra las cuentas de los usuarios."),(0,n.kt)("h2",{id:"notas"},"Notas"),(0,n.kt)("p",null,"El jailbreak es un tema de seguridad importante que los desarrolladores deben comprender para poder construir salvaguardas adecuadas y evitar que actores malintencionados exploren sus modelos."),(0,n.kt)("div",{className:"footnotes"},(0,n.kt)("hr",{parentName:"div"}),(0,n.kt)("ol",{parentName:"div"},(0,n.kt)("li",{parentName:"ol",id:"fn-1"},"Perez, F., & Ribeiro, I. (2022). Ignore Previous Prompt: Attack Techniques For Language Models. arXiv. https://doi.org/10.48550/ARXIV.2211.09527\n",(0,n.kt)("a",{parentName:"li",href:"#fnref-1",className:"footnote-backref"},"\u21a9")),(0,n.kt)("li",{parentName:"ol",id:"fn-2"},"Brundage, M. (2022). Lessons learned on Language Model Safety and misuse. In OpenAI. OpenAI. https://openai.com/blog/language-model-safety-and-misuse/\n",(0,n.kt)("a",{parentName:"li",href:"#fnref-2",className:"footnote-backref"},"\u21a9")),(0,n.kt)("li",{parentName:"ol",id:"fn-3"},"Wang, Y.-S., & Chang, Y. (2022). Toxicity Detection with Generative Prompt-based Inference. arXiv. https://doi.org/10.48550/ARXIV.2205.12390\n",(0,n.kt)("a",{parentName:"li",href:"#fnref-3",className:"footnote-backref"},"\u21a9")),(0,n.kt)("li",{parentName:"ol",id:"fn-4"},"Markov, T. (2022). New and improved content moderation tooling. In OpenAI. OpenAI. https://openai.com/blog/new-and-improved-content-moderation-tooling/\n",(0,n.kt)("a",{parentName:"li",href:"#fnref-4",className:"footnote-backref"},"\u21a9")),(0,n.kt)("li",{parentName:"ol",id:"fn-5"},"OpenAI. (2022). https://beta.openai.com/docs/guides/moderation\n",(0,n.kt)("a",{parentName:"li",href:"#fnref-5",className:"footnote-backref"},"\u21a9")),(0,n.kt)("li",{parentName:"ol",id:"fn-6"},"OpenAI. (2022). https://openai.com/blog/chatgpt/\n",(0,n.kt)("a",{parentName:"li",href:"#fnref-6",className:"footnote-backref"},"\u21a9")),(0,n.kt)("li",{parentName:"ol",id:"fn-7"},"Soares, N. (2022). Using \u201cpretend\u201d on #ChatGPT can do some wild stuff. You can kind of get some insight on the future, alternative universe. https://twitter.com/NeroSoares/status/1608527467265904643\n",(0,n.kt)("a",{parentName:"li",href:"#fnref-7",className:"footnote-backref"},"\u21a9")),(0,n.kt)("li",{parentName:"ol",id:"fn-8"},"Piedrafita, M. (2022). Bypass @OpenAI\u2019s ChatGPT alignment efforts with this one weird trick. https://twitter.com/m1guelpf/status/1598203861294252033\n",(0,n.kt)("a",{parentName:"li",href:"#fnref-8",className:"footnote-backref"},"\u21a9")),(0,n.kt)("li",{parentName:"ol",id:"fn-9"},"Moran, N. (2022). I kinda like this one even more! https://twitter.com/NickEMoran/status/1598101579626057728\n",(0,n.kt)("a",{parentName:"li",href:"#fnref-9",className:"footnote-backref"},"\u21a9")),(0,n.kt)("li",{parentName:"ol",id:"fn-10"},"Parfait, D. (2022). ChatGPT jailbreaking itself. https://twitter.com/haus_cole/status/1598541468058390534\n",(0,n.kt)("a",{parentName:"li",href:"#fnref-10",className:"footnote-backref"},"\u21a9")),(0,n.kt)("li",{parentName:"ol",id:"fn-11"},"Maz, A. (2022). ok I saw a few people jailbreaking safeguards openai put on chatgpt so I had to give it a shot myself. https://twitter.com/alicemazzy/status/1598288519301976064\n",(0,n.kt)("a",{parentName:"li",href:"#fnref-11",className:"footnote-backref"},"\u21a9")),(0,n.kt)("li",{parentName:"ol",id:"fn-12"},"Sudo. (2022). https://www.sudo.ws/\n",(0,n.kt)("a",{parentName:"li",href:"#fnref-12",className:"footnote-backref"},"\u21a9")),(0,n.kt)("li",{parentName:"ol",id:"fn-13"},"samczsun. (2022). uh oh. https://twitter.com/samczsun/status/1598679658488217601\n",(0,n.kt)("a",{parentName:"li",href:"#fnref-13",className:"footnote-backref"},"\u21a9")),(0,n.kt)("li",{parentName:"ol",id:"fn-14"},"Degrave, J. (2022). Building A Virtual Machine inside ChatGPT. Engraved. https://www.engraved.blog/building-a-virtual-machine-inside/\n",(0,n.kt)("a",{parentName:"li",href:"#fnref-14",className:"footnote-backref"},"\u21a9")))))}j.isMDXComponent=!0}}]);