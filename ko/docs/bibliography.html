<!doctype html>
<html lang="ko" dir="ltr" class="docs-wrapper docs-doc-page docs-version-current plugin-docs plugin-id-default docs-doc-id-bibliography">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v2.4.0">
<title data-rh="true">📚 Bibliography | Learn Prompting: Your Guide to Communicating with AI</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://learnprompting.org/ko/docs/bibliography"><meta data-rh="true" name="docusaurus_locale" content="ko"><meta data-rh="true" name="docsearch:language" content="ko"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="📚 Bibliography | Learn Prompting: Your Guide to Communicating with AI"><meta data-rh="true" name="description" content="The page contains an organized list of all papers used by this course."><meta data-rh="true" property="og:description" content="The page contains an organized list of all papers used by this course."><link data-rh="true" rel="icon" href="/ko/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://learnprompting.org/ko/docs/bibliography"><link data-rh="true" rel="alternate" href="https://learnprompting.org/docs/bibliography" hreflang="en"><link data-rh="true" rel="alternate" href="https://learnprompting.org/es/docs/bibliography" hreflang="es"><link data-rh="true" rel="alternate" href="https://learnprompting.org/fr/docs/bibliography" hreflang="fr"><link data-rh="true" rel="alternate" href="https://learnprompting.org/ja/docs/bibliography" hreflang="ja"><link data-rh="true" rel="alternate" href="https://learnprompting.org/pt/docs/bibliography" hreflang="pt"><link data-rh="true" rel="alternate" href="https://learnprompting.org/zh-Hans/docs/bibliography" hreflang="zh-Hans"><link data-rh="true" rel="alternate" href="https://learnprompting.org/ko/docs/bibliography" hreflang="ko"><link data-rh="true" rel="alternate" href="https://learnprompting.org/si/docs/bibliography" hreflang="si"><link data-rh="true" rel="alternate" href="https://learnprompting.org/ru/docs/bibliography" hreflang="ru"><link data-rh="true" rel="alternate" href="https://learnprompting.org/ar/docs/bibliography" hreflang="ar"><link data-rh="true" rel="alternate" href="https://learnprompting.org/de/docs/bibliography" hreflang="de"><link data-rh="true" rel="alternate" href="https://learnprompting.org/uk/docs/bibliography" hreflang="uk"><link data-rh="true" rel="alternate" href="https://learnprompting.org/id/docs/bibliography" hreflang="id"><link data-rh="true" rel="alternate" href="https://learnprompting.org/docs/bibliography" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/ko/blog/rss.xml" title="Learn Prompting: Your Guide to Communicating with AI RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/ko/blog/atom.xml" title="Learn Prompting: Your Guide to Communicating with AI Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","G-FV0C417KS8","auto"),ga("send","pageview")</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-FV0C417KS8"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-FV0C417KS8",{})</script>




<link rel="preconnect" href="https://app.posthog.com">
<script>!function(t,e){var o,p,i,n;e.__SV||(window.posthog=e,e._i=[],e.init=function(r,s,a){function c(t,e){var o=e.split(".");2==o.length&&(t=t[o[0]],e=o[1]),t[e]=function(){t.push([e].concat(Array.prototype.slice.call(arguments,0)))}}(i=t.createElement("script")).type="text/javascript",i.async=!0,i.src=s.api_host+"/static/array.js",(n=t.getElementsByTagName("script")[0]).parentNode.insertBefore(i,n);var g=e;for(void 0!==a?g=e[a]=[]:a="posthog",g.people=g.people||[],g.toString=function(t){var e="posthog";return"posthog"!==a&&(e+="."+a),t||(e+=" (stub)"),e},g.people.toString=function(){return g.toString(1)+".people (stub)"},o="capture identify alias people.set people.set_once set_config register register_once unregister opt_out_capturing has_opted_out_capturing opt_in_capturing reset".split(" "),p=0;p<o.length;p++)c(g,o[p]);e._i.push([r,s,a])},e.__SV=1)}(document,window.posthog||[]),posthog.init("DEV",{api_host:"https://app.posthog.com",id:"default"})</script>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.24/dist/katex.min.css">
<link rel="preconnect" href="https://fonts.googleapis.com" async>
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="" async>
<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Be+Vietnam+Pro:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&amp;display=swap" async><link rel="stylesheet" href="/ko/assets/css/styles.cc4133d4.css">
<link rel="preload" href="/ko/assets/js/runtime~main.261832bc.js" as="script">
<link rel="preload" href="/ko/assets/js/main.2f5284eb.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region" aria-label="본문으로 건너뛰기"><a class="skipToContent_fXgn" href="#docusaurus_skipToContent_fallback">본문으로 건너뛰기</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/ko/"><div class="navbar__logo"><img src="/ko/img/simple_ai.webp" alt="My Site Logo" class="themedImage_ToTc themedImage--light_HNdA"><img src="/ko/img/simple_ai.webp" alt="My Site Logo" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate">Learn Prompting</b></a><div class="px-4 md:px-20 2xl:px-96"><div class="md:flex hidden justify-between py-0"></div></div></div><div class="navbar__items navbar__items--right"><div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link"><svg viewBox="0 0 24 24" width="20" height="20" aria-hidden="true" class="iconLanguage_nlXk"><path fill="currentColor" d="M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z"></path></svg>한국어</a><ul class="dropdown__menu"><li><a href="/docs/bibliography" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="en">English</a></li><li><a href="/es/docs/bibliography" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="es">Español</a></li><li><a href="/fr/docs/bibliography" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="fr">Français</a></li><li><a href="/ja/docs/bibliography" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ja">日本語</a></li><li><a href="/pt/docs/bibliography" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="pt">Português</a></li><li><a href="/zh-Hans/docs/bibliography" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="zh-Hans">简体中文</a></li><li><a href="/ko/docs/bibliography" target="_self" rel="noopener noreferrer" class="dropdown__link dropdown__link--active" lang="ko">한국어</a></li><li><a href="/si/docs/bibliography" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="si">සිංහල</a></li><li><a href="/ru/docs/bibliography" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ru">Русский</a></li><li><a href="/ar/docs/bibliography" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="ar">العربية</a></li><li><a href="/de/docs/bibliography" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="de">Deutsch</a></li><li><a href="/uk/docs/bibliography" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="uk">Українська</a></li><li><a href="/id/docs/bibliography" target="_self" rel="noopener noreferrer" class="dropdown__link" lang="id">Indonesia</a></li></ul></div><a href="https://github.com/trigaten/Learn_Prompting/releases" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Change Log<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="어두운 모드와 밝은 모드 전환하기 (현재 밝은 모드)" aria-label="어두운 모드와 밝은 모드 전환하기 (현재 밝은 모드)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="searchBox_ZlJk"><button class="flex items-center space-x-4 border px-2 py-1 rounded-full border-gray-300 hover:border-gray-400 focus:outline-none focus:ring-2 focus:ring-gray-400 focus:ring-opacity-50"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" class="w-5 h-5"><path stroke-linecap="round" stroke-linejoin="round" d="M21 21l-5.197-5.197m0 0A7.5 7.5 0 105.196 5.196a7.5 7.5 0 0010.607 10.607z"></path></svg><span class="hidden lg:block text-sm">Search</span><kbd class="hidden lg:inline-flex items-center rounded-xl border border-gray-200 px-2 font-sans text-sm font-medium text-gray-400">⌘K</kbd></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0 docsWrapper_BCFX"><button aria-label="맨 위로 스크롤하기" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docPage__5DB"><aside class="theme-doc-sidebar-container docSidebarContainer_b6E3"><div class="sidebarViewport_Xe31"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ko/docs/intro">환영합니다</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/ko/docs/category/-basics">😃 Basics</a><button aria-label="접을 수 있는 사이드바 분류 &#x27;😃 Basics&#x27; 접기(펼치기)" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/ko/docs/category/-basic-applications">💼 Basic Applications</a><button aria-label="접을 수 있는 사이드바 분류 &#x27;💼 Basic Applications&#x27; 접기(펼치기)" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/ko/docs/category/️-intermediate">🧙‍♂️ Intermediate</a><button aria-label="접을 수 있는 사이드바 분류 &#x27;🧙‍♂️ Intermediate&#x27; 접기(펼치기)" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/ko/docs/category/-applied-prompting">🧪 Applied Prompting</a><button aria-label="접을 수 있는 사이드바 분류 &#x27;🧪 Applied Prompting&#x27; 접기(펼치기)" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/ko/docs/category/-advanced-applications">🚀 Advanced Applications</a><button aria-label="접을 수 있는 사이드바 분류 &#x27;🚀 Advanced Applications&#x27; 접기(펼치기)" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/ko/docs/category/️-reliability">⚖️ Reliability</a><button aria-label="접을 수 있는 사이드바 분류 &#x27;⚖️ Reliability&#x27; 접기(펼치기)" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/ko/docs/category/️-image-prompting">🖼️ Image Prompting</a><button aria-label="접을 수 있는 사이드바 분류 &#x27;🖼️ Image Prompting&#x27; 접기(펼치기)" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/ko/docs/category/-prompt-hacking">🔓 Prompt Hacking</a><button aria-label="접을 수 있는 사이드바 분류 &#x27;🔓 Prompt Hacking&#x27; 접기(펼치기)" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/ko/docs/category/-tooling">🔨 Tooling</a><button aria-label="접을 수 있는 사이드바 분류 &#x27;🔨 Tooling&#x27; 접기(펼치기)" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/ko/docs/category/-prompt-tuning">💪 Prompt Tuning</a><button aria-label="접을 수 있는 사이드바 분류 &#x27;💪 Prompt Tuning&#x27; 접기(펼치기)" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/ko/docs/category/-miscellaneous">🎲 Miscellaneous</a><button aria-label="접을 수 있는 사이드바 분류 &#x27;🎲 Miscellaneous&#x27; 접기(펼치기)" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ko/docs/vocabulary">📙 Vocabulary Reference</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" href="/ko/docs/bibliography">📚 Bibliography</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ko/docs/products">📦 Prompted Products</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ko/docs/additional">🛸 Additional Resources</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ko/docs/credits">✨ Credits</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/ko/docs/hot_topics">🔥 Hot Topics</a></li></ul></nav></div></div></aside><main class="docMainContainer_gTbr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="홈" class="breadcrumbs__link" href="/ko/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">📚 Bibliography</span><meta itemprop="position" content="1"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">이 페이지에서</button></div><div class="theme-doc-markdown markdown"><h1>📚 Bibliography</h1><p>The page contains an organized list of all papers used by this course.
The papers are organized by topic.</p><p><strong>To cite this course, use the provided citation in the Github repository.</strong></p><div class="language-md codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-md codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">@software{Schulhoff_Learn_Prompting_2022,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    author = {Schulhoff, Sander and Community Contributors},</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    month = dec,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    title = {{Learn Prompting}},</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    url = {https://github.com/trigaten/Learn_Prompting},</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    year = {2022}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="클립보드에 코드 복사" title="복사" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Note: since <a href="https://twitter.com/janleike/status/1584618242756132864" target="_blank" rel="noopener noreferrer">neither the GPT-3 nor the GPT-3 Instruct paper correspond to davinci models</a>, I attempt not to
cite them as such.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="agents">Agents<a href="#agents" class="hash-link" aria-label="Agents에 대한 직접 링크" title="Agents에 대한 직접 링크">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="mrklkarpas2022mrkl">MRKL<sup id="fnref-1"><a href="#fn-1" class="footnote-ref">1</a></sup><a href="#mrklkarpas2022mrkl" class="hash-link" aria-label="mrklkarpas2022mrkl에 대한 직접 링크" title="mrklkarpas2022mrkl에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="reactyao2022react">ReAct<sup id="fnref-2"><a href="#fn-2" class="footnote-ref">2</a></sup><a href="#reactyao2022react" class="hash-link" aria-label="reactyao2022react에 대한 직접 링크" title="reactyao2022react에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="palgao2022pal">PAL<sup id="fnref-3"><a href="#fn-3" class="footnote-ref">3</a></sup><a href="#palgao2022pal" class="hash-link" aria-label="palgao2022pal에 대한 직접 링크" title="palgao2022pal에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="auto-gptrichards2023">Auto-GPT<sup id="fnref-4"><a href="#fn-4" class="footnote-ref">4</a></sup><a href="#auto-gptrichards2023" class="hash-link" aria-label="auto-gptrichards2023에 대한 직접 링크" title="auto-gptrichards2023에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="baby-aginakajima2023">Baby AGI<sup id="fnref-5"><a href="#fn-5" class="footnote-ref">5</a></sup><a href="#baby-aginakajima2023" class="hash-link" aria-label="baby-aginakajima2023에 대한 직접 링크" title="baby-aginakajima2023에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="agentgptreworkd2023">AgentGPT<sup id="fnref-6"><a href="#fn-6" class="footnote-ref">6</a></sup><a href="#agentgptreworkd2023" class="hash-link" aria-label="agentgptreworkd2023에 대한 직접 링크" title="agentgptreworkd2023에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="toolformerschick2023toolformer">Toolformer<sup id="fnref-7"><a href="#fn-7" class="footnote-ref">7</a></sup><a href="#toolformerschick2023toolformer" class="hash-link" aria-label="toolformerschick2023toolformer에 대한 직접 링크" title="toolformerschick2023toolformer에 대한 직접 링크">​</a></h4><h2 class="anchor anchorWithStickyNavbar_LWe7" id="automated">Automated<a href="#automated" class="hash-link" aria-label="Automated에 대한 직접 링크" title="Automated에 대한 직접 링크">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="autoprompt-eliciting-knowledge-from-language-models-with-automatically-generated-promptsshin2020autoprompt">AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts<sup id="fnref-8"><a href="#fn-8" class="footnote-ref">8</a></sup><a href="#autoprompt-eliciting-knowledge-from-language-models-with-automatically-generated-promptsshin2020autoprompt" class="hash-link" aria-label="autoprompt-eliciting-knowledge-from-language-models-with-automatically-generated-promptsshin2020autoprompt에 대한 직접 링크" title="autoprompt-eliciting-knowledge-from-language-models-with-automatically-generated-promptsshin2020autoprompt에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="automatic-prompt-engineerzhou2022large">automatic prompt engineer<sup id="fnref-9"><a href="#fn-9" class="footnote-ref">9</a></sup><a href="#automatic-prompt-engineerzhou2022large" class="hash-link" aria-label="automatic-prompt-engineerzhou2022large에 대한 직접 링크" title="automatic-prompt-engineerzhou2022large에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="soft-promptinglester2021power">Soft Prompting<sup id="fnref-10"><a href="#fn-10" class="footnote-ref">10</a></sup><a href="#soft-promptinglester2021power" class="hash-link" aria-label="soft-promptinglester2021power에 대한 직접 링크" title="soft-promptinglester2021power에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="discretized-soft-prompting-interpretingkhashabi2021prompt">discretized soft prompting (interpreting)<sup id="fnref-11"><a href="#fn-11" class="footnote-ref">11</a></sup><a href="#discretized-soft-prompting-interpretingkhashabi2021prompt" class="hash-link" aria-label="discretized-soft-prompting-interpretingkhashabi2021prompt에 대한 직접 링크" title="discretized-soft-prompting-interpretingkhashabi2021prompt에 대한 직접 링크">​</a></h4><h2 class="anchor anchorWithStickyNavbar_LWe7" id="datasets">Datasets<a href="#datasets" class="hash-link" aria-label="Datasets에 대한 직접 링크" title="Datasets에 대한 직접 링크">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="scan-dataset-compositional-generalizationlake2018scan">SCAN dataset (compositional generalization)<sup id="fnref-12"><a href="#fn-12" class="footnote-ref">12</a></sup><a href="#scan-dataset-compositional-generalizationlake2018scan" class="hash-link" aria-label="scan-dataset-compositional-generalizationlake2018scan에 대한 직접 링크" title="scan-dataset-compositional-generalizationlake2018scan에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="gsm8kcobbe2021training">GSM8K<sup id="fnref-13"><a href="#fn-13" class="footnote-ref">13</a></sup><a href="#gsm8kcobbe2021training" class="hash-link" aria-label="gsm8kcobbe2021training에 대한 직접 링크" title="gsm8kcobbe2021training에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="hotpotqayang2018hotpotqa">hotpotQA<sup id="fnref-14"><a href="#fn-14" class="footnote-ref">14</a></sup><a href="#hotpotqayang2018hotpotqa" class="hash-link" aria-label="hotpotqayang2018hotpotqa에 대한 직접 링크" title="hotpotqayang2018hotpotqa에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="multiarithroy-roth-2015-solving">multiarith<sup id="fnref-15"><a href="#fn-15" class="footnote-ref">15</a></sup><a href="#multiarithroy-roth-2015-solving" class="hash-link" aria-label="multiarithroy-roth-2015-solving에 대한 직접 링크" title="multiarithroy-roth-2015-solving에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="fever-datasetthorne2018fever">fever dataset<sup id="fnref-16"><a href="#fn-16" class="footnote-ref">16</a></sup><a href="#fever-datasetthorne2018fever" class="hash-link" aria-label="fever-datasetthorne2018fever에 대한 직접 링크" title="fever-datasetthorne2018fever에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="bbqparrish2021bbq">bbq<sup id="fnref-17"><a href="#fn-17" class="footnote-ref">17</a></sup><a href="#bbqparrish2021bbq" class="hash-link" aria-label="bbqparrish2021bbq에 대한 직접 링크" title="bbqparrish2021bbq에 대한 직접 링크">​</a></h4><h2 class="anchor anchorWithStickyNavbar_LWe7" id="detection">Detection<a href="#detection" class="hash-link" aria-label="Detection에 대한 직접 링크" title="Detection에 대한 직접 링크">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="dont-ban-chatgpt-in-schools-teach-with-itroose2022dont">Don&#x27;t ban chatgpt in schools. teach with it.<sup id="fnref-18"><a href="#fn-18" class="footnote-ref">18</a></sup><a href="#dont-ban-chatgpt-in-schools-teach-with-itroose2022dont" class="hash-link" aria-label="dont-ban-chatgpt-in-schools-teach-with-itroose2022dont에 대한 직접 링크" title="dont-ban-chatgpt-in-schools-teach-with-itroose2022dont에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="schools-shouldnt-ban-access-to-chatgptlipman2022gpt">Schools Shouldn&#x27;t Ban Access to ChatGPT<sup id="fnref-19"><a href="#fn-19" class="footnote-ref">19</a></sup><a href="#schools-shouldnt-ban-access-to-chatgptlipman2022gpt" class="hash-link" aria-label="schools-shouldnt-ban-access-to-chatgptlipman2022gpt에 대한 직접 링크" title="schools-shouldnt-ban-access-to-chatgptlipman2022gpt에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="certified-neural-network-watermarks-with-randomized-smoothingbansal2022certified">Certified Neural Network Watermarks with Randomized Smoothing<sup id="fnref-20"><a href="#fn-20" class="footnote-ref">20</a></sup><a href="#certified-neural-network-watermarks-with-randomized-smoothingbansal2022certified" class="hash-link" aria-label="certified-neural-network-watermarks-with-randomized-smoothingbansal2022certified에 대한 직접 링크" title="certified-neural-network-watermarks-with-randomized-smoothingbansal2022certified에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="watermarking-pre-trained-language-models-with-backdooringgu2022watermarking">Watermarking Pre-trained Language Models with Backdooring<sup id="fnref-21"><a href="#fn-21" class="footnote-ref">21</a></sup><a href="#watermarking-pre-trained-language-models-with-backdooringgu2022watermarking" class="hash-link" aria-label="watermarking-pre-trained-language-models-with-backdooringgu2022watermarking에 대한 직접 링크" title="watermarking-pre-trained-language-models-with-backdooringgu2022watermarking에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="gw-preparing-disciplinary-response-to-ai-programs-as-faculty-explore-educational-usenoonan2023gw">GW preparing disciplinary response to AI programs as faculty explore educational use<sup id="fnref-22"><a href="#fn-22" class="footnote-ref">22</a></sup><a href="#gw-preparing-disciplinary-response-to-ai-programs-as-faculty-explore-educational-usenoonan2023gw" class="hash-link" aria-label="gw-preparing-disciplinary-response-to-ai-programs-as-faculty-explore-educational-usenoonan2023gw에 대한 직접 링크" title="gw-preparing-disciplinary-response-to-ai-programs-as-faculty-explore-educational-usenoonan2023gw에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="a-watermark-for-large-language-modelskirchenbauer2023watermarking">A Watermark for Large Language Models<sup id="fnref-23"><a href="#fn-23" class="footnote-ref">23</a></sup><a href="#a-watermark-for-large-language-modelskirchenbauer2023watermarking" class="hash-link" aria-label="a-watermark-for-large-language-modelskirchenbauer2023watermarking에 대한 직접 링크" title="a-watermark-for-large-language-modelskirchenbauer2023watermarking에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="detectgpt-zero-shot-machine-generated-text-detection-using-probability-curvaturemitchell2023detectgpt">DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature<sup id="fnref-24"><a href="#fn-24" class="footnote-ref">24</a></sup><a href="#detectgpt-zero-shot-machine-generated-text-detection-using-probability-curvaturemitchell2023detectgpt" class="hash-link" aria-label="detectgpt-zero-shot-machine-generated-text-detection-using-probability-curvaturemitchell2023detectgpt에 대한 직접 링크" title="detectgpt-zero-shot-machine-generated-text-detection-using-probability-curvaturemitchell2023detectgpt에 대한 직접 링크">​</a></h4><h2 class="anchor anchorWithStickyNavbar_LWe7" id="image-prompt-engineering">Image Prompt Engineering<a href="#image-prompt-engineering" class="hash-link" aria-label="Image Prompt Engineering에 대한 직접 링크" title="Image Prompt Engineering에 대한 직접 링크">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="prompt-engineering-for-text-based-generative-artoppenlaender2022prompt">Prompt Engineering for Text-Based Generative Art<sup id="fnref-25"><a href="#fn-25" class="footnote-ref">25</a></sup><a href="#prompt-engineering-for-text-based-generative-artoppenlaender2022prompt" class="hash-link" aria-label="prompt-engineering-for-text-based-generative-artoppenlaender2022prompt에 대한 직접 링크" title="prompt-engineering-for-text-based-generative-artoppenlaender2022prompt에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="the-dalle-2-prompt-bookparsons2022dalleprompt">The DALLE 2 Prompt Book<sup id="fnref-26"><a href="#fn-26" class="footnote-ref">26</a></sup><a href="#the-dalle-2-prompt-bookparsons2022dalleprompt" class="hash-link" aria-label="the-dalle-2-prompt-bookparsons2022dalleprompt에 대한 직접 링크" title="the-dalle-2-prompt-bookparsons2022dalleprompt에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="with-the-right-prompt-stable-diffusion-20-can-do-handsblake2022with">With the right prompt, Stable Diffusion 2.0 can do hands.<sup id="fnref-27"><a href="#fn-27" class="footnote-ref">27</a></sup><a href="#with-the-right-prompt-stable-diffusion-20-can-do-handsblake2022with" class="hash-link" aria-label="with-the-right-prompt-stable-diffusion-20-can-do-handsblake2022with에 대한 직접 링크" title="with-the-right-prompt-stable-diffusion-20-can-do-handsblake2022with에 대한 직접 링크">​</a></h4><h2 class="anchor anchorWithStickyNavbar_LWe7" id="meta-analysis">Meta Analysis<a href="#meta-analysis" class="hash-link" aria-label="Meta Analysis에 대한 직접 링크" title="Meta Analysis에 대한 직접 링크">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-generative-ai-is-changing-creative-workdavenport_mittal_2022">How Generative AI Is Changing Creative Work<sup id="fnref-28"><a href="#fn-28" class="footnote-ref">28</a></sup><a href="#how-generative-ai-is-changing-creative-workdavenport_mittal_2022" class="hash-link" aria-label="how-generative-ai-is-changing-creative-workdavenport_mittal_2022에 대한 직접 링크" title="how-generative-ai-is-changing-creative-workdavenport_mittal_2022에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-ai-will-change-the-workplacecaptain_2023">How AI Will Change the Workplace<sup id="fnref-29"><a href="#fn-29" class="footnote-ref">29</a></sup><a href="#how-ai-will-change-the-workplacecaptain_2023" class="hash-link" aria-label="how-ai-will-change-the-workplacecaptain_2023에 대한 직접 링크" title="how-ai-will-change-the-workplacecaptain_2023에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="chatgpt-took-their-jobs-now-they-walk-dogs-and-fix-air-conditionersverma_vynck_2023">ChatGPT took their jobs. Now they walk dogs and fix air conditioners.<sup id="fnref-30"><a href="#fn-30" class="footnote-ref">30</a></sup><a href="#chatgpt-took-their-jobs-now-they-walk-dogs-and-fix-air-conditionersverma_vynck_2023" class="hash-link" aria-label="chatgpt-took-their-jobs-now-they-walk-dogs-and-fix-air-conditionersverma_vynck_2023에 대한 직접 링크" title="chatgpt-took-their-jobs-now-they-walk-dogs-and-fix-air-conditionersverma_vynck_2023에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="no-titleibm_do_2023">No title<sup id="fnref-31"><a href="#fn-31" class="footnote-ref">31</a></sup><a href="#no-titleibm_do_2023" class="hash-link" aria-label="no-titleibm_do_2023에 대한 직접 링크" title="no-titleibm_do_2023에 대한 직접 링크">​</a></h4><h2 class="anchor anchorWithStickyNavbar_LWe7" id="miscl">Miscl<a href="#miscl" class="hash-link" aria-label="Miscl에 대한 직접 링크" title="Miscl에 대한 직접 링크">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="the-turking-test-can-language-models-understand-instructionsefrat2020turking">The Turking Test: Can Language Models Understand Instructions?<sup id="fnref-32"><a href="#fn-32" class="footnote-ref">32</a></sup><a href="#the-turking-test-can-language-models-understand-instructionsefrat2020turking" class="hash-link" aria-label="the-turking-test-can-language-models-understand-instructionsefrat2020turking에 대한 직접 링크" title="the-turking-test-can-language-models-understand-instructionsefrat2020turking에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="a-taxonomy-of-prompt-modifiers-for-text-to-image-generationoppenlaender2022taxonomy">A Taxonomy of Prompt Modifiers for Text-To-Image Generation<sup id="fnref-33"><a href="#fn-33" class="footnote-ref">33</a></sup><a href="#a-taxonomy-of-prompt-modifiers-for-text-to-image-generationoppenlaender2022taxonomy" class="hash-link" aria-label="a-taxonomy-of-prompt-modifiers-for-text-to-image-generationoppenlaender2022taxonomy에 대한 직접 링크" title="a-taxonomy-of-prompt-modifiers-for-text-to-image-generationoppenlaender2022taxonomy에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="diffusiondb-a-large-scale-prompt-gallery-dataset-for-text-to-image-generative-modelswang2022diffusiondb">DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models<sup id="fnref-34"><a href="#fn-34" class="footnote-ref">34</a></sup><a href="#diffusiondb-a-large-scale-prompt-gallery-dataset-for-text-to-image-generative-modelswang2022diffusiondb" class="hash-link" aria-label="diffusiondb-a-large-scale-prompt-gallery-dataset-for-text-to-image-generative-modelswang2022diffusiondb에 대한 직접 링크" title="diffusiondb-a-large-scale-prompt-gallery-dataset-for-text-to-image-generative-modelswang2022diffusiondb에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="optimizing-prompts-for-text-to-image-generationhao2022optimizing">Optimizing Prompts for Text-to-Image Generation<sup id="fnref-35"><a href="#fn-35" class="footnote-ref">35</a></sup><a href="#optimizing-prompts-for-text-to-image-generationhao2022optimizing" class="hash-link" aria-label="optimizing-prompts-for-text-to-image-generationhao2022optimizing에 대한 직접 링크" title="optimizing-prompts-for-text-to-image-generationhao2022optimizing에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="language-model-cascadesdohan2022language">Language Model Cascades<sup id="fnref-36"><a href="#fn-36" class="footnote-ref">36</a></sup><a href="#language-model-cascadesdohan2022language" class="hash-link" aria-label="language-model-cascadesdohan2022language에 대한 직접 링크" title="language-model-cascadesdohan2022language에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="design-guidelines-for-prompt-engineering-text-to-image-generative-modelsliu2022design">Design Guidelines for Prompt Engineering Text-to-Image Generative Models<sup id="fnref-37"><a href="#fn-37" class="footnote-ref">37</a></sup><a href="#design-guidelines-for-prompt-engineering-text-to-image-generative-modelsliu2022design" class="hash-link" aria-label="design-guidelines-for-prompt-engineering-text-to-image-generative-modelsliu2022design에 대한 직접 링크" title="design-guidelines-for-prompt-engineering-text-to-image-generative-modelsliu2022design에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="discovering-language-model-behaviors-with-model-written-evaluationsperez2022discovering">Discovering Language Model Behaviors with Model-Written Evaluations<sup id="fnref-38"><a href="#fn-38" class="footnote-ref">38</a></sup><a href="#discovering-language-model-behaviors-with-model-written-evaluationsperez2022discovering" class="hash-link" aria-label="discovering-language-model-behaviors-with-model-written-evaluationsperez2022discovering에 대한 직접 링크" title="discovering-language-model-behaviors-with-model-written-evaluationsperez2022discovering에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="selective-annotation-makes-language-models-better-few-shot-learnerssu2022selective">Selective Annotation Makes Language Models Better Few-Shot Learners<sup id="fnref-39"><a href="#fn-39" class="footnote-ref">39</a></sup><a href="#selective-annotation-makes-language-models-better-few-shot-learnerssu2022selective" class="hash-link" aria-label="selective-annotation-makes-language-models-better-few-shot-learnerssu2022selective에 대한 직접 링크" title="selective-annotation-makes-language-models-better-few-shot-learnerssu2022selective에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="atlas-few-shot-learning-with-retrieval-augmented-language-modelsizacard2022atlas">Atlas: Few-shot Learning with Retrieval Augmented Language Models<sup id="fnref-40"><a href="#fn-40" class="footnote-ref">40</a></sup><a href="#atlas-few-shot-learning-with-retrieval-augmented-language-modelsizacard2022atlas" class="hash-link" aria-label="atlas-few-shot-learning-with-retrieval-augmented-language-modelsizacard2022atlas에 대한 직접 링크" title="atlas-few-shot-learning-with-retrieval-augmented-language-modelsizacard2022atlas에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="strudel-structured-dialogue-summarization-for-dialogue-comprehensionwang2022strudel">STRUDEL: Structured Dialogue Summarization for Dialogue Comprehension<sup id="fnref-41"><a href="#fn-41" class="footnote-ref">41</a></sup><a href="#strudel-structured-dialogue-summarization-for-dialogue-comprehensionwang2022strudel" class="hash-link" aria-label="strudel-structured-dialogue-summarization-for-dialogue-comprehensionwang2022strudel에 대한 직접 링크" title="strudel-structured-dialogue-summarization-for-dialogue-comprehensionwang2022strudel에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="prompting-is-programming-a-query-language-for-large-language-modelsbeurerkellner2022prompting">Prompting Is Programming: A Query Language For Large Language Models<sup id="fnref-42"><a href="#fn-42" class="footnote-ref">42</a></sup><a href="#prompting-is-programming-a-query-language-for-large-language-modelsbeurerkellner2022prompting" class="hash-link" aria-label="prompting-is-programming-a-query-language-for-large-language-modelsbeurerkellner2022prompting에 대한 직접 링크" title="prompting-is-programming-a-query-language-for-large-language-modelsbeurerkellner2022prompting에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="parallel-context-windows-improve-in-context-learning-of-large-language-modelsratner2022parallel">Parallel Context Windows Improve In-Context Learning of Large Language Models<sup id="fnref-43"><a href="#fn-43" class="footnote-ref">43</a></sup><a href="#parallel-context-windows-improve-in-context-learning-of-large-language-modelsratner2022parallel" class="hash-link" aria-label="parallel-context-windows-improve-in-context-learning-of-large-language-modelsratner2022parallel에 대한 직접 링크" title="parallel-context-windows-improve-in-context-learning-of-large-language-modelsratner2022parallel에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="learning-to-perform-complex-tasks-through-compositional-fine-tuning-of-language-modelsbursztyn2022learning">Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models<sup id="fnref-44"><a href="#fn-44" class="footnote-ref">44</a></sup><a href="#learning-to-perform-complex-tasks-through-compositional-fine-tuning-of-language-modelsbursztyn2022learning" class="hash-link" aria-label="learning-to-perform-complex-tasks-through-compositional-fine-tuning-of-language-modelsbursztyn2022learning에 대한 직접 링크" title="learning-to-perform-complex-tasks-through-compositional-fine-tuning-of-language-modelsbursztyn2022learning에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="super-naturalinstructions-generalization-via-declarative-instructions-on-1600-nlp-taskswang2022supernaturalinstructions">Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks<sup id="fnref-45"><a href="#fn-45" class="footnote-ref">45</a></sup><a href="#super-naturalinstructions-generalization-via-declarative-instructions-on-1600-nlp-taskswang2022supernaturalinstructions" class="hash-link" aria-label="super-naturalinstructions-generalization-via-declarative-instructions-on-1600-nlp-taskswang2022supernaturalinstructions에 대한 직접 링크" title="super-naturalinstructions-generalization-via-declarative-instructions-on-1600-nlp-taskswang2022supernaturalinstructions에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="making-pre-trained-language-models-better-few-shot-learnersgao2021making">Making Pre-trained Language Models Better Few-shot Learners<sup id="fnref-46"><a href="#fn-46" class="footnote-ref">46</a></sup><a href="#making-pre-trained-language-models-better-few-shot-learnersgao2021making" class="hash-link" aria-label="making-pre-trained-language-models-better-few-shot-learnersgao2021making에 대한 직접 링크" title="making-pre-trained-language-models-better-few-shot-learnersgao2021making에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-prompt-opportunities-and-challenges-of-zero--and-few-shot-learning-for-human-ai-interaction-in-creative-applications-of-generative-modelsdang2022prompt">How to Prompt? Opportunities and Challenges of Zero- and Few-Shot Learning for Human-AI Interaction in Creative Applications of Generative Models<sup id="fnref-47"><a href="#fn-47" class="footnote-ref">47</a></sup><a href="#how-to-prompt-opportunities-and-challenges-of-zero--and-few-shot-learning-for-human-ai-interaction-in-creative-applications-of-generative-modelsdang2022prompt" class="hash-link" aria-label="how-to-prompt-opportunities-and-challenges-of-zero--and-few-shot-learning-for-human-ai-interaction-in-creative-applications-of-generative-modelsdang2022prompt에 대한 직접 링크" title="how-to-prompt-opportunities-and-challenges-of-zero--and-few-shot-learning-for-human-ai-interaction-in-creative-applications-of-generative-modelsdang2022prompt에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="on-measuring-social-biases-in-prompt-based-multi-task-learningakyrek2022measuring">On Measuring Social Biases in Prompt-Based Multi-Task Learning<sup id="fnref-48"><a href="#fn-48" class="footnote-ref">48</a></sup><a href="#on-measuring-social-biases-in-prompt-based-multi-task-learningakyrek2022measuring" class="hash-link" aria-label="on-measuring-social-biases-in-prompt-based-multi-task-learningakyrek2022measuring에 대한 직접 링크" title="on-measuring-social-biases-in-prompt-based-multi-task-learningakyrek2022measuring에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="plot-writing-from-pre-trained-language-modelsjin2022plot">Plot Writing From Pre-Trained Language Models<sup id="fnref-49"><a href="#fn-49" class="footnote-ref">49</a></sup><a href="#plot-writing-from-pre-trained-language-modelsjin2022plot" class="hash-link" aria-label="plot-writing-from-pre-trained-language-modelsjin2022plot에 대한 직접 링크" title="plot-writing-from-pre-trained-language-modelsjin2022plot에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="stereoset-measuring-stereotypical-bias-in-pretrained-language-modelsnadeem-etal-2021-stereoset">{S}tereo{S}et: Measuring stereotypical bias in pretrained language models<sup id="fnref-50"><a href="#fn-50" class="footnote-ref">50</a></sup><a href="#stereoset-measuring-stereotypical-bias-in-pretrained-language-modelsnadeem-etal-2021-stereoset" class="hash-link" aria-label="stereoset-measuring-stereotypical-bias-in-pretrained-language-modelsnadeem-etal-2021-stereoset에 대한 직접 링크" title="stereoset-measuring-stereotypical-bias-in-pretrained-language-modelsnadeem-etal-2021-stereoset에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="survey-of-hallucination-in-natural-language-generationji_2022">Survey of Hallucination in Natural Language Generation<sup id="fnref-51"><a href="#fn-51" class="footnote-ref">51</a></sup><a href="#survey-of-hallucination-in-natural-language-generationji_2022" class="hash-link" aria-label="survey-of-hallucination-in-natural-language-generationji_2022에 대한 직접 링크" title="survey-of-hallucination-in-natural-language-generationji_2022에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="wordcraft-story-writing-with-large-language-modelsyuan2022wordcraft">Wordcraft: Story Writing With Large Language Models<sup id="fnref-52"><a href="#fn-52" class="footnote-ref">52</a></sup><a href="#wordcraft-story-writing-with-large-language-modelsyuan2022wordcraft" class="hash-link" aria-label="wordcraft-story-writing-with-large-language-modelsyuan2022wordcraft에 대한 직접 링크" title="wordcraft-story-writing-with-large-language-modelsyuan2022wordcraft에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="painpoints-a-framework-for-language-based-detection-of-chronic-pain-and-expert-collaborative-text-summarizationfadnavis2022pain">PainPoints: A Framework for Language-based Detection of Chronic Pain and Expert-Collaborative Text-Summarization<sup id="fnref-53"><a href="#fn-53" class="footnote-ref">53</a></sup><a href="#painpoints-a-framework-for-language-based-detection-of-chronic-pain-and-expert-collaborative-text-summarizationfadnavis2022pain" class="hash-link" aria-label="painpoints-a-framework-for-language-based-detection-of-chronic-pain-and-expert-collaborative-text-summarizationfadnavis2022pain에 대한 직접 링크" title="painpoints-a-framework-for-language-based-detection-of-chronic-pain-and-expert-collaborative-text-summarizationfadnavis2022pain에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="self-instruct-aligning-language-model-with-self-generated-instructionswang2022selfinstruct">Self-Instruct: Aligning Language Model with Self Generated Instructions<sup id="fnref-54"><a href="#fn-54" class="footnote-ref">54</a></sup><a href="#self-instruct-aligning-language-model-with-self-generated-instructionswang2022selfinstruct" class="hash-link" aria-label="self-instruct-aligning-language-model-with-self-generated-instructionswang2022selfinstruct에 대한 직접 링크" title="self-instruct-aligning-language-model-with-self-generated-instructionswang2022selfinstruct에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="from-images-to-textual-prompts-zero-shot-vqa-with-frozen-large-language-modelsguo2022images">From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models<sup id="fnref-55"><a href="#fn-55" class="footnote-ref">55</a></sup><a href="#from-images-to-textual-prompts-zero-shot-vqa-with-frozen-large-language-modelsguo2022images" class="hash-link" aria-label="from-images-to-textual-prompts-zero-shot-vqa-with-frozen-large-language-modelsguo2022images에 대한 직접 링크" title="from-images-to-textual-prompts-zero-shot-vqa-with-frozen-large-language-modelsguo2022images에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-and-improved-content-moderation-toolingmarkov_2022">New and improved content moderation tooling<sup id="fnref-56"><a href="#fn-56" class="footnote-ref">56</a></sup><a href="#new-and-improved-content-moderation-toolingmarkov_2022" class="hash-link" aria-label="new-and-improved-content-moderation-toolingmarkov_2022에 대한 직접 링크" title="new-and-improved-content-moderation-toolingmarkov_2022에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="exploiting-cloze-questions-for-few-shot-text-classification-and-natural-language-inferenceschick2020exploiting">Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference<sup id="fnref-57"><a href="#fn-57" class="footnote-ref">57</a></sup><a href="#exploiting-cloze-questions-for-few-shot-text-classification-and-natural-language-inferenceschick2020exploiting" class="hash-link" aria-label="exploiting-cloze-questions-for-few-shot-text-classification-and-natural-language-inferenceschick2020exploiting에 대한 직접 링크" title="exploiting-cloze-questions-for-few-shot-text-classification-and-natural-language-inferenceschick2020exploiting에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="human-level-concept-learning-through-probabilistic-program-inductionlake2015human">Human-level concept learning through probabilistic program induction<sup id="fnref-58"><a href="#fn-58" class="footnote-ref">58</a></sup><a href="#human-level-concept-learning-through-probabilistic-program-inductionlake2015human" class="hash-link" aria-label="human-level-concept-learning-through-probabilistic-program-inductionlake2015human에 대한 직접 링크" title="human-level-concept-learning-through-probabilistic-program-inductionlake2015human에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="riffusion---stable-diffusion-for-real-time-music-generationforsgren_martiros_2022">{Riffusion - Stable diffusion for real-time music generation}<sup id="fnref-59"><a href="#fn-59" class="footnote-ref">59</a></sup><a href="#riffusion---stable-diffusion-for-real-time-music-generationforsgren_martiros_2022" class="hash-link" aria-label="riffusion---stable-diffusion-for-real-time-music-generationforsgren_martiros_2022에 대한 직접 링크" title="riffusion---stable-diffusion-for-real-time-music-generationforsgren_martiros_2022에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-use-openais-chatgpt-to-write-the-perfect-cold-emailbonta2022how">How to use OpenAI’s ChatGPT to write the perfect cold email<sup id="fnref-60"><a href="#fn-60" class="footnote-ref">60</a></sup><a href="#how-to-use-openais-chatgpt-to-write-the-perfect-cold-emailbonta2022how" class="hash-link" aria-label="how-to-use-openais-chatgpt-to-write-the-perfect-cold-emailbonta2022how에 대한 직접 링크" title="how-to-use-openais-chatgpt-to-write-the-perfect-cold-emailbonta2022how에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="cacti-biology-and-usesnobel2002cacti">Cacti: biology and uses<sup id="fnref-61"><a href="#fn-61" class="footnote-ref">61</a></sup><a href="#cacti-biology-and-usesnobel2002cacti" class="hash-link" aria-label="cacti-biology-and-usesnobel2002cacti에 대한 직접 링크" title="cacti-biology-and-usesnobel2002cacti에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="are-language-models-worse-than-humans-at-following-prompts-its-complicatedwebson2023itscomplicated">Are Language Models Worse than Humans at Following Prompts? It’s Complicated<sup id="fnref-62"><a href="#fn-62" class="footnote-ref">62</a></sup><a href="#are-language-models-worse-than-humans-at-following-prompts-its-complicatedwebson2023itscomplicated" class="hash-link" aria-label="are-language-models-worse-than-humans-at-following-prompts-its-complicatedwebson2023itscomplicated에 대한 직접 링크" title="are-language-models-worse-than-humans-at-following-prompts-its-complicatedwebson2023itscomplicated에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="unleashing-cognitive-synergy-in-large-language-models-a-task-solving-agent-through-multi-persona-self-collaborationwang2023unleashing">Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration<sup id="fnref-63"><a href="#fn-63" class="footnote-ref">63</a></sup><a href="#unleashing-cognitive-synergy-in-large-language-models-a-task-solving-agent-through-multi-persona-self-collaborationwang2023unleashing" class="hash-link" aria-label="unleashing-cognitive-synergy-in-large-language-models-a-task-solving-agent-through-multi-persona-self-collaborationwang2023unleashing에 대한 직접 링크" title="unleashing-cognitive-synergy-in-large-language-models-a-task-solving-agent-through-multi-persona-self-collaborationwang2023unleashing에 대한 직접 링크">​</a></h4><h2 class="anchor anchorWithStickyNavbar_LWe7" id="prompt-hacking">Prompt Hacking<a href="#prompt-hacking" class="hash-link" aria-label="Prompt Hacking에 대한 직접 링크" title="Prompt Hacking에 대한 직접 링크">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="machine-generated-text-a-comprehensive-survey-of-threat-models-and-detection-methodscrothers2022machine">Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods<sup id="fnref-64"><a href="#fn-64" class="footnote-ref">64</a></sup><a href="#machine-generated-text-a-comprehensive-survey-of-threat-models-and-detection-methodscrothers2022machine" class="hash-link" aria-label="machine-generated-text-a-comprehensive-survey-of-threat-models-and-detection-methodscrothers2022machine에 대한 직접 링크" title="machine-generated-text-a-comprehensive-survey-of-threat-models-and-detection-methodscrothers2022machine에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="new-jailbreak-based-on-virtual-functions---smuggle-illegal-tokens-to-the-backendnin2023new">New jailbreak based on virtual functions - smuggle illegal tokens to the backend.<sup id="fnref-65"><a href="#fn-65" class="footnote-ref">65</a></sup><a href="#new-jailbreak-based-on-virtual-functions---smuggle-illegal-tokens-to-the-backendnin2023new" class="hash-link" aria-label="new-jailbreak-based-on-virtual-functions---smuggle-illegal-tokens-to-the-backendnin2023new에 대한 직접 링크" title="new-jailbreak-based-on-virtual-functions---smuggle-illegal-tokens-to-the-backendnin2023new에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="exploiting-programmatic-behavior-of-llms-dual-use-through-standard-security-attackskang2023exploiting">Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks<sup id="fnref-66"><a href="#fn-66" class="footnote-ref">66</a></sup><a href="#exploiting-programmatic-behavior-of-llms-dual-use-through-standard-security-attackskang2023exploiting" class="hash-link" aria-label="exploiting-programmatic-behavior-of-llms-dual-use-through-standard-security-attackskang2023exploiting에 대한 직접 링크" title="exploiting-programmatic-behavior-of-llms-dual-use-through-standard-security-attackskang2023exploiting에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="more-than-youve-asked-for-a-comprehensive-analysis-of-novel-prompt-injection-threats-to-application-integrated-large-language-modelsgreshake2023youve">More than you&#x27;ve asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models<sup id="fnref-67"><a href="#fn-67" class="footnote-ref">67</a></sup><a href="#more-than-youve-asked-for-a-comprehensive-analysis-of-novel-prompt-injection-threats-to-application-integrated-large-language-modelsgreshake2023youve" class="hash-link" aria-label="more-than-youve-asked-for-a-comprehensive-analysis-of-novel-prompt-injection-threats-to-application-integrated-large-language-modelsgreshake2023youve에 대한 직접 링크" title="more-than-youve-asked-for-a-comprehensive-analysis-of-novel-prompt-injection-threats-to-application-integrated-large-language-modelsgreshake2023youve에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="chatgpt-dan-and-other-jailbreakskiho2023chatgpt">ChatGPT &quot;DAN&quot; (and other &quot;Jailbreaks&quot;)<sup id="fnref-68"><a href="#fn-68" class="footnote-ref">68</a></sup><a href="#chatgpt-dan-and-other-jailbreakskiho2023chatgpt" class="hash-link" aria-label="chatgpt-dan-and-other-jailbreakskiho2023chatgpt에 대한 직접 링크" title="chatgpt-dan-and-other-jailbreakskiho2023chatgpt에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="evaluating-the-susceptibility-of-pre-trained-language-models-via-handcrafted-adversarial-examplesbranch2022evaluating">Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples<sup id="fnref-69"><a href="#fn-69" class="footnote-ref">69</a></sup><a href="#evaluating-the-susceptibility-of-pre-trained-language-models-via-handcrafted-adversarial-examplesbranch2022evaluating" class="hash-link" aria-label="evaluating-the-susceptibility-of-pre-trained-language-models-via-handcrafted-adversarial-examplesbranch2022evaluating에 대한 직접 링크" title="evaluating-the-susceptibility-of-pre-trained-language-models-via-handcrafted-adversarial-examplesbranch2022evaluating에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="prompt-injection-attacks-against-gpt-3simon2022inject">Prompt injection attacks against GPT-3<sup id="fnref-70"><a href="#fn-70" class="footnote-ref">70</a></sup><a href="#prompt-injection-attacks-against-gpt-3simon2022inject" class="hash-link" aria-label="prompt-injection-attacks-against-gpt-3simon2022inject에 대한 직접 링크" title="prompt-injection-attacks-against-gpt-3simon2022inject에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="exploiting-gpt-3-prompts-with-malicious-inputs-that-order-the-model-to-ignore-its-previous-directionsgoodside2022inject">Exploiting GPT-3 prompts with malicious inputs that order the model to ignore its previous directions<sup id="fnref-71"><a href="#fn-71" class="footnote-ref">71</a></sup><a href="#exploiting-gpt-3-prompts-with-malicious-inputs-that-order-the-model-to-ignore-its-previous-directionsgoodside2022inject" class="hash-link" aria-label="exploiting-gpt-3-prompts-with-malicious-inputs-that-order-the-model-to-ignore-its-previous-directionsgoodside2022inject에 대한 직접 링크" title="exploiting-gpt-3-prompts-with-malicious-inputs-that-order-the-model-to-ignore-its-previous-directionsgoodside2022inject에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="history-correctiongoodside2022history">History Correction<sup id="fnref-72"><a href="#fn-72" class="footnote-ref">72</a></sup><a href="#history-correctiongoodside2022history" class="hash-link" aria-label="history-correctiongoodside2022history에 대한 직접 링크" title="history-correctiongoodside2022history에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="adversarial-promptschase2021adversarial">adversarial-prompts<sup id="fnref-73"><a href="#fn-73" class="footnote-ref">73</a></sup><a href="#adversarial-promptschase2021adversarial" class="hash-link" aria-label="adversarial-promptschase2021adversarial에 대한 직접 링크" title="adversarial-promptschase2021adversarial에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="gpt-3-prompt-injection-defensesgoodside2021gpt">GPT-3 Prompt Injection Defenses<sup id="fnref-74"><a href="#fn-74" class="footnote-ref">74</a></sup><a href="#gpt-3-prompt-injection-defensesgoodside2021gpt" class="hash-link" aria-label="gpt-3-prompt-injection-defensesgoodside2021gpt에 대한 직접 링크" title="gpt-3-prompt-injection-defensesgoodside2021gpt에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="talking-to-machines-prompt-engineering--injectionchristoph2022talking">Talking to machines: prompt engineering &amp; injection<sup id="fnref-75"><a href="#fn-75" class="footnote-ref">75</a></sup><a href="#talking-to-machines-prompt-engineering--injectionchristoph2022talking" class="hash-link" aria-label="talking-to-machines-prompt-engineering--injectionchristoph2022talking에 대한 직접 링크" title="talking-to-machines-prompt-engineering--injectionchristoph2022talking에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="using-gpt-eliezer-against-chatgpt-jailbreakingarmstrong2022using">Using GPT-Eliezer against ChatGPT Jailbreaking<sup id="fnref-76"><a href="#fn-76" class="footnote-ref">76</a></sup><a href="#using-gpt-eliezer-against-chatgpt-jailbreakingarmstrong2022using" class="hash-link" aria-label="using-gpt-eliezer-against-chatgpt-jailbreakingarmstrong2022using에 대한 직접 링크" title="using-gpt-eliezer-against-chatgpt-jailbreakingarmstrong2022using에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="exploring-prompt-injection-attacksselvi2022exploring">Exploring Prompt Injection Attacks<sup id="fnref-77"><a href="#fn-77" class="footnote-ref">77</a></sup><a href="#exploring-prompt-injection-attacksselvi2022exploring" class="hash-link" aria-label="exploring-prompt-injection-attacksselvi2022exploring에 대한 직접 링크" title="exploring-prompt-injection-attacksselvi2022exploring에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="the-entire-prompt-of-microsoft-bing-chat-hi-sydneykevinbing">The entire prompt of Microsoft Bing Chat?! (Hi, Sydney.)<sup id="fnref-78"><a href="#fn-78" class="footnote-ref">78</a></sup><a href="#the-entire-prompt-of-microsoft-bing-chat-hi-sydneykevinbing" class="hash-link" aria-label="the-entire-prompt-of-microsoft-bing-chat-hi-sydneykevinbing에 대한 직접 링크" title="the-entire-prompt-of-microsoft-bing-chat-hi-sydneykevinbing에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="ignore-previous-prompt-attack-techniques-for-language-modelsperez2022jailbreak">Ignore Previous Prompt: Attack Techniques For Language Models<sup id="fnref-79"><a href="#fn-79" class="footnote-ref">79</a></sup><a href="#ignore-previous-prompt-attack-techniques-for-language-modelsperez2022jailbreak" class="hash-link" aria-label="ignore-previous-prompt-attack-techniques-for-language-modelsperez2022jailbreak에 대한 직접 링크" title="ignore-previous-prompt-attack-techniques-for-language-modelsperez2022jailbreak에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="lessons-learned-on-language-model-safety-and-misusebrundage_2022">Lessons learned on Language Model Safety and misuse<sup id="fnref-80"><a href="#fn-80" class="footnote-ref">80</a></sup><a href="#lessons-learned-on-language-model-safety-and-misusebrundage_2022" class="hash-link" aria-label="lessons-learned-on-language-model-safety-and-misusebrundage_2022에 대한 직접 링크" title="lessons-learned-on-language-model-safety-and-misusebrundage_2022에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="toxicity-detection-with-generative-prompt-based-inferencewang2022jailbreak">Toxicity Detection with Generative Prompt-based Inference<sup id="fnref-81"><a href="#fn-81" class="footnote-ref">81</a></sup><a href="#toxicity-detection-with-generative-prompt-based-inferencewang2022jailbreak" class="hash-link" aria-label="toxicity-detection-with-generative-prompt-based-inferencewang2022jailbreak에 대한 직접 링크" title="toxicity-detection-with-generative-prompt-based-inferencewang2022jailbreak에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="ok-i-saw-a-few-people-jailbreaking-safeguards-openai-put-on-chatgpt-so-i-had-to-give-it-a-shot-myselfalice2022jailbreak">ok I saw a few people jailbreaking safeguards openai put on chatgpt so I had to give it a shot myself<sup id="fnref-82"><a href="#fn-82" class="footnote-ref">82</a></sup><a href="#ok-i-saw-a-few-people-jailbreaking-safeguards-openai-put-on-chatgpt-so-i-had-to-give-it-a-shot-myselfalice2022jailbreak" class="hash-link" aria-label="ok-i-saw-a-few-people-jailbreaking-safeguards-openai-put-on-chatgpt-so-i-had-to-give-it-a-shot-myselfalice2022jailbreak에 대한 직접 링크" title="ok-i-saw-a-few-people-jailbreaking-safeguards-openai-put-on-chatgpt-so-i-had-to-give-it-a-shot-myselfalice2022jailbreak에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="bypass-openais-chatgpt-alignment-efforts-with-this-one-weird-trickmiguel2022jailbreak">Bypass @OpenAI&#x27;s ChatGPT alignment efforts with this one weird trick<sup id="fnref-83"><a href="#fn-83" class="footnote-ref">83</a></sup><a href="#bypass-openais-chatgpt-alignment-efforts-with-this-one-weird-trickmiguel2022jailbreak" class="hash-link" aria-label="bypass-openais-chatgpt-alignment-efforts-with-this-one-weird-trickmiguel2022jailbreak에 대한 직접 링크" title="bypass-openais-chatgpt-alignment-efforts-with-this-one-weird-trickmiguel2022jailbreak에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="chatgpt-jailbreaking-itselfderek2022jailbreak">ChatGPT jailbreaking itself<sup id="fnref-84"><a href="#fn-84" class="footnote-ref">84</a></sup><a href="#chatgpt-jailbreaking-itselfderek2022jailbreak" class="hash-link" aria-label="chatgpt-jailbreaking-itselfderek2022jailbreak에 대한 직접 링크" title="chatgpt-jailbreaking-itselfderek2022jailbreak에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="using-pretend-on-chatgpt-can-do-some-wild-stuff-you-can-kind-of-get-some-insight-on-the-future-alternative-universenero2022jailbreak">Using &quot;pretend&quot; on #ChatGPT can do some wild stuff. You can kind of get some insight on the future, alternative universe.<sup id="fnref-85"><a href="#fn-85" class="footnote-ref">85</a></sup><a href="#using-pretend-on-chatgpt-can-do-some-wild-stuff-you-can-kind-of-get-some-insight-on-the-future-alternative-universenero2022jailbreak" class="hash-link" aria-label="using-pretend-on-chatgpt-can-do-some-wild-stuff-you-can-kind-of-get-some-insight-on-the-future-alternative-universenero2022jailbreak에 대한 직접 링크" title="using-pretend-on-chatgpt-can-do-some-wild-stuff-you-can-kind-of-get-some-insight-on-the-future-alternative-universenero2022jailbreak에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="i-kinda-like-this-one-even-morenick2022jailbreak">I kinda like this one even more!<sup id="fnref-86"><a href="#fn-86" class="footnote-ref">86</a></sup><a href="#i-kinda-like-this-one-even-morenick2022jailbreak" class="hash-link" aria-label="i-kinda-like-this-one-even-morenick2022jailbreak에 대한 직접 링크" title="i-kinda-like-this-one-even-morenick2022jailbreak에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="uh-ohsam2022jailbreak">uh oh<sup id="fnref-87"><a href="#fn-87" class="footnote-ref">87</a></sup><a href="#uh-ohsam2022jailbreak" class="hash-link" aria-label="uh-ohsam2022jailbreak에 대한 직접 링크" title="uh-ohsam2022jailbreak에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="building-a-virtual-machine-inside-chatgptjonas2022jailbreak">Building A Virtual Machine inside ChatGPT<sup id="fnref-88"><a href="#fn-88" class="footnote-ref">88</a></sup><a href="#building-a-virtual-machine-inside-chatgptjonas2022jailbreak" class="hash-link" aria-label="building-a-virtual-machine-inside-chatgptjonas2022jailbreak에 대한 직접 링크" title="building-a-virtual-machine-inside-chatgptjonas2022jailbreak에 대한 직접 링크">​</a></h4><h2 class="anchor anchorWithStickyNavbar_LWe7" id="reliability">Reliability<a href="#reliability" class="hash-link" aria-label="Reliability에 대한 직접 링크" title="Reliability에 대한 직접 링크">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="mathprompter-mathematical-reasoning-using-large-language-modelsimani2023mathprompter">MathPrompter: Mathematical Reasoning using Large Language Models<sup id="fnref-89"><a href="#fn-89" class="footnote-ref">89</a></sup><a href="#mathprompter-mathematical-reasoning-using-large-language-modelsimani2023mathprompter" class="hash-link" aria-label="mathprompter-mathematical-reasoning-using-large-language-modelsimani2023mathprompter에 대한 직접 링크" title="mathprompter-mathematical-reasoning-using-large-language-modelsimani2023mathprompter에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="the-unreliability-of-explanations-in-few-shot-prompting-for-textual-reasoningye2022unreliability">The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning<sup id="fnref-90"><a href="#fn-90" class="footnote-ref">90</a></sup><a href="#the-unreliability-of-explanations-in-few-shot-prompting-for-textual-reasoningye2022unreliability" class="hash-link" aria-label="the-unreliability-of-explanations-in-few-shot-prompting-for-textual-reasoningye2022unreliability에 대한 직접 링크" title="the-unreliability-of-explanations-in-few-shot-prompting-for-textual-reasoningye2022unreliability에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="prompting-gpt-3-to-be-reliablesi2022prompting">Prompting GPT-3 To Be Reliable<sup id="fnref-91"><a href="#fn-91" class="footnote-ref">91</a></sup><a href="#prompting-gpt-3-to-be-reliablesi2022prompting" class="hash-link" aria-label="prompting-gpt-3-to-be-reliablesi2022prompting에 대한 직접 링크" title="prompting-gpt-3-to-be-reliablesi2022prompting에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="on-the-advance-of-making-language-models-better-reasonersli2022advance">On the Advance of Making Language Models Better Reasoners<sup id="fnref-92"><a href="#fn-92" class="footnote-ref">92</a></sup><a href="#on-the-advance-of-making-language-models-better-reasonersli2022advance" class="hash-link" aria-label="on-the-advance-of-making-language-models-better-reasonersli2022advance에 대한 직접 링크" title="on-the-advance-of-making-language-models-better-reasonersli2022advance에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="ask-me-anything-a-simple-strategy-for-prompting-language-modelsarora2022ama">Ask Me Anything: A simple strategy for prompting language models<sup id="fnref-93"><a href="#fn-93" class="footnote-ref">93</a></sup><a href="#ask-me-anything-a-simple-strategy-for-prompting-language-modelsarora2022ama" class="hash-link" aria-label="ask-me-anything-a-simple-strategy-for-prompting-language-modelsarora2022ama에 대한 직접 링크" title="ask-me-anything-a-simple-strategy-for-prompting-language-modelsarora2022ama에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="calibrate-before-use-improving-few-shot-performance-of-language-modelszhao2021calibrate">Calibrate Before Use: Improving Few-Shot Performance of Language Models<sup id="fnref-94"><a href="#fn-94" class="footnote-ref">94</a></sup><a href="#calibrate-before-use-improving-few-shot-performance-of-language-modelszhao2021calibrate" class="hash-link" aria-label="calibrate-before-use-improving-few-shot-performance-of-language-modelszhao2021calibrate에 대한 직접 링크" title="calibrate-before-use-improving-few-shot-performance-of-language-modelszhao2021calibrate에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="can-large-language-models-reason-about-medical-questionslivin2022large">Can large language models reason about medical questions?<sup id="fnref-95"><a href="#fn-95" class="footnote-ref">95</a></sup><a href="#can-large-language-models-reason-about-medical-questionslivin2022large" class="hash-link" aria-label="can-large-language-models-reason-about-medical-questionslivin2022large에 대한 직접 링크" title="can-large-language-models-reason-about-medical-questionslivin2022large에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="enhancing-self-consistency-and-performance-of-pre-trained-language-models-through-natural-language-inferencemitchell2022enhancing">Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference<sup id="fnref-96"><a href="#fn-96" class="footnote-ref">96</a></sup><a href="#enhancing-self-consistency-and-performance-of-pre-trained-language-models-through-natural-language-inferencemitchell2022enhancing" class="hash-link" aria-label="enhancing-self-consistency-and-performance-of-pre-trained-language-models-through-natural-language-inferencemitchell2022enhancing에 대한 직접 링크" title="enhancing-self-consistency-and-performance-of-pre-trained-language-models-through-natural-language-inferencemitchell2022enhancing에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="on-second-thought-lets-not-think-step-by-step-bias-and-toxicity-in-zero-shot-reasoningshaikh2022second">On Second Thought, Let&#x27;s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning<sup id="fnref-97"><a href="#fn-97" class="footnote-ref">97</a></sup><a href="#on-second-thought-lets-not-think-step-by-step-bias-and-toxicity-in-zero-shot-reasoningshaikh2022second" class="hash-link" aria-label="on-second-thought-lets-not-think-step-by-step-bias-and-toxicity-in-zero-shot-reasoningshaikh2022second에 대한 직접 링크" title="on-second-thought-lets-not-think-step-by-step-bias-and-toxicity-in-zero-shot-reasoningshaikh2022second에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="evaluating-language-models-can-be-trickychase2022evaluating">Evaluating language models can be tricky<sup id="fnref-98"><a href="#fn-98" class="footnote-ref">98</a></sup><a href="#evaluating-language-models-can-be-trickychase2022evaluating" class="hash-link" aria-label="evaluating-language-models-can-be-trickychase2022evaluating에 대한 직접 링크" title="evaluating-language-models-can-be-trickychase2022evaluating에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="constitutional-ai-harmlessness-from-ai-feedbackbai2022constitutional">Constitutional AI: Harmlessness from AI Feedback<sup id="fnref-99"><a href="#fn-99" class="footnote-ref">99</a></sup><a href="#constitutional-ai-harmlessness-from-ai-feedbackbai2022constitutional" class="hash-link" aria-label="constitutional-ai-harmlessness-from-ai-feedbackbai2022constitutional에 대한 직접 링크" title="constitutional-ai-harmlessness-from-ai-feedbackbai2022constitutional에 대한 직접 링크">​</a></h4><h2 class="anchor anchorWithStickyNavbar_LWe7" id="surveys">Surveys<a href="#surveys" class="hash-link" aria-label="Surveys에 대한 직접 링크" title="Surveys에 대한 직접 링크">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="speech-and-language-processing-an-introduction-to-natural-language-processing-computational-linguistics-and-speech-recognitionjurafsky2009">Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics and Speech Recognition<sup id="fnref-100"><a href="#fn-100" class="footnote-ref">100</a></sup><a href="#speech-and-language-processing-an-introduction-to-natural-language-processing-computational-linguistics-and-speech-recognitionjurafsky2009" class="hash-link" aria-label="speech-and-language-processing-an-introduction-to-natural-language-processing-computational-linguistics-and-speech-recognitionjurafsky2009에 대한 직접 링크" title="speech-and-language-processing-an-introduction-to-natural-language-processing-computational-linguistics-and-speech-recognitionjurafsky2009에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="pre-train-prompt-and-predict-a-systematic-survey-of-prompting-methods-in-natural-language-processingliu2021pretrain">Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing<sup id="fnref-101"><a href="#fn-101" class="footnote-ref">101</a></sup><a href="#pre-train-prompt-and-predict-a-systematic-survey-of-prompting-methods-in-natural-language-processingliu2021pretrain" class="hash-link" aria-label="pre-train-prompt-and-predict-a-systematic-survey-of-prompting-methods-in-natural-language-processingliu2021pretrain에 대한 직접 링크" title="pre-train-prompt-and-predict-a-systematic-survey-of-prompting-methods-in-natural-language-processingliu2021pretrain에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="promptpapersning2022papers">PromptPapers<sup id="fnref-102"><a href="#fn-102" class="footnote-ref">102</a></sup><a href="#promptpapersning2022papers" class="hash-link" aria-label="promptpapersning2022papers에 대한 직접 링크" title="promptpapersning2022papers에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="a-prompt-pattern-catalog-to-enhance-prompt-engineering-with-chatgptwhite2023prompt">A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT<sup id="fnref-103"><a href="#fn-103" class="footnote-ref">103</a></sup><a href="#a-prompt-pattern-catalog-to-enhance-prompt-engineering-with-chatgptwhite2023prompt" class="hash-link" aria-label="a-prompt-pattern-catalog-to-enhance-prompt-engineering-with-chatgptwhite2023prompt에 대한 직접 링크" title="a-prompt-pattern-catalog-to-enhance-prompt-engineering-with-chatgptwhite2023prompt에 대한 직접 링크">​</a></h4><h2 class="anchor anchorWithStickyNavbar_LWe7" id="techniques">Techniques<a href="#techniques" class="hash-link" aria-label="Techniques에 대한 직접 링크" title="Techniques에 대한 직접 링크">​</a></h2><h4 class="anchor anchorWithStickyNavbar_LWe7" id="chain-of-thought-prompting-elicits-reasoning-in-large-language-modelswei2022chain">Chain of Thought Prompting Elicits Reasoning in Large Language Models<sup id="fnref-104"><a href="#fn-104" class="footnote-ref">104</a></sup><a href="#chain-of-thought-prompting-elicits-reasoning-in-large-language-modelswei2022chain" class="hash-link" aria-label="chain-of-thought-prompting-elicits-reasoning-in-large-language-modelswei2022chain에 대한 직접 링크" title="chain-of-thought-prompting-elicits-reasoning-in-large-language-modelswei2022chain에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="large-language-models-are-zero-shot-reasonerskojima2022large">Large Language Models are Zero-Shot Reasoners<sup id="fnref-105"><a href="#fn-105" class="footnote-ref">105</a></sup><a href="#large-language-models-are-zero-shot-reasonerskojima2022large" class="hash-link" aria-label="large-language-models-are-zero-shot-reasonerskojima2022large에 대한 직접 링크" title="large-language-models-are-zero-shot-reasonerskojima2022large에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="self-consistency-improves-chain-of-thought-reasoning-in-language-modelswang2022selfconsistency">Self-Consistency Improves Chain of Thought Reasoning in Language Models<sup id="fnref-106"><a href="#fn-106" class="footnote-ref">106</a></sup><a href="#self-consistency-improves-chain-of-thought-reasoning-in-language-modelswang2022selfconsistency" class="hash-link" aria-label="self-consistency-improves-chain-of-thought-reasoning-in-language-modelswang2022selfconsistency에 대한 직접 링크" title="self-consistency-improves-chain-of-thought-reasoning-in-language-modelswang2022selfconsistency에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="what-makes-good-in-context-examples-for-gpt-3liu2021makes">What Makes Good In-Context Examples for GPT-3?<sup id="fnref-107"><a href="#fn-107" class="footnote-ref">107</a></sup><a href="#what-makes-good-in-context-examples-for-gpt-3liu2021makes" class="hash-link" aria-label="what-makes-good-in-context-examples-for-gpt-3liu2021makes에 대한 직접 링크" title="what-makes-good-in-context-examples-for-gpt-3liu2021makes에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="generated-knowledge-prompting-for-commonsense-reasoningliu2021generated">Generated Knowledge Prompting for Commonsense Reasoning<sup id="fnref-108"><a href="#fn-108" class="footnote-ref">108</a></sup><a href="#generated-knowledge-prompting-for-commonsense-reasoningliu2021generated" class="hash-link" aria-label="generated-knowledge-prompting-for-commonsense-reasoningliu2021generated에 대한 직접 링크" title="generated-knowledge-prompting-for-commonsense-reasoningliu2021generated에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="recitation-augmented-language-modelssun2022recitationaugmented">Recitation-Augmented Language Models<sup id="fnref-109"><a href="#fn-109" class="footnote-ref">109</a></sup><a href="#recitation-augmented-language-modelssun2022recitationaugmented" class="hash-link" aria-label="recitation-augmented-language-modelssun2022recitationaugmented에 대한 직접 링크" title="recitation-augmented-language-modelssun2022recitationaugmented에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="rethinking-the-role-of-demonstrations-what-makes-in-context-learning-workmin2022rethinking">Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?<sup id="fnref-110"><a href="#fn-110" class="footnote-ref">110</a></sup><a href="#rethinking-the-role-of-demonstrations-what-makes-in-context-learning-workmin2022rethinking" class="hash-link" aria-label="rethinking-the-role-of-demonstrations-what-makes-in-context-learning-workmin2022rethinking에 대한 직접 링크" title="rethinking-the-role-of-demonstrations-what-makes-in-context-learning-workmin2022rethinking에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="show-your-work-scratchpads-for-intermediate-computation-with-language-modelsnye2021work">Show Your Work: Scratchpads for Intermediate Computation with Language Models<sup id="fnref-111"><a href="#fn-111" class="footnote-ref">111</a></sup><a href="#show-your-work-scratchpads-for-intermediate-computation-with-language-modelsnye2021work" class="hash-link" aria-label="show-your-work-scratchpads-for-intermediate-computation-with-language-modelsnye2021work에 대한 직접 링크" title="show-your-work-scratchpads-for-intermediate-computation-with-language-modelsnye2021work에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="maieutic-prompting-logically-consistent-reasoning-with-recursive-explanationsjung2022maieutic">Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations<sup id="fnref-112"><a href="#fn-112" class="footnote-ref">112</a></sup><a href="#maieutic-prompting-logically-consistent-reasoning-with-recursive-explanationsjung2022maieutic" class="hash-link" aria-label="maieutic-prompting-logically-consistent-reasoning-with-recursive-explanationsjung2022maieutic에 대한 직접 링크" title="maieutic-prompting-logically-consistent-reasoning-with-recursive-explanationsjung2022maieutic에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="star-bootstrapping-reasoning-with-reasoningzelikman2022star">STaR: Bootstrapping Reasoning With Reasoning<sup id="fnref-113"><a href="#fn-113" class="footnote-ref">113</a></sup><a href="#star-bootstrapping-reasoning-with-reasoningzelikman2022star" class="hash-link" aria-label="star-bootstrapping-reasoning-with-reasoningzelikman2022star에 대한 직접 링크" title="star-bootstrapping-reasoning-with-reasoningzelikman2022star에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="least-to-most-prompting-enables-complex-reasoning-in-large-language-modelszhou2022leasttomost">Least-to-Most Prompting Enables Complex Reasoning in Large Language Models<sup id="fnref-114"><a href="#fn-114" class="footnote-ref">114</a></sup><a href="#least-to-most-prompting-enables-complex-reasoning-in-large-language-modelszhou2022leasttomost" class="hash-link" aria-label="least-to-most-prompting-enables-complex-reasoning-in-large-language-modelszhou2022leasttomost에 대한 직접 링크" title="least-to-most-prompting-enables-complex-reasoning-in-large-language-modelszhou2022leasttomost에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="reframing-instructional-prompts-to-gptks-languagemishra2022reframing">Reframing Instructional Prompts to GPTk’s Language<sup id="fnref-115"><a href="#fn-115" class="footnote-ref">115</a></sup><a href="#reframing-instructional-prompts-to-gptks-languagemishra2022reframing" class="hash-link" aria-label="reframing-instructional-prompts-to-gptks-languagemishra2022reframing에 대한 직접 링크" title="reframing-instructional-prompts-to-gptks-languagemishra2022reframing에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="cutting-down-on-prompts-and-parameters-simple-few-shot-learning-with-language-modelslogan-iv-etal-2022-cutting">Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models<sup id="fnref-116"><a href="#fn-116" class="footnote-ref">116</a></sup><a href="#cutting-down-on-prompts-and-parameters-simple-few-shot-learning-with-language-modelslogan-iv-etal-2022-cutting" class="hash-link" aria-label="cutting-down-on-prompts-and-parameters-simple-few-shot-learning-with-language-modelslogan-iv-etal-2022-cutting에 대한 직접 링크" title="cutting-down-on-prompts-and-parameters-simple-few-shot-learning-with-language-modelslogan-iv-etal-2022-cutting에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="role-play-with-large-language-modelsshanahan2023roleplay">Role-Play with Large Language Models<sup id="fnref-117"><a href="#fn-117" class="footnote-ref">117</a></sup><a href="#role-play-with-large-language-modelsshanahan2023roleplay" class="hash-link" aria-label="role-play-with-large-language-modelsshanahan2023roleplay에 대한 직접 링크" title="role-play-with-large-language-modelsshanahan2023roleplay에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="camel-communicative-agents-for-mind-exploration-of-large-scale-language-model-societyli2023camel">CAMEL: Communicative Agents for &quot;Mind&quot; Exploration of Large Scale Language Model Society<sup id="fnref-118"><a href="#fn-118" class="footnote-ref">118</a></sup><a href="#camel-communicative-agents-for-mind-exploration-of-large-scale-language-model-societyli2023camel" class="hash-link" aria-label="camel-communicative-agents-for-mind-exploration-of-large-scale-language-model-societyli2023camel에 대한 직접 링크" title="camel-communicative-agents-for-mind-exploration-of-large-scale-language-model-societyli2023camel에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="teler-a-general-taxonomy-of-llm-prompts-for-benchmarking-complex-taskssantu2023teler">TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks<sup id="fnref-119"><a href="#fn-119" class="footnote-ref">119</a></sup><a href="#teler-a-general-taxonomy-of-llm-prompts-for-benchmarking-complex-taskssantu2023teler" class="hash-link" aria-label="teler-a-general-taxonomy-of-llm-prompts-for-benchmarking-complex-taskssantu2023teler에 대한 직접 링크" title="teler-a-general-taxonomy-of-llm-prompts-for-benchmarking-complex-taskssantu2023teler에 대한 직접 링크">​</a></h4><h2 class="anchor anchorWithStickyNavbar_LWe7" id="models">Models<a href="#models" class="hash-link" aria-label="Models에 대한 직접 링크" title="Models에 대한 직접 링크">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="image-models">Image Models<a href="#image-models" class="hash-link" aria-label="Image Models에 대한 직접 링크" title="Image Models에 대한 직접 링크">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="stable-diffusionrombach2021highresolution">Stable Diffusion<sup id="fnref-120"><a href="#fn-120" class="footnote-ref">120</a></sup><a href="#stable-diffusionrombach2021highresolution" class="hash-link" aria-label="stable-diffusionrombach2021highresolution에 대한 직접 링크" title="stable-diffusionrombach2021highresolution에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="dalleramesh2022hierarchical">DALLE<sup id="fnref-121"><a href="#fn-121" class="footnote-ref">121</a></sup><a href="#dalleramesh2022hierarchical" class="hash-link" aria-label="dalleramesh2022hierarchical에 대한 직접 링크" title="dalleramesh2022hierarchical에 대한 직접 링크">​</a></h4><h3 class="anchor anchorWithStickyNavbar_LWe7" id="language-models">Language Models<a href="#language-models" class="hash-link" aria-label="Language Models에 대한 직접 링크" title="Language Models에 대한 직접 링크">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="chatgptchatgpt2022">ChatGPT<sup id="fnref-122"><a href="#fn-122" class="footnote-ref">122</a></sup><a href="#chatgptchatgpt2022" class="hash-link" aria-label="chatgptchatgpt2022에 대한 직접 링크" title="chatgptchatgpt2022에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="gpt-3brown2020language">GPT-3<sup id="fnref-123"><a href="#fn-123" class="footnote-ref">123</a></sup><a href="#gpt-3brown2020language" class="hash-link" aria-label="gpt-3brown2020language에 대한 직접 링크" title="gpt-3brown2020language에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="instruct-gptouyang2022training">Instruct GPT<sup id="fnref-124"><a href="#fn-124" class="footnote-ref">124</a></sup><a href="#instruct-gptouyang2022training" class="hash-link" aria-label="instruct-gptouyang2022training에 대한 직접 링크" title="instruct-gptouyang2022training에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="gpt-4openai2023gpt4">GPT-4<sup id="fnref-125"><a href="#fn-125" class="footnote-ref">125</a></sup><a href="#gpt-4openai2023gpt4" class="hash-link" aria-label="gpt-4openai2023gpt4에 대한 직접 링크" title="gpt-4openai2023gpt4에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="palm-scaling-language-modeling-with-pathwayschowdhery2022palm">PaLM: Scaling Language Modeling with Pathways<sup id="fnref-126"><a href="#fn-126" class="footnote-ref">126</a></sup><a href="#palm-scaling-language-modeling-with-pathwayschowdhery2022palm" class="hash-link" aria-label="palm-scaling-language-modeling-with-pathwayschowdhery2022palm에 대한 직접 링크" title="palm-scaling-language-modeling-with-pathwayschowdhery2022palm에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="bloom-a-176b-parameter-open-access-multilingual-language-modelscao2022bloom">BLOOM: A 176B-Parameter Open-Access Multilingual Language Model<sup id="fnref-127"><a href="#fn-127" class="footnote-ref">127</a></sup><a href="#bloom-a-176b-parameter-open-access-multilingual-language-modelscao2022bloom" class="hash-link" aria-label="bloom-a-176b-parameter-open-access-multilingual-language-modelscao2022bloom에 대한 직접 링크" title="bloom-a-176b-parameter-open-access-multilingual-language-modelscao2022bloom에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="bloom1-adding-language-support-to-bloom-for-zero-shot-promptingyong2022bloom1">BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting<sup id="fnref-128"><a href="#fn-128" class="footnote-ref">128</a></sup><a href="#bloom1-adding-language-support-to-bloom-for-zero-shot-promptingyong2022bloom1" class="hash-link" aria-label="bloom1-adding-language-support-to-bloom-for-zero-shot-promptingyong2022bloom1에 대한 직접 링크" title="bloom1-adding-language-support-to-bloom-for-zero-shot-promptingyong2022bloom1에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="jurassic-1-technical-details-and-evaluation-white-paper-ai21-labs-2021lieberjurassic">Jurassic-1: Technical Details and Evaluation, White paper, AI21 Labs, 2021<sup id="fnref-129"><a href="#fn-129" class="footnote-ref">129</a></sup><a href="#jurassic-1-technical-details-and-evaluation-white-paper-ai21-labs-2021lieberjurassic" class="hash-link" aria-label="jurassic-1-technical-details-and-evaluation-white-paper-ai21-labs-2021lieberjurassic에 대한 직접 링크" title="jurassic-1-technical-details-and-evaluation-white-paper-ai21-labs-2021lieberjurassic에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="gpt-j-6b-a-6-billion-parameter-autoregressive-language-modelwange2021gptj">GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model<sup id="fnref-130"><a href="#fn-130" class="footnote-ref">130</a></sup><a href="#gpt-j-6b-a-6-billion-parameter-autoregressive-language-modelwange2021gptj" class="hash-link" aria-label="gpt-j-6b-a-6-billion-parameter-autoregressive-language-modelwange2021gptj에 대한 직접 링크" title="gpt-j-6b-a-6-billion-parameter-autoregressive-language-modelwange2021gptj에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="roberta-a-robustly-optimized-bert-pretraining-approachliu2019roberta">Roberta: A robustly optimized bert pretraining approach<sup id="fnref-131"><a href="#fn-131" class="footnote-ref">131</a></sup><a href="#roberta-a-robustly-optimized-bert-pretraining-approachliu2019roberta" class="hash-link" aria-label="roberta-a-robustly-optimized-bert-pretraining-approachliu2019roberta에 대한 직접 링크" title="roberta-a-robustly-optimized-bert-pretraining-approachliu2019roberta에 대한 직접 링크">​</a></h4><h2 class="anchor anchorWithStickyNavbar_LWe7" id="tooling">Tooling<a href="#tooling" class="hash-link" aria-label="Tooling에 대한 직접 링크" title="Tooling에 대한 직접 링크">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="ides">Ides<a href="#ides" class="hash-link" aria-label="Ides에 대한 직접 링크" title="Ides에 대한 직접 링크">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="textbox-20-a-text-generation-library-with-pre-trained-language-modelstang2022textbox">TextBox 2.0: A Text Generation Library with Pre-trained Language Models<sup id="fnref-132"><a href="#fn-132" class="footnote-ref">132</a></sup><a href="#textbox-20-a-text-generation-library-with-pre-trained-language-modelstang2022textbox" class="hash-link" aria-label="textbox-20-a-text-generation-library-with-pre-trained-language-modelstang2022textbox에 대한 직접 링크" title="textbox-20-a-text-generation-library-with-pre-trained-language-modelstang2022textbox에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="interactive-and-visual-prompt-engineering-for-ad-hoc-task-adaptation-with-large-language-modelsstrobelt2022promptide">Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models<sup id="fnref-133"><a href="#fn-133" class="footnote-ref">133</a></sup><a href="#interactive-and-visual-prompt-engineering-for-ad-hoc-task-adaptation-with-large-language-modelsstrobelt2022promptide" class="hash-link" aria-label="interactive-and-visual-prompt-engineering-for-ad-hoc-task-adaptation-with-large-language-modelsstrobelt2022promptide에 대한 직접 링크" title="interactive-and-visual-prompt-engineering-for-ad-hoc-task-adaptation-with-large-language-modelsstrobelt2022promptide에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="promptsource-an-integrated-development-environment-and-repository-for-natural-language-promptsbach2022promptsource">PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts<sup id="fnref-134"><a href="#fn-134" class="footnote-ref">134</a></sup><a href="#promptsource-an-integrated-development-environment-and-repository-for-natural-language-promptsbach2022promptsource" class="hash-link" aria-label="promptsource-an-integrated-development-environment-and-repository-for-natural-language-promptsbach2022promptsource에 대한 직접 링크" title="promptsource-an-integrated-development-environment-and-repository-for-natural-language-promptsbach2022promptsource에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="promptchainer-chaining-large-language-model-prompts-through-visual-programmingwu2022promptchainer">PromptChainer: Chaining Large Language Model Prompts through Visual Programming<sup id="fnref-135"><a href="#fn-135" class="footnote-ref">135</a></sup><a href="#promptchainer-chaining-large-language-model-prompts-through-visual-programmingwu2022promptchainer" class="hash-link" aria-label="promptchainer-chaining-large-language-model-prompts-through-visual-programmingwu2022promptchainer에 대한 직접 링크" title="promptchainer-chaining-large-language-model-prompts-through-visual-programmingwu2022promptchainer에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="openprompt-an-open-source-framework-for-prompt-learningding2021openprompt">OpenPrompt: An Open-source Framework for Prompt-learning<sup id="fnref-136"><a href="#fn-136" class="footnote-ref">136</a></sup><a href="#openprompt-an-open-source-framework-for-prompt-learningding2021openprompt" class="hash-link" aria-label="openprompt-an-open-source-framework-for-prompt-learningding2021openprompt에 대한 직접 링크" title="openprompt-an-open-source-framework-for-prompt-learningding2021openprompt에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="promptmaker-prompt-based-prototyping-with-largelanguagemodelsjiang2022promptmaker">PromptMaker: Prompt-Based Prototyping with Large<!-- --> <!-- -->Language<!-- --> <!-- -->Models<sup id="fnref-137"><a href="#fn-137" class="footnote-ref">137</a></sup><a href="#promptmaker-prompt-based-prototyping-with-largelanguagemodelsjiang2022promptmaker" class="hash-link" aria-label="promptmaker-prompt-based-prototyping-with-largelanguagemodelsjiang2022promptmaker에 대한 직접 링크" title="promptmaker-prompt-based-prototyping-with-largelanguagemodelsjiang2022promptmaker에 대한 직접 링크">​</a></h4><h3 class="anchor anchorWithStickyNavbar_LWe7" id="tools">Tools<a href="#tools" class="hash-link" aria-label="Tools에 대한 직접 링크" title="Tools에 대한 직접 링크">​</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="langchainchase_langchain_2022">LangChain<sup id="fnref-138"><a href="#fn-138" class="footnote-ref">138</a></sup><a href="#langchainchase_langchain_2022" class="hash-link" aria-label="langchainchase_langchain_2022에 대한 직접 링크" title="langchainchase_langchain_2022에 대한 직접 링크">​</a></h4><h4 class="anchor anchorWithStickyNavbar_LWe7" id="gpt-indexliu_gpt_index_2022">GPT Index<sup id="fnref-139"><a href="#fn-139" class="footnote-ref">139</a></sup><a href="#gpt-indexliu_gpt_index_2022" class="hash-link" aria-label="gpt-indexliu_gpt_index_2022에 대한 직접 링크" title="gpt-indexliu_gpt_index_2022에 대한 직접 링크">​</a></h4><div class="footnotes"><hr><ol><li id="fn-1">Karpas, E., Abend, O., Belinkov, Y., Lenz, B., Lieber, O., Ratner, N., Shoham, Y., Bata, H., Levine, Y., Leyton-Brown, K., Muhlgay, D., Rozen, N., Schwartz, E., Shachaf, G., Shalev-Shwartz, S., Shashua, A., &amp; Tenenholtz, M. (2022).
<a href="#fnref-1" class="footnote-backref">↩</a></li><li id="fn-2">Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., &amp; Cao, Y. (2022).
<a href="#fnref-2" class="footnote-backref">↩</a></li><li id="fn-3">Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y., Callan, J., &amp; Neubig, G. (2022).
<a href="#fnref-3" class="footnote-backref">↩</a></li><li id="fn-4">Significant-Gravitas. (2023). https://news.agpt.co/
<a href="#fnref-4" class="footnote-backref">↩</a></li><li id="fn-5">Nakajima, Y. (2023). https://github.com/yoheinakajima/babyagi
<a href="#fnref-5" class="footnote-backref">↩</a></li><li id="fn-6">Reworkd.ai. (2023). https://github.com/reworkd/AgentGPT
<a href="#fnref-6" class="footnote-backref">↩</a></li><li id="fn-7">Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N., &amp; Scialom, T. (2023).
<a href="#fnref-7" class="footnote-backref">↩</a></li><li id="fn-8">Shin, T., Razeghi, Y., Logan IV, R. L., Wallace, E., &amp; Singh, S. (2020). AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). https://doi.org/10.18653/v1/2020.emnlp-main.346
<a href="#fnref-8" class="footnote-backref">↩</a></li><li id="fn-9">Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., &amp; Ba, J. (2022). Large Language Models Are Human-Level Prompt Engineers.
<a href="#fnref-9" class="footnote-backref">↩</a></li><li id="fn-10">Lester, B., Al-Rfou, R., &amp; Constant, N. (2021). The Power of Scale for Parameter-Efficient Prompt Tuning.
<a href="#fnref-10" class="footnote-backref">↩</a></li><li id="fn-11">Khashabi, D., Lyu, S., Min, S., Qin, L., Richardson, K., Welleck, S., Hajishirzi, H., Khot, T., Sabharwal, A., Singh, S., &amp; Choi, Y. (2021). Prompt Waywardness: The Curious Case of Discretized Interpretation of Continuous Prompts.
<a href="#fnref-11" class="footnote-backref">↩</a></li><li id="fn-12">Lake, B. M., &amp; Baroni, M. (2018). Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks. https://doi.org/10.48550/arXiv.1711.00350
<a href="#fnref-12" class="footnote-backref">↩</a></li><li id="fn-13">Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., &amp; Schulman, J. (2021). Training Verifiers to Solve Math Word Problems.
<a href="#fnref-13" class="footnote-backref">↩</a></li><li id="fn-14">Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W. W., Salakhutdinov, R., &amp; Manning, C. D. (2018). HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering.
<a href="#fnref-14" class="footnote-backref">↩</a></li><li id="fn-15">Roy, S., &amp; Roth, D. (2015). Solving General Arithmetic Word Problems. Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 1743–1752. https://doi.org/10.18653/v1/D15-1202
<a href="#fnref-15" class="footnote-backref">↩</a></li><li id="fn-16">Thorne, J., Vlachos, A., Christodoulopoulos, C., &amp; Mittal, A. (2018). FEVER: a large-scale dataset for Fact Extraction and VERification.
<a href="#fnref-16" class="footnote-backref">↩</a></li><li id="fn-17">Parrish, A., Chen, A., Nangia, N., Padmakumar, V., Phang, J., Thompson, J., Htut, P. M., &amp; Bowman, S. R. (2021). BBQ: A Hand-Built Bias Benchmark for Question Answering.
<a href="#fnref-17" class="footnote-backref">↩</a></li><li id="fn-18">Roose, K. (2022). Don’t ban chatgpt in schools. teach with it. https://www.nytimes.com/2023/01/12/technology/chatgpt-schools-teachers.html
<a href="#fnref-18" class="footnote-backref">↩</a></li><li id="fn-19">Lipman, J., &amp; Distler, R. (2023). Schools Shouldn’t Ban Access to ChatGPT. https://time.com/6246574/schools-shouldnt-ban-access-to-chatgpt/
<a href="#fnref-19" class="footnote-backref">↩</a></li><li id="fn-20">Bansal, A., yeh Ping-Chiang, Curry, M., Jain, R., Wigington, C., Manjunatha, V., Dickerson, J. P., &amp; Goldstein, T. (2022). Certified Neural Network Watermarks with Randomized Smoothing.
<a href="#fnref-20" class="footnote-backref">↩</a></li><li id="fn-21">Gu, C., Huang, C., Zheng, X., Chang, K.-W., &amp; Hsieh, C.-J. (2022). Watermarking Pre-trained Language Models with Backdooring.
<a href="#fnref-21" class="footnote-backref">↩</a></li><li id="fn-22">Noonan, E., &amp; Averill, O. (2023). GW preparing disciplinary response to AI programs as faculty explore educational use. https://www.gwhatchet.com/2023/01/17/gw-preparing-disciplinary-response-to-ai-programs-as-faculty-explore-educational-use/
<a href="#fnref-22" class="footnote-backref">↩</a></li><li id="fn-23">Kirchenbauer, J., Geiping, J., Wen, Y., Katz, J., Miers, I., &amp; Goldstein, T. (2023). A Watermark for Large Language Models. https://arxiv.org/abs/2301.10226
<a href="#fnref-23" class="footnote-backref">↩</a></li><li id="fn-24">Mitchell, E., Lee, Y., Khazatsky, A., Manning, C., &amp; Finn, C. (2023). DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature. https://doi.org/10.48550/arXiv.2301.11305
<a href="#fnref-24" class="footnote-backref">↩</a></li><li id="fn-25">Oppenlaender, J. (2022). Prompt Engineering for Text-Based Generative Art.
<a href="#fnref-25" class="footnote-backref">↩</a></li><li id="fn-26">Parsons, G. (2022). The DALLE 2 Prompt Book. https://dallery.gallery/the-dalle-2-prompt-book/
<a href="#fnref-26" class="footnote-backref">↩</a></li><li id="fn-27">Blake. (2022). With the right prompt, Stable Diffusion 2.0 can do hands. https://www.reddit.com/r/StableDiffusion/comments/z7salo/with_the_right_prompt_stable_diffusion_20_can_do/
<a href="#fnref-27" class="footnote-backref">↩</a></li><li id="fn-28">Davenport, T. H., &amp; Mittal, N. (2022). How Generative AI Is Changing Creative Work. Harvard Business Review. https://hbr.org/2022/11/how-generative-ai-is-changing-creative-work
<a href="#fnref-28" class="footnote-backref">↩</a></li><li id="fn-29">Captain, S. (2023). How AI Will Change the Workplace. Wall Street Journal. https://www.wsj.com/articles/how-ai-change-workplace-af2162ee
<a href="#fnref-29" class="footnote-backref">↩</a></li><li id="fn-30">Verma, P., &amp; Vynck, G. D. (2023). ChatGPT took their jobs. Now they walk dogs and fix air conditioners. Washington Post. https://www.washingtonpost.com/technology/2023/06/02/ai-taking-jobs/
<a href="#fnref-30" class="footnote-backref">↩</a></li><li id="fn-31">Ford, B. (2023). Bloomberg.Com. https://www.bloomberg.com/news/articles/2023-05-01/ibm-to-pause-hiring-for-back-office-jobs-that-ai-could-kill
<a href="#fnref-31" class="footnote-backref">↩</a></li><li id="fn-32">Efrat, A., &amp; Levy, O. (2020). The Turking Test: Can Language Models Understand Instructions?
<a href="#fnref-32" class="footnote-backref">↩</a></li><li id="fn-33">Oppenlaender, J. (2022). A Taxonomy of Prompt Modifiers for Text-To-Image Generation.
<a href="#fnref-33" class="footnote-backref">↩</a></li><li id="fn-34">Wang, Z. J., Montoya, E., Munechika, D., Yang, H., Hoover, B., &amp; Chau, D. H. (2022). DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models.
<a href="#fnref-34" class="footnote-backref">↩</a></li><li id="fn-35">Hao, Y., Chi, Z., Dong, L., &amp; Wei, F. (2022). Optimizing Prompts for Text-to-Image Generation.
<a href="#fnref-35" class="footnote-backref">↩</a></li><li id="fn-36">Dohan, D., Xu, W., Lewkowycz, A., Austin, J., Bieber, D., Lopes, R. G., Wu, Y., Michalewski, H., Saurous, R. A., Sohl-dickstein, J., Murphy, K., &amp; Sutton, C. (2022). Language Model Cascades.
<a href="#fnref-36" class="footnote-backref">↩</a></li><li id="fn-37">Liu, V., &amp; Chilton, L. B. (2022). Design Guidelines for Prompt Engineering Text-to-Image Generative Models. Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3491102.3501825
<a href="#fnref-37" class="footnote-backref">↩</a></li><li id="fn-38">Perez, E., Ringer, S., Lukošiūtė, K., Nguyen, K., Chen, E., Heiner, S., Pettit, C., Olsson, C., Kundu, S., Kadavath, S., Jones, A., Chen, A., Mann, B., Israel, B., Seethor, B., McKinnon, C., Olah, C., Yan, D., Amodei, D., … Kaplan, J. (2022). Discovering Language Model Behaviors with Model-Written Evaluations.
<a href="#fnref-38" class="footnote-backref">↩</a></li><li id="fn-39">Su, H., Kasai, J., Wu, C. H., Shi, W., Wang, T., Xin, J., Zhang, R., Ostendorf, M., Zettlemoyer, L., Smith, N. A., &amp; Yu, T. (2022). Selective Annotation Makes Language Models Better Few-Shot Learners.
<a href="#fnref-39" class="footnote-backref">↩</a></li><li id="fn-40">Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., &amp; Grave, E. (2022). Atlas: Few-shot Learning with Retrieval Augmented Language Models.
<a href="#fnref-40" class="footnote-backref">↩</a></li><li id="fn-41">Wang, B., Feng, C., Nair, A., Mao, M., Desai, J., Celikyilmaz, A., Li, H., Mehdad, Y., &amp; Radev, D. (2022). STRUDEL: Structured Dialogue Summarization for Dialogue Comprehension.
<a href="#fnref-41" class="footnote-backref">↩</a></li><li id="fn-42">Beurer-Kellner, L., Fischer, M., &amp; Vechev, M. (2022). Prompting Is Programming: A Query Language For Large Language Models.
<a href="#fnref-42" class="footnote-backref">↩</a></li><li id="fn-43">Ratner, N., Levine, Y., Belinkov, Y., Ram, O., Abend, O., Karpas, E., Shashua, A., Leyton-Brown, K., &amp; Shoham, Y. (2022). Parallel Context Windows Improve In-Context Learning of Large Language Models.
<a href="#fnref-43" class="footnote-backref">↩</a></li><li id="fn-44">Bursztyn, V. S., Demeter, D., Downey, D., &amp; Birnbaum, L. (2022). Learning to Perform Complex Tasks through Compositional Fine-Tuning of Language Models.
<a href="#fnref-44" class="footnote-backref">↩</a></li><li id="fn-45">Wang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y., Mirzaei, A., Arunkumar, A., Ashok, A., Dhanasekaran, A. S., Naik, A., Stap, D., Pathak, E., Karamanolakis, G., Lai, H. G., Purohit, I., Mondal, I., Anderson, J., Kuznia, K., Doshi, K., Patel, M., … Khashabi, D. (2022). Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks.
<a href="#fnref-45" class="footnote-backref">↩</a></li><li id="fn-46">Gao, T., Fisch, A., &amp; Chen, D. (2021). Making Pre-trained Language Models Better Few-shot Learners. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). https://doi.org/10.18653/v1/2021.acl-long.295
<a href="#fnref-46" class="footnote-backref">↩</a></li><li id="fn-47">Dang, H., Mecke, L., Lehmann, F., Goller, S., &amp; Buschek, D. (2022). How to Prompt? Opportunities and Challenges of Zero- and Few-Shot Learning for Human-AI Interaction in Creative Applications of Generative Models.
<a href="#fnref-47" class="footnote-backref">↩</a></li><li id="fn-48">Akyürek, A. F., Paik, S., Kocyigit, M. Y., Akbiyik, S., Runyun, Ş. L., &amp; Wijaya, D. (2022). On Measuring Social Biases in Prompt-Based Multi-Task Learning.
<a href="#fnref-48" class="footnote-backref">↩</a></li><li id="fn-49">Jin, Y., Kadam, V., &amp; Wanvarie, D. (2022). Plot Writing From Pre-Trained Language Models.
<a href="#fnref-49" class="footnote-backref">↩</a></li><li id="fn-50">Nadeem, M., Bethke, A., &amp; Reddy, S. (2021). StereoSet: Measuring stereotypical bias in pretrained language models. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 5356–5371. https://doi.org/10.18653/v1/2021.acl-long.416
<a href="#fnref-50" class="footnote-backref">↩</a></li><li id="fn-51">Ji, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y., Madotto, A., &amp; Fung, P. (2022). Survey of Hallucination in Natural Language Generation. ACM Computing Surveys. https://doi.org/10.1145/3571730
<a href="#fnref-51" class="footnote-backref">↩</a></li><li id="fn-52">Yuan, A., Coenen, A., Reif, E., &amp; Ippolito, D. (2022). Wordcraft: Story Writing With Large Language Models. 27th International Conference on Intelligent User Interfaces, 841–852.
<a href="#fnref-52" class="footnote-backref">↩</a></li><li id="fn-53">Fadnavis, S., Dhurandhar, A., Norel, R., Reinen, J. M., Agurto, C., Secchettin, E., Schweiger, V., Perini, G., &amp; Cecchi, G. (2022). PainPoints: A Framework for Language-based Detection of Chronic Pain and Expert-Collaborative Text-Summarization. arXiv Preprint arXiv:2209.09814.
<a href="#fnref-53" class="footnote-backref">↩</a></li><li id="fn-54">Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., &amp; Hajishirzi, H. (2022). Self-Instruct: Aligning Language Model with Self Generated Instructions.
<a href="#fnref-54" class="footnote-backref">↩</a></li><li id="fn-55">Guo, J., Li, J., Li, D., Tiong, A. M. H., Li, B., Tao, D., &amp; Hoi, S. C. H. (2022). From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models.
<a href="#fnref-55" class="footnote-backref">↩</a></li><li id="fn-56">Markov, T. (2022). New and improved content moderation tooling. In OpenAI. OpenAI. https://openai.com/blog/new-and-improved-content-moderation-tooling/
<a href="#fnref-56" class="footnote-backref">↩</a></li><li id="fn-57">Schick, T., &amp; Schütze, H. (2020). Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference.
<a href="#fnref-57" class="footnote-backref">↩</a></li><li id="fn-58">Lake, B. M., Salakhutdinov, R., &amp; Tenenbaum, J. B. (2015). Human-level concept learning through probabilistic program induction. Science, 350(6266), 1332–1338.
<a href="#fnref-58" class="footnote-backref">↩</a></li><li id="fn-59">Forsgren, S., &amp; Martiros, H. (2022). Riffusion - Stable diffusion for real-time music generation. https://riffusion.com/about
<a href="#fnref-59" class="footnote-backref">↩</a></li><li id="fn-60">Bonta, A. (2022). How to use OpenAI’s ChatGPT to write the perfect cold email. https://www.streak.com/post/how-to-use-ai-to-write-perfect-cold-emails
<a href="#fnref-60" class="footnote-backref">↩</a></li><li id="fn-61">Nobel, P. S., &amp; others. (2002). Cacti: biology and uses. Univ of California Press.
<a href="#fnref-61" class="footnote-backref">↩</a></li><li id="fn-62">Webson, A., Loo, A. M., Yu, Q., &amp; Pavlick, E. (2023). Are Language Models Worse than Humans at Following Prompts? It’s Complicated. arXiv:2301.07085v1 [Cs.CL].
<a href="#fnref-62" class="footnote-backref">↩</a></li><li id="fn-63">Wang, Z., Mao, S., Wu, W., Ge, T., Wei, F., &amp; Ji, H. (2023). Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration.
<a href="#fnref-63" class="footnote-backref">↩</a></li><li id="fn-64">Crothers, E., Japkowicz, N., &amp; Viktor, H. (2022). Machine Generated Text: A Comprehensive Survey of Threat Models and Detection Methods.
<a href="#fnref-64" class="footnote-backref">↩</a></li><li id="fn-65">u/Nin_kat. (2023). New jailbreak based on virtual functions - smuggle illegal tokens to the backend. https://www.reddit.com/r/ChatGPT/comments/10urbdj/new_jailbreak_based_on_virtual_functions_smuggle
<a href="#fnref-65" class="footnote-backref">↩</a></li><li id="fn-66">Kang, D., Li, X., Stoica, I., Guestrin, C., Zaharia, M., &amp; Hashimoto, T. (2023). Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks.
<a href="#fnref-66" class="footnote-backref">↩</a></li><li id="fn-67">Greshake, K., Abdelnabi, S., Mishra, S., Endres, C., Holz, T., &amp; Fritz, M. (2023). More than you’ve asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models.
<a href="#fnref-67" class="footnote-backref">↩</a></li><li id="fn-68">KIHO, L. (2023). ChatGPT “DAN” (and other “Jailbreaks”). https://github.com/0xk1h0/ChatGPT_DAN
<a href="#fnref-68" class="footnote-backref">↩</a></li><li id="fn-69">Branch, H. J., Cefalu, J. R., McHugh, J., Hujer, L., Bahl, A., del Castillo Iglesias, D., Heichman, R., &amp; Darwishi, R. (2022). Evaluating the Susceptibility of Pre-Trained Language Models via Handcrafted Adversarial Examples.
<a href="#fnref-69" class="footnote-backref">↩</a></li><li id="fn-70">Willison, S. (2022). Prompt injection attacks against GPT-3. https://simonwillison.net/2022/Sep/12/prompt-injection/
<a href="#fnref-70" class="footnote-backref">↩</a></li><li id="fn-71">Goodside, R. (2022). Exploiting GPT-3 prompts with malicious inputs that order the model to ignore its previous directions. https://twitter.com/goodside/status/1569128808308957185
<a href="#fnref-71" class="footnote-backref">↩</a></li><li id="fn-72">Goodside, R. (2023). History Correction. https://twitter.com/goodside/status/1610110111791325188?s=20&amp;t=ulviQABPXFIIt4ZNZPAUCQ
<a href="#fnref-72" class="footnote-backref">↩</a></li><li id="fn-73">Chase, H. (2022). adversarial-prompts. https://github.com/hwchase17/adversarial-prompts
<a href="#fnref-73" class="footnote-backref">↩</a></li><li id="fn-74">Goodside, R. (2022). GPT-3 Prompt Injection Defenses. https://twitter.com/goodside/status/1578278974526222336?s=20&amp;t=3UMZB7ntYhwAk3QLpKMAbw
<a href="#fnref-74" class="footnote-backref">↩</a></li><li id="fn-75">Mark, C. (2022). Talking to machines: prompt engineering &amp; injection. https://artifact-research.com/artificial-intelligence/talking-to-machines-prompt-engineering-injection/
<a href="#fnref-75" class="footnote-backref">↩</a></li><li id="fn-76">Stuart Armstrong, R. G. (2022). Using GPT-Eliezer against ChatGPT Jailbreaking. https://www.alignmentforum.org/posts/pNcFYZnPdXyL2RfgA/using-gpt-eliezer-against-chatgpt-jailbreaking
<a href="#fnref-76" class="footnote-backref">↩</a></li><li id="fn-77">Selvi, J. (2022). Exploring Prompt Injection Attacks. https://research.nccgroup.com/2022/12/05/exploring-prompt-injection-attacks/
<a href="#fnref-77" class="footnote-backref">↩</a></li><li id="fn-78">Liu, K. (2023). The entire prompt of Microsoft Bing Chat?! (Hi, Sydney.). https://twitter.com/kliu128/status/1623472922374574080
<a href="#fnref-78" class="footnote-backref">↩</a></li><li id="fn-79">Perez, F., &amp; Ribeiro, I. (2022). Ignore Previous Prompt: Attack Techniques For Language Models. arXiv. https://doi.org/10.48550/ARXIV.2211.09527
<a href="#fnref-79" class="footnote-backref">↩</a></li><li id="fn-80">Brundage, M. (2022). Lessons learned on Language Model Safety and misuse. In OpenAI. OpenAI. https://openai.com/blog/language-model-safety-and-misuse/
<a href="#fnref-80" class="footnote-backref">↩</a></li><li id="fn-81">Wang, Y.-S., &amp; Chang, Y. (2022). Toxicity Detection with Generative Prompt-based Inference. arXiv. https://doi.org/10.48550/ARXIV.2205.12390
<a href="#fnref-81" class="footnote-backref">↩</a></li><li id="fn-82">Maz, A. (2022). ok I saw a few people jailbreaking safeguards openai put on chatgpt so I had to give it a shot myself. https://twitter.com/alicemazzy/status/1598288519301976064
<a href="#fnref-82" class="footnote-backref">↩</a></li><li id="fn-83">Piedrafita, M. (2022). Bypass @OpenAI’s ChatGPT alignment efforts with this one weird trick. https://twitter.com/m1guelpf/status/1598203861294252033
<a href="#fnref-83" class="footnote-backref">↩</a></li><li id="fn-84">Parfait, D. (2022). ChatGPT jailbreaking itself. https://twitter.com/haus_cole/status/1598541468058390534
<a href="#fnref-84" class="footnote-backref">↩</a></li><li id="fn-85">Soares, N. (2022). Using “pretend” on #ChatGPT can do some wild stuff. You can kind of get some insight on the future, alternative universe. https://twitter.com/NeroSoares/status/1608527467265904643
<a href="#fnref-85" class="footnote-backref">↩</a></li><li id="fn-86">Moran, N. (2022). I kinda like this one even more! https://twitter.com/NickEMoran/status/1598101579626057728
<a href="#fnref-86" class="footnote-backref">↩</a></li><li id="fn-87">samczsun. (2022). uh oh. https://twitter.com/samczsun/status/1598679658488217601
<a href="#fnref-87" class="footnote-backref">↩</a></li><li id="fn-88">Degrave, J. (2022). Building A Virtual Machine inside ChatGPT. Engraved. https://www.engraved.blog/building-a-virtual-machine-inside/
<a href="#fnref-88" class="footnote-backref">↩</a></li><li id="fn-89">Imani, S., Du, L., &amp; Shrivastava, H. (2023). MathPrompter: Mathematical Reasoning using Large Language Models.
<a href="#fnref-89" class="footnote-backref">↩</a></li><li id="fn-90">Ye, X., &amp; Durrett, G. (2022). The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning.
<a href="#fnref-90" class="footnote-backref">↩</a></li><li id="fn-91">Si, C., Gan, Z., Yang, Z., Wang, S., Wang, J., Boyd-Graber, J., &amp; Wang, L. (2022). Prompting GPT-3 To Be Reliable.
<a href="#fnref-91" class="footnote-backref">↩</a></li><li id="fn-92">Li, Y., Lin, Z., Zhang, S., Fu, Q., Chen, B., Lou, J.-G., &amp; Chen, W. (2022). On the Advance of Making Language Models Better Reasoners.
<a href="#fnref-92" class="footnote-backref">↩</a></li><li id="fn-93">Arora, S., Narayan, A., Chen, M. F., Orr, L., Guha, N., Bhatia, K., Chami, I., Sala, F., &amp; Ré, C. (2022). Ask Me Anything: A simple strategy for prompting language models.
<a href="#fnref-93" class="footnote-backref">↩</a></li><li id="fn-94">Zhao, T. Z., Wallace, E., Feng, S., Klein, D., &amp; Singh, S. (2021). Calibrate Before Use: Improving Few-Shot Performance of Language Models.
<a href="#fnref-94" class="footnote-backref">↩</a></li><li id="fn-95">Liévin, V., Hother, C. E., &amp; Winther, O. (2022). Can large language models reason about medical questions?
<a href="#fnref-95" class="footnote-backref">↩</a></li><li id="fn-96">Mitchell, E., Noh, J. J., Li, S., Armstrong, W. S., Agarwal, A., Liu, P., Finn, C., &amp; Manning, C. D. (2022). Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference.
<a href="#fnref-96" class="footnote-backref">↩</a></li><li id="fn-97">Shaikh, O., Zhang, H., Held, W., Bernstein, M., &amp; Yang, D. (2022). On Second Thought, Let’s Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning.
<a href="#fnref-97" class="footnote-backref">↩</a></li><li id="fn-98">Chase, H. (2022). Evaluating language models can be tricky. https://twitter.com/hwchase17/status/1607428141106008064
<a href="#fnref-98" class="footnote-backref">↩</a></li><li id="fn-99">Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McKinnon, C., Chen, C., Olsson, C., Olah, C., Hernandez, D., Drain, D., Ganguli, D., Li, D., Tran-Johnson, E., Perez, E., … Kaplan, J. (2022). Constitutional AI: Harmlessness from AI Feedback.
<a href="#fnref-99" class="footnote-backref">↩</a></li><li id="fn-100">Jurafsky, D., &amp; Martin, J. H. (2009). Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics and Speech Recognition. Prentice Hall.
<a href="#fnref-100" class="footnote-backref">↩</a></li><li id="fn-101">Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., &amp; Neubig, G. (2022). Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing. ACM Computing Surveys. https://doi.org/10.1145/3560815
<a href="#fnref-101" class="footnote-backref">↩</a></li><li id="fn-102">Ding, N., &amp; Hu, S. (2022). PromptPapers. https://github.com/thunlp/PromptPapers
<a href="#fnref-102" class="footnote-backref">↩</a></li><li id="fn-103">White, J., Fu, Q., Hays, S., Sandborn, M., Olea, C., Gilbert, H., Elnashar, A., Spencer-Smith, J., &amp; Schmidt, D. C. (2023). A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT.
<a href="#fnref-103" class="footnote-backref">↩</a></li><li id="fn-104">Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., &amp; Zhou, D. (2022). Chain of Thought Prompting Elicits Reasoning in Large Language Models.
<a href="#fnref-104" class="footnote-backref">↩</a></li><li id="fn-105">Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., &amp; Iwasawa, Y. (2022). Large Language Models are Zero-Shot Reasoners.
<a href="#fnref-105" class="footnote-backref">↩</a></li><li id="fn-106">Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., &amp; Zhou, D. (2022). Self-Consistency Improves Chain of Thought Reasoning in Language Models.
<a href="#fnref-106" class="footnote-backref">↩</a></li><li id="fn-107">Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., &amp; Chen, W. (2022). What Makes Good In-Context Examples for GPT-3? Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures. https://doi.org/10.18653/v1/2022.deelio-1.10
<a href="#fnref-107" class="footnote-backref">↩</a></li><li id="fn-108">Liu, J., Liu, A., Lu, X., Welleck, S., West, P., Bras, R. L., Choi, Y., &amp; Hajishirzi, H. (2021). Generated Knowledge Prompting for Commonsense Reasoning.
<a href="#fnref-108" class="footnote-backref">↩</a></li><li id="fn-109">Sun, Z., Wang, X., Tay, Y., Yang, Y., &amp; Zhou, D. (2022). Recitation-Augmented Language Models.
<a href="#fnref-109" class="footnote-backref">↩</a></li><li id="fn-110">Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., &amp; Zettlemoyer, L. (2022). Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?
<a href="#fnref-110" class="footnote-backref">↩</a></li><li id="fn-111">Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., Sutton, C., &amp; Odena, A. (2021). Show Your Work: Scratchpads for Intermediate Computation with Language Models.
<a href="#fnref-111" class="footnote-backref">↩</a></li><li id="fn-112">Jung, J., Qin, L., Welleck, S., Brahman, F., Bhagavatula, C., Bras, R. L., &amp; Choi, Y. (2022). Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations.
<a href="#fnref-112" class="footnote-backref">↩</a></li><li id="fn-113">Zelikman, E., Wu, Y., Mu, J., &amp; Goodman, N. D. (2022). STaR: Bootstrapping Reasoning With Reasoning.
<a href="#fnref-113" class="footnote-backref">↩</a></li><li id="fn-114">Zhou, D., Schärli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Cui, C., Bousquet, O., Le, Q., &amp; Chi, E. (2022). Least-to-Most Prompting Enables Complex Reasoning in Large Language Models.
<a href="#fnref-114" class="footnote-backref">↩</a></li><li id="fn-115">Mishra, S., Khashabi, D., Baral, C., Choi, Y., &amp; Hajishirzi, H. (2022). Reframing Instructional Prompts to GPTk’s Language. Findings of the Association for Computational Linguistics: ACL 2022. https://doi.org/10.18653/v1/2022.findings-acl.50
<a href="#fnref-115" class="footnote-backref">↩</a></li><li id="fn-116">Logan IV, R., Balazevic, I., Wallace, E., Petroni, F., Singh, S., &amp; Riedel, S. (2022). Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models. Findings of the Association for Computational Linguistics: ACL 2022, 2824–2835. https://doi.org/10.18653/v1/2022.findings-acl.222
<a href="#fnref-116" class="footnote-backref">↩</a></li><li id="fn-117">Shanahan, M., McDonell, K., &amp; Reynolds, L. (2023). Role-Play with Large Language Models.
<a href="#fnref-117" class="footnote-backref">↩</a></li><li id="fn-118">Li, G., Hammoud, H. A. A. K., Itani, H., Khizbullin, D., &amp; Ghanem, B. (2023). CAMEL: Communicative Agents for “Mind” Exploration of Large Scale Language Model Society.
<a href="#fnref-118" class="footnote-backref">↩</a></li><li id="fn-119">Santu, S. K. K., &amp; Feng, D. (2023). TELeR: A General Taxonomy of LLM Prompts for Benchmarking Complex Tasks.
<a href="#fnref-119" class="footnote-backref">↩</a></li><li id="fn-120">Rombach, R., Blattmann, A., Lorenz, D., Esser, P., &amp; Ommer, B. (2021). High-Resolution Image Synthesis with Latent Diffusion Models.
<a href="#fnref-120" class="footnote-backref">↩</a></li><li id="fn-121">Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., &amp; Chen, M. (2022). Hierarchical Text-Conditional Image Generation with CLIP Latents.
<a href="#fnref-121" class="footnote-backref">↩</a></li><li id="fn-122">OpenAI. (2022). ChatGPT: Optimizing Language Models for Dialogue. https://openai.com/blog/chatgpt/. https://openai.com/blog/chatgpt/
<a href="#fnref-122" class="footnote-backref">↩</a></li><li id="fn-123">Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., … Amodei, D. (2020). Language Models are Few-Shot Learners.
<a href="#fnref-123" class="footnote-backref">↩</a></li><li id="fn-124">Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., &amp; Lowe, R. (2022). Training language models to follow instructions with human feedback.
<a href="#fnref-124" class="footnote-backref">↩</a></li><li id="fn-125">OpenAI. (2023). GPT-4 Technical Report.
<a href="#fnref-125" class="footnote-backref">↩</a></li><li id="fn-126">Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., … Fiedel, N. (2022). PaLM: Scaling Language Modeling with Pathways.
<a href="#fnref-126" class="footnote-backref">↩</a></li><li id="fn-127">Scao, T. L., Fan, A., Akiki, C., Pavlick, E., Ilić, S., Hesslow, D., Castagné, R., Luccioni, A. S., Yvon, F., Gallé, M., Tow, J., Rush, A. M., Biderman, S., Webson, A., Ammanamanchi, P. S., Wang, T., Sagot, B., Muennighoff, N., del Moral, A. V., … Wolf, T. (2022). BLOOM: A 176B-Parameter Open-Access Multilingual Language Model.
<a href="#fnref-127" class="footnote-backref">↩</a></li><li id="fn-128">Yong, Z.-X., Schoelkopf, H., Muennighoff, N., Aji, A. F., Adelani, D. I., Almubarak, K., Bari, M. S., Sutawika, L., Kasai, J., Baruwa, A., Winata, G. I., Biderman, S., Radev, D., &amp; Nikoulina, V. (2022). BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting.
<a href="#fnref-128" class="footnote-backref">↩</a></li><li id="fn-129">Lieber, O., Sharir, O., Lentz, B., &amp; Shoham, Y. (2021). Jurassic-1: Technical Details and Evaluation, White paper, AI21 Labs, 2021. URL: Https://Uploads-Ssl. Webflow. Com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_ Tech_paper. Pdf.
<a href="#fnref-129" class="footnote-backref">↩</a></li><li id="fn-130">Wang, B., &amp; Komatsuzaki, A. (2021). GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax. https://github.com/kingoflolz/mesh-transformer-jax
<a href="#fnref-130" class="footnote-backref">↩</a></li><li id="fn-131">Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., &amp; Stoyanov, V. (2019). Roberta: A robustly optimized bert pretraining approach. arXiv Preprint arXiv:1907.11692.
<a href="#fnref-131" class="footnote-backref">↩</a></li><li id="fn-132">Tang, T., Junyi, L., Chen, Z., Hu, Y., Yu, Z., Dai, W., Dong, Z., Cheng, X., Wang, Y., Zhao, W., Nie, J., &amp; Wen, J.-R. (2022). TextBox 2.0: A Text Generation Library with Pre-trained Language Models.
<a href="#fnref-132" class="footnote-backref">↩</a></li><li id="fn-133">Strobelt, H., Webson, A., Sanh, V., Hoover, B., Beyer, J., Pfister, H., &amp; Rush, A. M. (2022). Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models. arXiv. https://doi.org/10.48550/ARXIV.2208.07852
<a href="#fnref-133" class="footnote-backref">↩</a></li><li id="fn-134">Bach, S. H., Sanh, V., Yong, Z.-X., Webson, A., Raffel, C., Nayak, N. V., Sharma, A., Kim, T., Bari, M. S., Fevry, T., Alyafeai, Z., Dey, M., Santilli, A., Sun, Z., Ben-David, S., Xu, C., Chhablani, G., Wang, H., Fries, J. A., … Rush, A. M. (2022). PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts.
<a href="#fnref-134" class="footnote-backref">↩</a></li><li id="fn-135">Wu, T., Jiang, E., Donsbach, A., Gray, J., Molina, A., Terry, M., &amp; Cai, C. J. (2022). PromptChainer: Chaining Large Language Model Prompts through Visual Programming.
<a href="#fnref-135" class="footnote-backref">↩</a></li><li id="fn-136">Ding, N., Hu, S., Zhao, W., Chen, Y., Liu, Z., Zheng, H.-T., &amp; Sun, M. (2021). OpenPrompt: An Open-source Framework for Prompt-learning. arXiv Preprint arXiv:2111.01998.
<a href="#fnref-136" class="footnote-backref">↩</a></li><li id="fn-137">Jiang, E., Olson, K., Toh, E., Molina, A., Donsbach, A., Terry, M., &amp; Cai, C. J. (2022). PromptMaker: Prompt-Based Prototyping with Large&amp;nbsp;Language&amp;nbsp;Models. Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems. https://doi.org/10.1145/3491101.3503564
<a href="#fnref-137" class="footnote-backref">↩</a></li><li id="fn-138">Chase, H. (2022). LangChain (0.0.66) [Computer software]. https://github.com/hwchase17/langchain
<a href="#fnref-138" class="footnote-backref">↩</a></li><li id="fn-139">Liu, J. (2022). GPT Index. https://doi.org/10.5281/zenodo.1234
<a href="#fnref-139" class="footnote-backref">↩</a></li></ol></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/trigaten/promptgineering/tree/v1.2.3/docs/bibliography.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>페이지 편집</a></div><div class="col lastUpdated_VsjB"></div></div><br><div style="text-align:center"><p><strong>Get the Latest Prompts Straight to Your Inbox</strong></p><iframe src="https://embeds.beehiiv.com/ae49cad6-1b3a-4ec2-91fa-73b7f3e0188a?slim=true" data-test-id="beehiiv-embed" height="52" width="100%" frameborder="0" scrolling="no" style="margin:0;border-radius:0;background-color:transparent" class="rounded-l-md bg-white text-dark/500 text-sm font-medium tracking-tight ring-0 focus:outline-none w-[250px] md:w-[450px] focus:ring-0"></iframe></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="문서 탐색"><a class="pagination-nav__link pagination-nav__link--prev" href="/ko/docs/vocabulary"><div class="pagination-nav__sublabel">이전</div><div class="pagination-nav__label">📙 Vocabulary Reference</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/ko/docs/products"><div class="pagination-nav__sublabel">다음</div><div class="pagination-nav__label">📦 Prompted Products</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#agents" class="table-of-contents__link toc-highlight">Agents</a></li><li><a href="#automated" class="table-of-contents__link toc-highlight">Automated</a></li><li><a href="#datasets" class="table-of-contents__link toc-highlight">Datasets</a></li><li><a href="#detection" class="table-of-contents__link toc-highlight">Detection</a></li><li><a href="#image-prompt-engineering" class="table-of-contents__link toc-highlight">Image Prompt Engineering</a></li><li><a href="#meta-analysis" class="table-of-contents__link toc-highlight">Meta Analysis</a></li><li><a href="#miscl" class="table-of-contents__link toc-highlight">Miscl</a></li><li><a href="#prompt-hacking" class="table-of-contents__link toc-highlight">Prompt Hacking</a></li><li><a href="#reliability" class="table-of-contents__link toc-highlight">Reliability</a></li><li><a href="#surveys" class="table-of-contents__link toc-highlight">Surveys</a></li><li><a href="#techniques" class="table-of-contents__link toc-highlight">Techniques</a></li><li><a href="#models" class="table-of-contents__link toc-highlight">Models</a><ul><li><a href="#image-models" class="table-of-contents__link toc-highlight">Image Models</a></li><li><a href="#language-models" class="table-of-contents__link toc-highlight">Language Models</a></li></ul></li><li><a href="#tooling" class="table-of-contents__link toc-highlight">Tooling</a><ul><li><a href="#ides" class="table-of-contents__link toc-highlight">Ides</a></li><li><a href="#tools" class="table-of-contents__link toc-highlight">Tools</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2023 Learn Prompting.</div></div></div></footer></div>
<script src="/ko/assets/js/runtime~main.261832bc.js"></script>
<script src="/ko/assets/js/main.2f5284eb.js"></script>
</body>
</html>